[{"content":"\rBayesian A/B Testing - Practice This interactive notebook demonstrates a concise, pragmatic approach to Bayesian A/B testing using PyMC and analytic Beta-Binomial formulas.\nWhat you\u0026rsquo;ll find here:\nA minimal, runnable PyMC example to obtain posterior samples. A hierarchical model example for multiple related tests (shrinkage). Analytic Beta-Binomial updates and closed-form PoS / Expected Loss expressions. A Monte Carlo harness comparing frequentist sequential z-tests and several Bayesian stopping rules. TL;DR: Posterior summaries like the Probability of Superiority (PoS) are fantastic for interpretation, but if you stare at them until they cross a threshold (peeking), you will break your error guarantees. üõë If you care about long-run false positives, use decision-theoretic rules (Expected Loss) or precision-aware metrics (HDI).\nü•ß Part 1: A Simple Slice of PyMC Let\u0026rsquo;s kick things off with the basics: two variants, A and B, and a binary outcome (conversion: yes/no). Our goal? Use Markov Chains to sample the success probabilities and figure out if A is actually beating B.\nStep 1: Set up imports Grab PyMC along with must have DS and BI libraries.\nCode\rimport pymc as pm from pymc import Uniform, Bernoulli from matplotlib import pyplot as plt import seaborn as sns import pandas as pd import numpy as np from scipy import stats as sts Step 2: Simulate observed data Generate sample data: two variants with known conversion rates. In practice, these would be your real observed counts from the experiment.\na_default, b_default = 0.06, 0.04 a_count, b_count = 200, 150 rng = np.random.default_rng(seed=42) a_bernoulli_samples = rng.binomial(n=1, p=a_default, size=a_count) b_bernoulli_samples = rng.binomial(n=1, p=b_default, size=b_count) print( \u0026#34;Point Estimate\u0026#34; f\u0026#34;\\n- A: {a_bernoulli_samples.sum() / a_count :.3f}\u0026#34; f\u0026#34;\\n- B: {b_bernoulli_samples.sum() / b_count :.3f}\u0026#34; ) Point Estimate\r- A: 0.040\r- B: 0.020\rQuick sanity check: plug-in estimates (observed proportions). These will be compared with posterior estimates below.\nStep 3: Define the Bayesian model We treat success probabilities as independent random variables. Since we don\u0026rsquo;t know much yet, we use a uniform prior (weakly informative - go to for proportions). The observed data (Bernoulli trials) are the likelihood. We also track deterministic difference $= A - B $ , - because that\u0026rsquo;s what we actually care about!\nwith pm.Model() as my_model: A_prior = Uniform(\u0026#39;A_prior\u0026#39;, lower=0, upper=1) B_prior = Uniform(\u0026#39;B_prior\u0026#39;, lower=0, upper=1) A_observed = Bernoulli(\u0026#39;A_observed\u0026#39;, p=A_prior, observed=a_bernoulli_samples) B_observed = Bernoulli(\u0026#39;B_observed\u0026#39;, p=B_prior, observed=b_bernoulli_samples) delta = pm.Deterministic(\u0026#34;delta\u0026#34;, A_prior - B_prior) Step 4: Sample from the posterior PyMC unleashes the \u0026ldquo;No-U-Turn Sampler\u0026rdquo; by default to draw samples from the joint posterior. The tune parameter controls burn-in iterations (discarded); draws are the kept samples used for inference.\nwith my_model: idata = pm.sample(draws=5000, tune=1000, cores=-1) Step 5: Visualize the posteriors Plot those posterior distributions of each variant and their difference. The vertical black line shows the true (generating) difference; the red line marks zero (no difference). If the posterior difference doesn\u0026rsquo;t touch zero, you\u0026rsquo;re onto something.\nCode\r# --- Setup Dark Mode --- plt.style.use(\u0026#39;dark_background\u0026#39;) # Ensure texts are white (sometimes needed depending on Jupyter setup) plt.rcParams.update({ \u0026#34;text.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;axes.labelcolor\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;xtick.color\u0026#34;: \u0026#34;white\u0026#34;, \u0026#34;ytick.color\u0026#34;: \u0026#34;white\u0026#34; }) fig, axes = plt.subplots(3, 1, figsize=(10, 10), constrained_layout=True) # Neon colors for pop against dark background colors = [\u0026#39;#00FFFF\u0026#39;, \u0026#39;#FF00FF\u0026#39;, \u0026#39;#32CD32\u0026#39;] # Cyan, Magenta, Lime # --- Plot 1: Posterior P(A) --- ax = axes[0] data_a = idata.posterior[\u0026#39;A_prior\u0026#39;].values.ravel() ax.hist(data_a, bins=50, density=True, color=colors[0], alpha=0.7, edgecolor=\u0026#39;black\u0026#39;, linewidth=1.2, label=\u0026#34;Posterior $P(A)$\u0026#34;) ax.set_title(\u0026#34;Posterior Probability of A\u0026#34;, fontsize=14, loc=\u0026#39;left\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax.legend(loc=\u0026#39;upper right\u0026#39;, frameon=False) ax.set_ylabel(\u0026#34;Density\u0026#34;) ax.spines[\u0026#39;top\u0026#39;].set_visible(False) ax.spines[\u0026#39;right\u0026#39;].set_visible(False) ax.grid(True, alpha=0.3, linestyle=\u0026#39;--\u0026#39;) # --- Plot 2: Posterior P(B) --- ax = axes[1] data_b = idata.posterior[\u0026#39;B_prior\u0026#39;].values.ravel() ax.hist(data_b, bins=50, density=True, color=colors[1], alpha=0.7, edgecolor=\u0026#39;black\u0026#39;, linewidth=1.2, label=\u0026#34;Posterior $P(B)$\u0026#34;) ax.set_title(\u0026#34;Posterior Probability of B\u0026#34;, fontsize=14, loc=\u0026#39;left\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax.legend(loc=\u0026#39;upper right\u0026#39;, frameon=False) ax.set_ylabel(\u0026#34;Density\u0026#34;) ax.spines[\u0026#39;top\u0026#39;].set_visible(False) ax.spines[\u0026#39;right\u0026#39;].set_visible(False) ax.grid(True, alpha=0.3, linestyle=\u0026#39;--\u0026#39;) # --- Plot 3: Difference --- ax = axes[2] data_delta = idata.posterior[\u0026#39;delta\u0026#39;].values.ravel() ax.hist(data_delta, bins=50, density=True, color=colors[2], alpha=0.7, edgecolor=\u0026#39;black\u0026#39;, linewidth=1.2, label=\u0026#34;Difference ($A - B$)\u0026#34;) ax.set_title(\u0026#34;Difference in Probabilities\u0026#34;, fontsize=14, loc=\u0026#39;left\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax.set_ylabel(\u0026#34;Density\u0026#34;) ax.set_xlabel(\u0026#34;Delta Value\u0026#34;) # Vertical Lines ax.axvline(a_default - b_default, color=\u0026#39;white\u0026#39;, linestyle=\u0026#39;:\u0026#39;, linewidth=2, label=\u0026#34;Expected Diff\u0026#34;) ax.axvline(0, color=\u0026#39;#FF4444\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=2, label=\u0026#34;Zero Difference\u0026#34;) ax.legend(loc=\u0026#39;upper right\u0026#39;, frameon=False) ax.spines[\u0026#39;top\u0026#39;].set_visible(False) ax.spines[\u0026#39;right\u0026#39;].set_visible(False) ax.grid(True, alpha=0.3, linestyle=\u0026#39;--\u0026#39;) # --- Saving with Transparent Background --- # The magic argument is transparent=True + bbox_inches=\u0026#39;tight\u0026#39; cuts off extra whitespace around the labels # plt.savefig(\u0026#39;dark_bayes_plot_transparent.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;, transparent=True) # to evaluate probability (idata.posterior[\u0026#39;delta\u0026#39;].values \u0026gt; 0).mean() plt.show() Figure 1. Probability of superiority: 83.2%\rüèóÔ∏è Part 2: Hierarchical Models (The \u0026ldquo;Robin Hood\u0026rdquo; Approach) When you run many related experiments or compare several variants in the same domain, hierarchical (multilevel) models are a practical way to borrow strength across groups. They reduce variance for small groups (shrinkage) and improve estimation stability. Below is a compact PyMC implementation that models group probabilities as draws from a shared Beta(a, b) prior.\nThis pattern is useful for dashboarding many A/B results together or for pooling information when sample sizes vary across tests.\nWhat is hierarchical models In a hierarchical (or multilevel) model, you assume that the parameters for each group are related and drawn from a common, overarching distribution. This shared distribution is governed by hyper-parameters.\nAll group-level parameters are drawn from a single, shared distribution defined by hyper-parameters a and b. For example each test\u0026rsquo;s probability is drawn from a shared Beta(a,b), where a and b are themselves parameters to be estimated.\nThe estimate for any single group is influenced both by its own data and the data from all other groups (via the shared a and b). This is called partial pooling, Say a and b are estimated to best fit all test data, and then every test\u0026rsquo;s probability is pulled slightly toward the overall average defined by a and b.\nEstimates for small groups are pulled toward the average (a process called shrinkage), leading to more stable, less extreme estimates\nWhich distribution may be used? Non-Informative Prior for Beta Hyperparameters: $$p(a,b) \\propto (a+b)^{-5/2}$$\nSimplification or approximation of the Jeffreys Prior for the hyper-parameters of the Beta distribution, which is used as a conjugate prior for binomial or Bernoulli likelihoods (common in multiple testing models, e.g., estimating the probability of a true null hypothesis)\nWhen $a$ and $b$ are the shape parameters of Beta distribution, the actual Jeffreys Prior is defined by the Fisher Information matrix:\n$$p(a,b) \\propto \\sqrt{\\det(\\mathbf{I}(a,b))}$$\nThe determinant of the Fisher Information matrix for the Beta distribution\u0026rsquo;s parameters is a complex function involving the trigamma function ($\\psi\u0026rsquo;$). Specifically, a known simplification used in some computational Bayesian contexts is related to the mean and total effective count of the Beta distribution.\nThe term $\\tau = a+b$ is often interpreted as the total effective sample size (or precision) of the Beta distribution. The exponent $-5/2$ is a specific value that results from one of the approximations designed to make the prior less influential on the posterior, often for the standard deviation or variance of the underlying distribution.\nExample # --- 1. Data Definition --- trials = np.array([842, 854, 862, 821, 839]) successes = np.array([27, 47, 69, 52, 35]) N_GROUPS = len(trials) # Constraint for Beta parameters (a and b must be \u0026gt; 0) ALPHA_MIN = 0.01 with pm.Model() as hierarchical_model: # --- 2. Hyper-parameter Priors (a and b) --- # The Beta shape parameters a and b must be positive. # We define them with a minimally informative uniform prior. a = pm.Uniform(\u0026#34;a\u0026#34;, lower=ALPHA_MIN, upper=100) b = pm.Uniform(\u0026#34;b\u0026#34;, lower=ALPHA_MIN, upper=100) # --- 3. Custom Precision Prior (pm.Potential) --- # The original prior was: log p(a, b) = log((a+b)^-2.5) = -2.5 * log(a + b) PRIOR_EXPONENT = -2.5 # Use pm.Potential to add the custom log-prior term to the model\u0026#39;s log(P) log_precision_prior = PRIOR_EXPONENT * np.log(a + b) pm.Potential(\u0026#34;beta_precision_potential\u0026#34;, log_precision_prior) # --- 4. Group-level Prior (occurrences) --- # \u0026#39;occurrences\u0026#39; is the probability for each group # drawn from a Beta distribution defined by the hyper-parameters a and b. occurrences = pm.Beta(\u0026#34;occurrences\u0026#34;, alpha=a, beta=b, shape=N_GROUPS) # --- 5. Likelihood (l_obs) --- # The observed successes follow a Binomial distribution. likelihood = pm.Binomial(\u0026#34;likelihood\u0026#34;, n=trials, p=occurrences, observed=successes) # --- 6. Sampling --- # Sampling is now done with pm.sample() # 5000 draws, 1000 tune (burn-in) idata = pm.sample(draws=1000, tune=1000, cores=-1, chains=2, random_seed=13) # To view the results: print(pm.summary(idata)) Code\rimport warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;, category=UserWarning) # 1. Activate Dark Mode plt.style.use(\u0026#39;dark_background\u0026#39;) # 2. Use the figure-level function sns.displot g = sns.displot( idata.posterior.occurrences[0, :, :].values, kind=\u0026#39;kde\u0026#39;, # Use KDE for a smoother distribution line color=\u0026#39;#00FFFF\u0026#39;, # Neon Cyan for high contrast fill=True, alpha=0.6, height=5, aspect=1.5 # Set figure size/ratio ) # 3. Apply final aesthetic touches to the single axis ax = g.ax ax.set_title(\u0026#34;Distribution of Occurrences\u0026#34;, fontsize=16, loc=\u0026#39;left\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, pad=20) ax.set_xlabel(\u0026#34;Occurrences Count\u0026#34;, fontsize=12) ax.set_ylabel(\u0026#34;Density\u0026#34;, fontsize=12) # Ensure grid lines are subtle ax.grid(True, alpha=0.3, linestyle=\u0026#39;--\u0026#39;) # 4. Remove top and right spines ax.spines[\u0026#39;top\u0026#39;].set_visible(False) ax.spines[\u0026#39;right\u0026#39;].set_visible(False) plt.show() Figure 2. Posterior probabilities in Hierarchical Model\rCode\rplt.style.use(\u0026#39;dark_background\u0026#39;) diff_1_vs_4 = (idata.posterior.occurrences[:, :, 1] - idata.posterior.occurrences[:, :, 4]).values.ravel() prob_v1_gt_v4 = (diff_1_vs_4 \u0026gt; 0).mean() # Setup Figure and Axis fig, ax = plt.subplots(figsize=(10, 5)) # Define color (Neon Magenta) neon_color = \u0026#39;#FF00FF\u0026#39; # 1. Create the KDE plot sns.kdeplot( diff_1_vs_4, ax=ax, fill=True, color=neon_color, alpha=0.6, linewidth=2, # The label includes the P(V1 \u0026gt; V4) calculation label=f\u0026#34;P(V1 \u0026gt; V4)\u0026#34; ) # 2. Add Zero Line (Crucial for interpretation) # This white dashed line marks the threshold for the probability calculation ax.axvline(0, color=\u0026#39;white\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=1.5, label=\u0026#34;V1 = V4 (Zero Difference)\u0026#34;) # 3. Apply final aesthetic touches ax.set_title( f\u0026#34;Posterior Distribution of Difference: V1 vs. V4\u0026#34;, fontsize=16, loc=\u0026#39;left\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, pad=20 ) ax.set_xlabel(\u0026#34;V1 - V4 (Difference)\u0026#34;, fontsize=12) ax.set_ylabel(\u0026#34;Density\u0026#34;, fontsize=12) # Clean up ax.legend(loc=\u0026#39;upper left\u0026#39;, frameon=False, fontsize=10) ax.spines[\u0026#39;top\u0026#39;].set_visible(False) ax.spines[\u0026#39;right\u0026#39;].set_visible(False) ax.grid(True, alpha=0.3, linestyle=\u0026#39;--\u0026#39;) plt.show() Figure 3. Probability of superiority: 88.7%\r‚ö° Part 3: The Need for Speed (Analytic Bayesian Solutions) MCMC is great, but sometimes you need speed. For the math nerds among us, the Beta-Binomial conjugacy is pure magic. ‚ú®\nIf you have a Beta prior and Binomial data, the posterior is \u0026hellip; drumroll \u0026hellip; just another Beta distribution! No complex sampling required - just simple arithmetic. Say there is $Beta(\\alpha, \\beta)$ prior and $k$ successes in $n$ trials, the posterior is $Beta(\\alpha + k, \\beta + n - k)$. This gives us closed-form solutions for the Probability of Superiority (PoS) instantly.\nSay we have 10 heads from ten coin flips, what is the probability to get get a head in the next flip?\nUsing Bayesian approach, where\n$ $ - hypothesis, $\\mathcal{D}$ - data\n$P(\\mathcal{H}) - prior$\n$P( | ) - likelihood $\n$P( | ) - posterior $\n$$ P(\\mathcal{H} | \\mathcal{D}) \\propto P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H}) $$\nIt can be shown that if\n$$ P(\\mathcal{H}) = {Beta}(p; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1} $$\n$$ P(\\mathcal{D} | \\mathcal{H}) = {Binom}(p; k, n) = C_{n}^{k} p^{k} (1-p)^{n-k} $$\nThen\nProof $$ P(\\mathcal{H} | \\mathcal{D}) = \\frac{P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H})}{P(\\mathcal{D})} $$\n$$ P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H}) = \\biggl ( C_{n}^{k} \\cdot p^{k} (1-p)^{n-k} \\biggr ) \\cdot \\biggl ( \\mathbf{\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}} \\cdot p^{\\alpha-1} (1-p)^{\\beta-1} \\biggr ) = C \\cdot p^{\\alpha+k-1}(1-p)^{\\beta+n-k-1} $$\n$$ P(\\mathcal{D}) = \\int_{0}^{1} P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H}) dp = C \\cdot \\int_{0}^{1} p^{\\alpha+k-1} (1-p)^{\\beta+n-k-1} dp = C \\cdot B(\\alpha+k, \\beta+n-k)$$\nConsequently combining those two equations, The Binomial Constant and the Prior Beta Constant are completely canceled out\n$$ P(\\mathcal{H} | \\mathcal{D}) = \\frac{1}{B(\\alpha+k, \\beta+n-k)} p^{\\alpha+k-1}(1-p)^{\\beta+n-k-1} = {Beta}(p; \\alpha + k, \\beta + n - k) $$\nEnd of Proof\nIn case of non-informative prior $Beta(p; 1, 1) $ what basically given Uniform distribution of the prior - Beta function may be presented with a short binomial coefficient formula\n$$ B(k+1, n-k+1) = \\frac{–ì(k+1)–ì(n-k+1)}{–ì(n+2)} = \\frac{k!(n-k)!}{(n+1)!} = \\frac{1}{(n+1); C_{n}^{k}} $$\nIn our case the posterior distribution is $ {Beta}(p; k + 1, n - k + 1) $\nHence we can build the predictive interval using this Beta distribution moments:\n$$ \\mu = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{k+1}{n+2} $$\nThis formula for $ $ is also used as the Laplace sequence rule, which requires adding one positive and one negative observation to estimate the posterior probability distribution for a random sample.\nAdding second moment\n$$ \\sigma^2 = {\\frac {\\alpha \\beta }{(\\alpha +\\beta )^{2}(\\alpha +\\beta +1)}} = \\frac{(k+1)(n-k+1)}{(n+2)^2(n+3)} $$\nmu = 11 / 12 sigma = (11 / 12 ** 2 / 13) ** 0.5 print(f\u0026#34;Hence 2 sigma predictive interval for 10/10 successful flips is {mu:.2%} ¬± {2 * sigma :.2%}\u0026#34;) print(f\u0026#34;Alternatively as a pair of bounds: {mu - 2 * sigma:.2%} - {min(1, mu + 2 * sigma):.2%}\u0026#34;) Hence 2 sigma predictive interval for 10/10 successful flips is 91.67% ¬± 15.33%\rAlternatively as a pair of bounds: 76.34% - 100.00%\rBut it\u0026rsquo;s not a Normal distribution, so it\u0026rsquo;s not 95% confidence interval, we need to take Beta distribution quantile instead or calculate it precisely. It resembles the approximation that we get using normal distribution quantiles.\nl, r = sts.beta.ppf([0.05, 1.0], a=11, b=1) print(f\u0026#34;Beta predictive interval {l:.2%} - {r:.2%}\u0026#34;) Beta predictive interval 76.16% - 100.00%\rAnother way is get that number analytically from the integral equation that fully coincides with the value from stats package.\n$$ \\int_{p_{crit}}^{1} (n+1) \\cdot C_n^n \\cdot p^n (1-p)^0 dp = 0.95 $$\n$$ p_{crit} = \\sqrt[n + 1]{0.05}$$\nprint(f\u0026#34;What makes it an easy computation: p is from {0.05 ** (1/11):.2%} - 100.00%\u0026#34;) What makes it an easy computation: p is from 76.16% - 100.00%\rCriterion that is used for analytical model decision making in A/B experiment\n$$ P(\\lambda_B \u0026gt; \\lambda_A) = \\int_{p_B \u0026gt; p_A} P(p_A, p_B | \\text{Data}) , dp_A , dp_B = \\sum_{i=0}^{\\alpha_B-1} \\frac{B(\\alpha_A+i, \\beta_A+\\beta_B)}{(\\beta_B+i)B(1+i, \\beta_B)B(\\alpha_A, \\beta_A)} $$\nThe formula is the result of applying a well-known mathematical identity that allows the cumulative probability of one Beta variable being less than another Beta variable to be expressed as a finite sum of terms involving the Beta function, rather than requiring complex numerical integration. This is why this formula is computationally efficient and preferred for exact Bayesian A/B calculations.\nRule of three: when no successes are observed The rule of three is used to provide a simple way of stating an approximate 95% confidence interval in the special case that no successes have been observed - $(0, 3/n)$, alternatively by symmetry, in case of only successes $(1 - 3/n, 1) $.\nOn the other hand mathematically, if all conversions are zero, then we simply may build an equation for upper bound\n$(1-p)^n \\geq \\alpha$, where $\\alpha = .05$ and hence $n \\leq log_{.95}(.05)$\nFor example - how many trials needed to challenge the null hypothesis that the success probability is zero?\nprint(\u0026#34;Approximate N:\u0026#34;, int(3 / 0.05)) print(\u0026#34;Exact N:\u0026#34;, 1 + int((np.log(0.05) / np.log(0.95)))) Approximate N: 60\rExact N: 59\rQuick Tip for Zero Successes: If you launch a test and get zero conversions, don\u0026rsquo;t panic. Use the Rule of Three: your approximate 95% upper bound is simply 3/n.¬†It\u0026rsquo;s a \u0026ldquo;pretty decent\u0026rdquo; (and fast) estimate without needing a calculator.\nüò± Part 4: The Plot Twist (Peeking) Here is the controversial bit: Bayesian A/B testing is NOT immune to peeking. The Myth: \u0026ldquo;I can check my Bayesian results whenever I want, and it\u0026rsquo;s always valid!\u0026rdquo; The Reality: Mathematically, the posterior is valid. BUT, if you use a fixed rule like \u0026ldquo;Stop when Probability \u0026gt; 95%,\u0026rdquo; you will inflate your False Positive Rate over time. You are essentially fishing for significance. If you stop early just because you crossed a line, you are falling into the same trap as frequentists, but but first things first.\nTo compare stopping rules and power, the notebook includes a Monte Carlo harness. It simulates repeated experiments, applies frequentist sequential z-tests and several Bayesian stopping rules (naive PoS threshold, expected-loss stopping (OLF), and HDI \u0026amp; PoS combinations), and compares false positive rates and average stopping sample sizes.\nCode\rfrom typing import Callable n_iterations = 1000 def min_sample_size(mde, mu, sigma, alpha=0.05, power=0.80) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Defines superiority one-side z-test sample size Args: mde: Relative uplift mu: Expected Value sigma: Square root of variance alpha: False Positive Rate, default = 0.05 power: Experiment power, default = 0.80 Returns: Required sample size to achieve the power \u0026#34;\u0026#34;\u0026#34; effect_size = abs(mde) * mu / sigma return int(((sts.norm.ppf(1 - alpha) + sts.norm.ppf(power)) / effect_size) ** 2) def stops_at(is_significant: np.ndarray, sample_size: np.ndarray) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Determines the stopping sample size. This function identifies the first instance where the input condition is True and returns the corresponding sample size. Args: is_significant: A boolean array of the stop condition for each size sample_size: An array of sample sizes. Returns: The stopping sample size. Example: \u0026gt;\u0026gt;\u0026gt; stops_at([False, False, True, True], [50, 100, 150, 200]) 150 \u0026#34;\u0026#34;\u0026#34; if len(is_significant) != len(sample_size): raise ValueError(\u0026#34;Input arrays must have the same length.\u0026#34;) w = np.where(is_significant)[0] return np.nan if len(w) == 0 else sample_size[w[0]] def monte_carlo( bayesian_stop_rule, effect_size: float=0.10, aa_test: bool=True, alpha: float=0.05, peeks: int = 1, ) -\u0026gt; None: result = { \u0026#39;Frequentist\u0026#39;: [], \u0026#39;Bayesian\u0026#39;: [], } p = 0.20 sigma = (p * (1 - p)) ** 0.5 relative_effect = 0 if aa_test else effect_size N = min_sample_size(mde=effect_size, mu=p, sigma=sigma, alpha=alpha) n = int(N / peeks) print(f\u0026#34;Running {n_iterations} simulations with total sample size {N} that is achieved in {peeks} iterations of {n} size each\u0026#34;) for seed in tqdm(range(n_iterations)): rng = np.random.default_rng(seed) binomial_samples = rng.binomial(n=n, p=p*(1+relative_effect), size=peeks) sizes = np.arange(n, N + 1, n) conversions = np.cumsum(binomial_samples) z_scores = [(success / trials - p) / np.sqrt(sigma ** 2 / trials) for success, trials in zip(conversions, sizes)] is_prob_high_enough = [ bayesian_stop_rule( success=success, trials=trials, alpha=alpha, p=p, effect_size=effect_size ) for success, trials in zip(conversions, sizes) ] result[\u0026#39;Frequentist\u0026#39;].append(stops_at(z_scores \u0026gt; sts.norm.ppf(1 - alpha), sizes)) result[\u0026#39;Bayesian\u0026#39;].append(stops_at(is_prob_high_enough, sizes)) result = \u0026#34;\\n\u0026#34;.join([ f\u0026#34;Frequentist Rejected Rate: {np.mean(~np.isnan(result[\u0026#39;Frequentist\u0026#39;]))}\u0026#34;, f\u0026#34;Frequentist Required Sample Size: {int(np.nanmean(result[\u0026#39;Frequentist\u0026#39;]))}\u0026#34;, f\u0026#34;Bayesian Rejected Rate: {np.mean(~np.isnan(result[\u0026#39;Bayesian\u0026#39;]))}\u0026#34;, f\u0026#34;Bayesian Required Sample Size: {int(np.nanmean(result[\u0026#39;Bayesian\u0026#39;]))}\u0026#34;, ]) print(result) return def POS(success: int, trials: int, alpha: float, p: float, **kwargs) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Probability of Superiority decision rule \u0026#34;\u0026#34;\u0026#34; return sts.beta.cdf(p, a = 1 + success, b = 1 + trials - success) \u0026lt; alpha Correctness monte_carlo(bayesian_stop_rule=POS, peeks=1, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 1 iterations of 2473 size each\rFrequentist Rejected Rate: 0.06\rFrequentist Required Sample Size: 2473\rBayesian Rejected Rate: 0.06\rBayesian Required Sample Size: 2473\rmonte_carlo(bayesian_stop_rule=POS, peeks=5, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each\rFrequentist Rejected Rate: 0.126\rFrequentist Required Sample Size: 1015\rBayesian Rejected Rate: 0.126\rBayesian Required Sample Size: 1015\rA/B design power monte_carlo(bayesian_stop_rule=POS, peeks=5, aa_test=False) Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each\rFrequentist Rejected Rate: 0.855\rFrequentist Required Sample Size: 1146\rBayesian Rejected Rate: 0.855\rBayesian Required Sample Size: 1146\rThat is a crucial observation and it points to a common misunderstanding about Bayesian A/B testing:\nNo, the standard Bayesian approach does not handle peeking (optional stopping) correctly by default if your goal is to control the frequentist Type I Error Rate (False Positive Rate).\nWhile the Bayesian interpretation of results remains valid at any time, using a fixed threshold (e.g., stopping when $P(A \u0026gt; B) \u0026gt; 95%$) and checking repeatedly will lead to an inflated False Positive Rate over many hypothetical experiments, just like in the frequentist approach.\nWhat is NOT Affected (The Bayesian Advantage) The Posterior Distribution and the Probability of Superiority \u0026mdash; is always valid, regardless of when you look at the data.\nWhat IS Affected (The Peeking Problem) The problem arises when you use a fixed decision rule (like the $95%$ threshold) to stop the test prematurely, based on the outcome.\nThe Myth: The common claim that \u0026ldquo;Bayesian testing is immune to peeking\u0026rdquo; is overstated. It is only immune in the sense that the posterior is always mathematically correct. It is not immune in the sense that it prevents the inflation of the frequentist Type I Error Rate when using a simple, fixed stopping threshold\nüõ°Ô∏è Part 5: How to Peek Safely How Bayesian Methods Truly Handle Peeking To safely peek and stop early in a Bayesian framework, you need to base your decision on a metric that incorporates the cost of a wrong decision, not just the probability of a difference.\nThe correct Bayesian decision procedure is to stop when:\nExpected Loss (EL) is Minimized: You stop the test when the Expected Loss of choosing the suboptimal variant falls below a commercially acceptable threshold $\\epsilon$. This naturally accounts for uncertainty.13 If the posterior distributions are still wide (high uncertainty), the loss will be high, and you won\u0026rsquo;t stop. $$ E[L](p_a \u0026gt; p_b) = \\int_0^1\\int_0^1L(p_a,p_b, p_a \u0026gt; p_b)P(p_a|a,b,n_a,k_a)P(p_b|a,b,n_b,k_b)dp_adp_b $$\nSequential Designs (Like Multi-Armed Bandits): Techniques like Thompson Sampling are inherently Bayesian and sequential. They don\u0026rsquo;t have a stopping rule based on error rates; they simply choose the best variant to show next based on the posterior, which naturally directs more traffic to the likely winner, making the experiment efficient without needing a fixed sample size. Example of Loss function applied Define the Opportunity Loss Function $L(p_A, p_B)$, which is the regret you incur by choosing a variant that is not the best.If we Choose B, the loss only happens if $p_A \u0026gt; p_B$: $$L(\\text{Choose B}) = \\max(0, p_A - p_B)$$ If we Choose A, the loss only happens if $p_B \u0026gt; p_A$: $$L(\\text{Choose A}) = \\max(0, p_B - p_A)$$\nUsing known properties and identities related to the Beta function, this complex double integral for Opportunity Loss function can be transformed into a closed-form summation:\n$$ EL_A = \\sum_{i=0}^{\\alpha_B-1} \\frac{\\alpha_A \\cdot B(\\alpha_A+i+1, \\beta_A+\\beta_B)}{\\beta_A \\cdot B(i+1, \\beta_B) \\cdot B(\\alpha_A, \\beta_A)} - \\sum_{i=0}^{\\alpha_B-1} \\frac{\\alpha_B \\cdot B(\\alpha_A+i, \\beta_A+\\beta_B+1)}{(\\beta_B+i) \\cdot B(i+1, \\beta_B) \\cdot B(\\alpha_A, \\beta_A)} $$\nThe Formula for one-sample test is against benchmark $\\lambda_0$:\n$$ EL_A = \\lambda_0 \\cdot I_{\\lambda_0}(\\alpha, \\beta) - \\frac{\\alpha}{\\alpha+\\beta} \\cdot I_{\\lambda_0}(\\alpha+1, \\beta) $$\nPython Implementation note that betainc $(\\alpha, \\beta, \\lambda)$ calculates $I_{\\lambda}(\\alpha, \\beta)$, that is an equivalent of sts.beta.cdf$(\\lambda, \\alpha, \\beta)$\nCode\rfrom scipy.special import betainc def calculate_opportunity_loss_one_sample( k: int, n: int, lambda_0: float, prior_alpha: int = 1, prior_beta: int = 1, ) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Calculates the Expected Loss of choosing the observed variant against a benchmark using the analytical formula. Absolute value of conversion loss. Non-informative conjugate prior is used by Default. Args: k (int): Observed successes (conversions). n (int): Observed trials (sizes). lambda_0 (float): The fixed conversion rate benchmark. prior_alpha (int): Prior alpha hyper-parameter. prior_beta (int): Prior beta hyper-parameter. Returns: float: The Expected Loss of choosing the observed variant. \u0026#34;\u0026#34;\u0026#34; # Calculate Posterior Parameters (alpha and beta) alpha = k + prior_alpha beta = n - k + prior_beta # --- Term 1: Probability of Loss --- # Calculates I_lambda_bench(alpha, beta) = P(lambda_obs \u0026lt; lambda_bench) # The term is: lambda_bench * P(lambda_obs \u0026lt; lambda_bench) term1 = lambda_0 * betainc(alpha, beta, lambda_0) # --- Term 2: Weighted Expected Value --- # The fraction part: alpha / (alpha + beta) is the mean of Beta(alpha, beta) term2 = alpha / (alpha + beta) * betainc(alpha + 1, beta, lambda_0) return term1 - term2 Frequentist: We must collect $N$ samples to have an $1-\\beta$ chance of detecting a $MDE$ difference with $\\alpha$ error.\nBayesian: We will stop the test when the average potential loss incurred by choosing the sub-optimal variant is less than $\\epsilon$ percentage points. By setting a small $\\epsilon$, you ensure that the test continues until the potential future regret (loss) is extremely low, thus ensuring a high degree of confidence in the final decision while retaining the ability to peek safely\nAdjust Monte Carlo procedure with another Bayesian stopping rule\nCode\rdef OLF(success: int, trials: int, p: float, effect_size: float, **kwargs) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Opportunity Loss Function stopping rule. Epsilon is usually set to a fraction of MDE\u0026#34;\u0026#34;\u0026#34; fraction = 1 / 100 epsilon = fraction * effect_size * p return calculate_opportunity_loss_one_sample(success, trials, lambda_0=p) \u0026lt; epsilon monte_carlo(bayesian_stop_rule=OLF, peeks=30, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 30 iterations of 82 size each\rFrequentist Rejected Rate: 0.249\rFrequentist Required Sample Size: 590\rBayesian Rejected Rate: 0.21\rBayesian Required Sample Size: 839\rSo, Loss Function doesn\u0026rsquo;t save from Peeking problem, it\u0026rsquo;s even more vulnerable than well-known p-value approach\nWe need another piece of the puzzle \u0026hellip;\nHere comes the sun üå§Ô∏è \u0026hellip; and HDI Highest Density Interval (sometimes called Highest Posterior Density Interval).\nDefinition: The $X%$ HDI is the narrowest interval that contains $X%$ of the probability mass of the posterior distribution.\nPurpose: It is the Bayesian equivalent of the frequentist Confidence Interval (CI), but unlike the CI, you can state that there is an $X%$ probability that the true parameter value (e.g., the true conversion rate) lies within the HDI.\nThe width of the HDI is simply (Upper Bound - Lower Bound). It is a direct and intuitive measure of the remaining uncertainty. A wide HDI means your posterior is flat and uncertain; a narrow HDI means your posterior is sharply peaked and confident.\nCode\rimport arviz as az def calculate_beta_hdi_width(alpha: float, beta: float, hdi_prob=0.95, num_samples=10_000) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Calculates the Highest Density Interval (HDI) for a Beta distribution using Monte Carlo sampling and the arviz library. The calculation of the HDI is an iterative process that must find the interval boundary points where the probability density is equal, while the area between the points equals the target probability. Since Beta distribution is generally not symmetrical, the HDI bounds are not the same as the quantiles, which is why a specialized function is needed. Args: alpha (float): The posterior alpha parameter (k_obs + prior_alpha). beta (float): The posterior beta parameter (n_obs - k_obs + prior_beta). hdi_prob (float): The target probability mass (e.g., 0.95 for 95% HDI). num_samples (int): Number of samples to draw for Monte Carlo calculation. Returns: tuple: (lower_bound, upper_bound, width) \u0026#34;\u0026#34;\u0026#34; posterior_samples = sts.beta.rvs(a=alpha, b=beta, size=num_samples) # designed to work on posterior samples hdi_interval = az.hdi(posterior_samples, hdi_prob=hdi_prob) lower_bound = hdi_interval[0] upper_bound = hdi_interval[1] return upper_bound - lower_bound Let\u0026rsquo;s update Monte Carlo once again with a combination of PoS and HDI stopping what represent business and statistical robustness respectively, shall we? - Note that HDI density affects the inference vastly and you\u0026rsquo;d better experiment to pick up a good for for the data.\nCode\rprint(f\u0026#34;Width is multiplied by {round(calculate_beta_hdi_width(100, 100, .99) / calculate_beta_hdi_width(100, 100, 0.8), 2)} when increase required density from 0.8 to 0.99\u0026#34;) Width is multiplied by 1.98 when increase required density from 0.8 to 0.99\rCode\rdef HDI(success: int, trials: int, alpha: float, p: float, effect_size: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; PoS combined with 95% HDI stopping rule \u0026#34;\u0026#34;\u0026#34; return ( # checks if 95% of posterior distribution is narrow enough and lays in ¬± MDE (calculate_beta_hdi_width(1 + success, 1 + trials - success) \u0026lt; 2 * effect_size * p) # checks if posterior distribution by 95% chance better and POS(success, trials, alpha, p) ) Correctness monte_carlo(bayesian_stop_rule=OLF, peeks=1, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 1 iterations of 2473 size each\rFrequentist Rejected Rate: 0.06\rFrequentist Required Sample Size: 2473\rBayesian Rejected Rate: 0.069\rBayesian Required Sample Size: 2473\rmonte_carlo(bayesian_stop_rule=HDI, peeks=10, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 10 iterations of 247 size each\rFrequentist Rejected Rate: 0.193\rFrequentist Required Sample Size: 876\rBayesian Rejected Rate: 0.1\rBayesian Required Sample Size: 1924\rPower monte_carlo(bayesian_stop_rule=HDI, peeks=5, aa_test=False) Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each\rFrequentist Rejected Rate: 0.855\rFrequentist Required Sample Size: 1146\rBayesian Rejected Rate: 0.818\rBayesian Required Sample Size: 2035\rHDI accompanied by Probability of Superiority is a good criterion, although very strict if you increase the required density for HDI from 80% above 95% and it hence requires bigger sample size than Frequentist approach.\nCombination of HDI and PoS checks makes the criterion less sensitive to peeking, however it\u0026rsquo;s yet not fully immune.\nStopping Rules Overview This table breaks down three common criteria used in Bayesian A/B testing, highlighting their function and their robustness against peeking (stopping a test too early based on transient results).\nCriterion Function / Definition Robustness Against Peeking Probability of Superiority (PoS) Measures how often the posterior probability of $\\lambda_B$ is greater than $\\lambda_A$ (i.e., $P(\\lambda_B \u0026gt; \\lambda_A)$). Low Robustness. The threshold can be quickly and spuriously crossed by early noise or transient fluctuations. Expected Loss (EL) Measures the average Cost or Regret of selecting the inferior variant. High Robustness. Requires the posterior distribution to be tight enough that the potential loss (regret) is small, preventing premature stopping. HDI Width Measures the Precision or Uncertainty of the posterior distribution (e.g., the width of the 95% credible interval). High Robustness. Forces the test to continue until the uncertainty is low (the HDI is narrow), regardless of the posterior mean, ensuring adequate data collection. Conclusions \u0026amp; Practical Recommendations Use the posterior (and PoS) to interpret results, but prefer decision-theoretic stopping when making business choices: stop when the expected loss of choosing the sub-optimal variant is below a tolerated threshold. . Combine HDI (precision) with PoS (direction) for a conservative, safe stopping rule - but higher density HDI thresholds require larger samples. When many variants or small groups are present, hierarchical models provide safer estimates via partial pooling. If your goal is to guarantee frequentist properties (e.g., Type I control under peeking), design the sequential procedure explicitly: group sequential testing with alpha-spending function or always valid inference approach - Sequential Testing Guide will let you know all you need Further experiments to try:\nReplace uniform priors with domain-informed priors when available. Explore Thompson Sampling for continuous allocation instead of fixed-sample stopping. Visualize posterior trajectories and stopping-rule trade-offs across simulated peeks. Thanks for reading, feel free to fork this notebook and go forth, experiment with your own traffic and loss thresholds as well as react to the post below, and may your posteriors always be narrow!\n","permalink":"https://npodlozhniy.github.io/posts/bayesian-test/","summary":"Practical guide: PyMC examples, hierarchical models, loss functions, HDI, and safe stopping rules","title":"Bayesian A/B Test is NOT immune to peeking"},{"content":"\rIntro Sequential testing designs have gained significant popularity in AB testing due to their ability to potentially reduce the required sample size and experiment duration while maintaining statistical correctness. This approach allows for interim analyses of data as it accumulates, offering the possibility to stop the experiment early if a clear winner emerges, or if it becomes evident that the treatment effect is insufficient to justify continuing (stop for futility)\nIn this article, we shift our focus away from the theoretical intricacies of the problem and instead delve into a comprehensive exploration of available sequential testing solutions. We will discuss their implementations, compare their performance, and highlight their strengths and weaknesses. By examining these practical aspects, we aim to equip practitioners with the knowledge to make informed decisions when incorporating sequential testing into their AB testing workflows.\nApproach Among the variety of sequential testing designs, there are basically two broad families of algorithms: Group Sequential Testing (GST) and Always Valid Inference (AVI). These methods represent distinct philosophies in their handling of interim analyses and experiment stopping criteria..\nGST works with predefined interim analysis points and utilizes predetermined stopping boundaries to decide whether to stop the experiment at each stage.\nAVI allows continuous monitoring and providing valid confident intervals at any point, making it adaptable for uncertain experiment duration or analysis frequencies.\nThis article primarily focuses on these two techniques, providing an overview of their methodologies and practical implications.\nGroup Sequential Testing For group sequential testing there is a handy package in R and I will show below how to use it, although for those, who prefers Python due to any reason, whether it is the absence of an interpreter, infrastructure limitations or just personal preferences, there is no direct and popular alternative package, so I had to write it on my own and now ready to share with you after careful testing and benchmarking.\nGST in R If you\u0026rsquo;re ready to use R library there are two options: use R runtime directly or through rpy2 Python package. Both options are available for example within Google Colab environment.\nHere is an instance of rpy2 package inference within Colab Notebook, when you run R code from the Python interpreter using an extension.\niPython Notebook %load_ext rpy2.ipython %%R R.version.string install.packages(\u0026#34;ldbounds\u0026#34;) %%R library(ldbounds) ldBounds(t=seq(1/4, 1, 1/4), iuse=3, phi=1, alpha=0.05, sides=1)$upper.bounds [1] 2.241403 2.125078 2.018644 1.925452\rPython File The code above may be rewritten into a simple .py file as follows, you are to use created stats package as a plain Python package thereafter.\n‚ö†Ô∏è Caution\nIt will not work in Google Colab for instance, as it requires R installed in addition to Python.\nCode\rimport rpy2.robjects.packages as rpackages from rpy2.robjects.vectors import StrVector utils = rpackages.importr(\u0026#39;utils\u0026#39;) # select a mirror for R packages utils.chooseCRANmirror(ind=1) # R package names packnames = (\u0026#39;ldbounds\u0026#39;) # Selectively install what needs to be install. names_to_install = [ package for package in packnames if not rpackages.isinstalled(package) ] if len(names_to_install) \u0026gt; 0: utils.install_packages(StrVector(names_to_install)) stats = rpackages.importr(\u0026#39;ldbounds\u0026#39;) If R is not your cup of tea, or simply there is no option to run it within the scope of the production infrastructure, what is the most common limitation by the way, now begins exactly what you need.\nGST in Python There is quite popular yet inaccurate implementation powered by Zalando expan. It works without probability integration and mistakenly leverage alpha spending function as a critical value at each step, it\u0026rsquo;s common misunderstanding about alpha-spending function approach, there are number of implementations that do it in exact same way, and even world\u0026rsquo;s leading publication for data science, according to their own definition, sometimes makes the same mistake, for instance: Understanding of Group Sequential Testing published in Towards Data Science is good post for beginners, though alpha spending function\u0026rsquo;s application is misleading. As it will shown below that approach is statistically incorrect and so it\u0026rsquo;s highly recommended to avoid it.\nInstead I propose you to apply the new library seqabpy that is powerful and accurate and what is more important implemented according to the original papers Interim analysis: The alpha spending function approach by K. K. Gordon Lan and David L. DeMets (1983) and further related publications, you may find them all mentioned in Reference lines of the methods\u0026rsquo; docstrings, let\u0026rsquo;s take a look at the functionality\nCode\rimport numpy as np import pandas as pd from scipy.stats import norm seabpy provides Group Sequential Testing in a separate module - gatsby which name is an anagram of Group Sequential AB Testing in PYthon and below it\u0026rsquo;s shown why gatsby is often referred as ¬´The Great Gatsby¬ª\n#!pip install seqabpy from seqabpy import gatsby Lan-DeMets calculate_sequential_bounds function implements rigorous approach to calculate confidence bounds in one-sided GST. In addition to upper bounds, if beta is provided it calculates lower bounds to unlock the option of stopping the test for futility, maintaining provided Type II error rate. The algorithm is taken from the article Group sequential designs using both type I and type II error probability spending functions by Chang MN, Hwang I, Shih WJ. (1998)\ngatsby.calculate_sequential_bounds(np.linspace(1/10, 1, 10), alpha=0.05, beta=0.2) Sequential bounds algorithm to stop for futility converged to 0.00064 tolerance in 8 iterations using O'Brien-Fleming spending function.\r(array([-3.02102866, -1.41478896, -0.59632535, -0.05664366, 0.35069266,\r0.68203514, 0.96419224, 1.21185008, 1.43395402, 1.79496377]),\rarray([6.08789285, 4.22919942, 3.39632756, 2.90614903, 2.57897214,\r2.34174062, 2.15981329, 2.0146325 , 1.89528829, 1.79496377]))\rldBounds function returns the exact same numbers as R package and tailored to have similar interface, in both input and output.\nAs a subtle benefit it supports more spending functions, take a look at the docstring to know more details.\ngatsby.ldBounds(t=np.linspace(1/4, 1, 4), iuse=3, phi=1) {'time.points': array([0.25, 0.5 , 0.75, 1. ]),\r'alpha.spending': array([0.0125, 0.0125, 0.0125, 0.0125]),\r'overall.alpha': 0.05,\r'upper.bounds': array([2.24140273, 2.1251188 , 2.01870509, 1.92553052]),\r'nominal.alpha': array([0.0125 , 0.01678835, 0.02175894, 0.02708151])}\rIn case of calculation of upper bounds only the algorithm is faster, given that these boundaries shall be defined and fixed offline prior to the experiment start, it\u0026rsquo;s totally sensible performance, when in addition lower aka futility bounds are computed it takes more time\n%%time gatsby.ldBounds(t=np.linspace(1/10, 1, 10), alpha=0.1) CPU times: total: 7.73 s\rWall time: 7.75 s\r{'time.points': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\r'alpha.spending': array([1.97703624e-07, 2.34868089e-04, 2.43757240e-03, 6.62960181e-03,\r1.07070137e-02, 1.37029812e-02, 1.55891351e-02, 1.66134835e-02,\r1.70337597e-02, 1.70513868e-02]),\r'overall.alpha': 0.10000000000000009,\r'upper.bounds': array([5.07115563, 3.4973068 , 2.79510152, 2.38848818, 2.11870064,\r1.92350461, 1.77394818, 1.65463799, 1.55659472, 1.47418673]),\r'nominal.alpha': array([1.97703624e-07, 2.34990500e-04, 2.59417098e-03, 8.45892623e-03,\r1.70578873e-02, 2.72083541e-02, 3.80358613e-02, 4.89989759e-02,\r5.97833696e-02, 7.02156613e-02])}\rAn example of less popular Haybittle-Peto spending function usage\ngatsby.ldBounds(t=np.linspace(1/4, 1, 4), iuse=5) {'time.points': array([0.25, 0.5 , 0.75, 1. ]),\r'alpha.spending': array([0.0013499, 0. , 0. , 0.0486501]),\r'overall.alpha': 0.05,\r'upper.bounds': array([3. , 3. , 3. , 1.63391418]),\r'nominal.alpha': array([0.0013499 , 0.0013499 , 0.0013499 , 0.05113844])}\rGST GST is the general function that accounts for various deviations in the experiment design, in other words if the peeking strategy is different the method adjusts the bounds to guarantee the valid statistical approach whenever it\u0026rsquo;s possible.\nIn particular it\u0026rsquo;s perfect to handle a few changed peeking points when the total number of peeking remains the same, and it works in a best possible way with under- and oversampling, in the latter case the procedure is not fully correct though as we will see in the simulations part.\nThe idea of implementation is taken from Group Sequential and Confirmatory Adaptive Designs in Clinical Trials by G. Wassmer and W. Brannath (2016)\nHere are a few examples, that shows how different peeking strategy affects the bounds:\ngatsby.ldBounds(t=np.array([0.3, 0.6, 1.0]), alpha=0.025) {'time.points': array([0.3, 0.6, 1. ]),\r'alpha.spending': array([4.27257874e-05, 3.76533752e-03, 2.11919367e-02]),\r'overall.alpha': 0.02499999999999991,\r'upper.bounds': array([3.92857254, 2.669972 , 1.98103004]),\r'nominal.alpha': array([4.27257874e-05, 3.79287858e-03, 2.37939526e-02])}\rin case of under-sampling the last upper bound is lower what reflects that all the rest of alpha volume is spent at this point in case of over-sampling the last upper bound is higher, what helps to control Type I error rate, after the expected sample size is reached gatsby.GST(actual=np.array([0.3, 0.6, 0.8]), expected=np.array([0.3, 0.6, 1]), alpha=0.025) array([3.92857254, 2.669972 , 1.96890411])\rgatsby.GST(actual=np.array([0.3, 0.6, 1.2]), expected=np.array([0.3, 0.6, 1]), alpha=0.025) array([3.92857254, 2.669972 , 1.98949242])\rUnder- and over- sampling may also happen in a more natural way, when a few peeking points are added or removed\ngatsby.ldBounds(t=np.array([0.3, 0.6, 0.8, 1.0]), alpha=0.025) {'time.points': array([0.3, 0.6, 0.8, 1. ]),\r'alpha.spending': array([4.27257874e-05, 3.76533752e-03, 8.40372704e-03, 1.27882097e-02]),\r'overall.alpha': 0.02499999999999991,\r'upper.bounds': array([3.92857254, 2.669972 , 2.28886308, 2.03074404]),\r'nominal.alpha': array([4.27257874e-05, 3.79287858e-03, 1.10436544e-02, 2.11404831e-02])}\rgatsby.GST(actual=np.array([0.3, 0.6, 0.8]), expected=np.array([0.3, 0.6, 0.8, 1]), alpha=0.025) array([3.92857254, 2.669972 , 2.15083427])\rgatsby.GST(actual=np.array([0.3, 0.6, 1, 1.2]), expected=np.array([0.3, 0.6, 1]), alpha=0.025) array([3.92857254, 2.669972 , 1.98102292, 2.0375539 ])\rGST also supports int as an input, if peeking point are distributed uniformly it\u0026rsquo;s what you should use in sake of convenience\ngatsby.GST(7, 7) array([5.05481268, 3.48557771, 2.78550934, 2.38021304, 2.1113425 ,\r1.9168349 , 1.76778516])\rWhile the method comes handy in most of scenarios, it doesn\u0026rsquo;t support all the possible deviations: the beginning of the expected and actual peeking strategies must be the same: so it\u0026rsquo;s either over- or under- sampling or the change in peeking points when their number remains equal\nNeedless to say that the application of GST and other functions mentioned above apparently is not limited to one-sided hypotheses, in order to test two-sided alternative: just set $\\alpha$ to half of the value, like 0.025 if you want to challenge two-sided hypothesis at 0.95 confidence level, and define lower bounds symmetrically about zero, so they would be the same in absolute values, but negative.\ngatsby.GST(actual=np.array([0.3, 0.6, 0.9, 1.2]), expected=np.array([0.3, 0.6, 0.8, 1]), alpha=0.025) # the following will not work # gatsby.GST(actual=np.array([0.3, 0.6]), expected=np.array([0.8, 1])) # gatsby.GST( # actual=np.array([0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1]), # expected=np.array([0.3, 0.6, 0.8, 1]) # ) array([3.92857254, 2.669972 , 2.30467653, 2.05129976])\rAlways Valid Inference While AVI is becoming increasingly popular in the field, bypassing GST, it\u0026rsquo;s worth noting that there are currently no widely adopted, comprehensive Python or R packages that focus solely on this approach.\nThere is one recent package savvi appeared this year, but it\u0026rsquo;s still in v0.. version and have not been yet fully acknowledged by the community. What is more it focuses only on the publications of Lindon et al.¬†from 2022 and 2024, while there are other notable authors like Zhao et al. and Howard et al. whose approach will be challenged in addition to Lindon\u0026rsquo;s work\nfrom seqabpy import gavi seqabpy provides Always Valid Inference functionality in gavi module where as of now, AlwaysValidInference is a main class that implements confidence intervals valid at any point. While intervals and namely their continuous comparison to the current z-score provides the apparatus that is just enough for practical decisions, p-values are to be released later as well, to complete experiment analysis picture.\nAlwaysValidInference an array of sample sizes when the peeking happens along with the metric variance and the result point difference. Multiple supported properties comprise different algorithms (the detailed description may be found in each docsrting) that return a boolean array indicating whether the null hypothesis is rejected in favour of one- or two- sided alternative for each size.\navi = gavi.AlwaysValidInference(size=np.arange(10, 100, 10), sigma2=1, estimate=1) GAVI is the method proposed by Howard et al.¬†and widely adopted in tech by Eppo\navi.GAVI(50) array([False, True, True, True, True, True, True, True, True])\rmSPRT is the approach proposed by M. Lindon in his article and is leveraged by Netflix\navi.mSPRT(0.08) array([False, False, True, True, True, True, True, True, True])\rStatSig_SPRT is the variation proposed by Zhao et al.¬†and as it comes from the name used currently by StatSig\navi.StatSig_SPRT() array([False, True, True, True, True, True, True, True, True])\rThe last and, this time, indeed least heavily criticized statsig_alpha_corrected_v1 approach, which was their first attempt to furnish their platform with a sequential testing framework. It\u0026rsquo;s mainly added for the reference to show how sequential testing must not work like\navi.statsig_alpha_corrected_v1(100) array([False, False, False, True, True, True, True, True, True])\rSimulations For those who have visited my blog before, there is nothing new in how we will conduct testing, it is good old Monte Carlo. For more details checkout my previous posts like Dunnett\u0026rsquo;s Correction for ABC testing\nWe will measure False and True positive rates for two kinds of the target metric: a continuous variable and a conversion. Furthermore we will learn how tolerant are different methods to under- and over- sampling.\nCode\r# Global simulation settings N = 500 alpha = 0.05 n_iterations = 100_000 def stops_at(is_significant: np.ndarray, sample_size: np.ndarray) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Determines the stopping sample size. This function identifies the first instance where the input condition is True and returns the corresponding sample size. Args: is_significant: A boolean array of the stop condition for each size sample_size: An array of sample sizes. Returns: The stopping sample size. Example: \u0026gt;\u0026gt;\u0026gt; detN([False, False, True, True], [50, 100, 150, 200]) 150 \u0026#34;\u0026#34;\u0026#34; if len(is_significant) != len(sample_size): raise ValueError(\u0026#34;Input arrays must have the same length.\u0026#34;) w = np.where(is_significant)[0] return None if len(w) == 0 else sample_size[w[0]] One thing about GST is that incredible freedom in spending function choice what makes it possible to experiment and find the best fit for your data. For demonstration purposes I suggest using Kim-DeMets spending function with different values of the power $\\phi$: the higher $\\phi$ the more strict the function is at the beginning of the experiment.\nCode\rgst_linear = gatsby.GST(actual=10, expected=10, iuse=3, phi=1, alpha=alpha) gst_quadratic = gatsby.GST(actual=10, expected=10, iuse=3, phi=2, alpha=alpha) gst_cubic = gatsby.GST(actual=10, expected=10, iuse=3, phi=3, alpha=alpha) You can play around the trade-off: would you like to spend more $\\alpha$ at the start, detecting faster if there are greater uplifts in you experiment group or to preserve the major part of alpha until the end keeping maximum power to reject the hypothesis when the expected sample size is reached.\nüí° Tip\nIf the title or the legend items are not visible to you - double click to one of legend items and it will make the chart rendered properly. It may happen due to LaTeX usage.\nCode\rimport plotly.express as px import plotly.graph_objs as go def hex2rgba(hex, alpha): \u0026#34;\u0026#34;\u0026#34; Convert plotly hex colors to rgb and enables transparency adjustment \u0026#34;\u0026#34;\u0026#34; col_hex = hex.lstrip(\u0026#39;#\u0026#39;) col_rgb = tuple(int(col_hex[i : i + 2], 16) for i in (0, 2, 4)) col_rgb += (alpha,) return \u0026#39;rgba\u0026#39; + str(col_rgb) def get_new_color(colors): while True: for color in colors: yield color colors_list = px.colors.qualitative.Plotly rgba_colors = [hex2rgba(color, alpha=0.5) for color in colors_list] palette = get_new_color(rgba_colors) def add_chart(figure, data, title=None): x = np.arange(1, len(data) + 1) / len(data) color = next(palette) figure.add_trace( go.Scatter( name=title, x=x, y=data, mode=\u0026#39;lines\u0026#39;, line=dict(color=color, width=4, dash=\u0026#39;solid\u0026#39;), hovertemplate=\u0026#34;%{y:.3f}\u0026#34; ), ) figure = go.Figure() add_chart(figure, gst_linear, r\u0026#34;$\\text{Linear: } \\phi = 1$\u0026#34;) add_chart(figure, gst_quadratic, r\u0026#34;$\\text{Quadratic: } \\phi = 2$\u0026#34;) add_chart(figure, gst_cubic, r\u0026#34;$\\text{Cubic: } \\phi = 3$\u0026#34;) figure.update_xaxes( title_text=\u0026#34;Peeking moments\u0026#34; ) figure.update_layout( yaxis_title=\u0026#34;Critical value for z-score\u0026#34;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: r\u0026#34;$\\text{Kim-DeMets spending function: } \\alpha \\cdot t^{\\phi} \\text{ differences}$\u0026#34;, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) figure.write_json(\u0026#34;alpha-spending-functions-comparison.json\u0026#34;) figure.show() Expan Flaw It\u0026rsquo;s kind of a hot take, remember I promised to provide evidence that expan way to determine boundaries is inaccurate, so here is a quick proof: the code is taken with no changes from their GitHub: zalando/expan/early_stopping\nCode\rfrom statsmodels.stats.proportion import proportion_confint def sample_size(x): \u0026#34;\u0026#34;\u0026#34; Calculates valid sample size given the data. :param x: sample to calculate the sample size :type x: pd.Series or list (array-like) :return: sample size of the sample excluding nans :rtype: int \u0026#34;\u0026#34;\u0026#34; # cast into a dummy numpy array to infer the dtype x_as_array = np.array(x) if np.issubdtype(x_as_array.dtype, np.number): _x = np.array(x, dtype=float) x_nan = np.isnan(_x).sum() # assuming categorical sample elif isinstance(x, pd.core.series.Series): x_nan = x.str.contains(\u0026#39;NA\u0026#39;).sum() else: x_nan = list(x).count(\u0026#39;NA\u0026#39;) return int(len(x) - x_nan) def obrien_fleming(information_fraction, alpha=0.05): \u0026#34;\u0026#34;\u0026#34; Calculate an approximation of the O\u0026#39;Brien-Fleming alpha spending function. :param information_fraction: share of the information amount at the point of evaluation, e.g. the share of the maximum sample size :type information_fraction: float :param alpha: type-I error rate :type alpha: float :return: redistributed alpha value at the time point with the given information fraction :rtype: float \u0026#34;\u0026#34;\u0026#34; return (1 - norm.cdf(norm.ppf(1 - alpha / 2) / np.sqrt(information_fraction))) * 2 def group_sequential(x, y, spending_function=\u0026#39;obrien_fleming\u0026#39;, estimated_sample_size=None, alpha=0.05, cap=8): \u0026#34;\u0026#34;\u0026#34; Group sequential method to determine whether to stop early. :param x: sample of a treatment group :type x: pd.Series or array-like :param y: sample of a control group :type y: pd.Series or array-like :param spending_function: name of the alpha spending function, currently supports only \u0026#39;obrien_fleming\u0026#39;. :type spending_function: str :param estimated_sample_size: sample size to be achieved towards the end of experiment :type estimated_sample_size: int :param alpha: type-I error rate :type alpha: float :param cap: upper bound of the adapted z-score :type cap: int :return: results of type EarlyStoppingTestStatistics :rtype: EarlyStoppingTestStatistics \u0026#34;\u0026#34;\u0026#34; # Coercing missing values to right format _x = np.array(x, dtype=float) _y = np.array(y, dtype=float) n_x = sample_size(_x) n_y = sample_size(_y) if not estimated_sample_size: information_fraction = 1.0 else: information_fraction = min(1.0, (n_x + n_y) / estimated_sample_size) # alpha spending function if spending_function in (\u0026#39;obrien_fleming\u0026#39;): func = eval(spending_function) else: raise NotImplementedError alpha_new = func(information_fraction, alpha=alpha) # calculate the z-score bound bound = norm.ppf(1 - alpha_new / 2) # replace potential inf with an upper bound if bound == np.inf: bound = cap mu_x = np.nanmean(_x) mu_y = np.nanmean(_y) sigma_x = np.nanstd(_x) sigma_y = np.nanstd(_y) z = (mu_x - mu_y) / np.sqrt(sigma_x ** 2 / n_x + sigma_y ** 2 / n_y) if z \u0026gt; bound or z \u0026lt; -bound: stop = True else: stop = False return stop fpr = 0 for r in range(n_iterations): x = np.random.normal(1, 1, N) y = np.random.normal(1, 1, N) for current_size in np.linspace(N/10, N, 10).astype(int): stopping = group_sequential(x[:current_size], y[:current_size], estimated_sample_size=2*N, alpha=0.05) if stopping: fpr += 1 break l, r = proportion_confint(count=fpr, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#34;false positives: {fpr/n_iterations:.3f} ¬± {(r - l) / 2:.3f} is significantly higher than {alpha}\u0026#34;) false positives: 0.070 ¬± 0.001 is significantly higher than 0.05\rSo, as it was said above, it doesn\u0026rsquo;t control FPR as it should according to Group Sequential Testing problem design and hence this myth of the direct application of alpha spending function have to be dispelled: it doesn\u0026rsquo;t work this way and further you will see that it\u0026rsquo;s not way better than custom ad-hoc corrections.\n‚ö†Ô∏è Warning\nPlease, do not use expan for sequential testing as it inflates Type I error rate.\nMonte Carlo Code\rfrom collections import defaultdict def monte_carlo( metric: str=\u0026#34;normal\u0026#34;, sampling: str=\u0026#34;accurate\u0026#34;, effect_size: float=0.10, aa_test: bool=True, N: int = N, ) -\u0026gt; pd.DataFrame: result = defaultdict(list) eff = 0 if aa_test else effect_size if metric == \u0026#34;normal\u0026#34;: mu, sigma = 1, 1 else: p = 0.10 sigma = (p * (1 - p)) ** 0.5 # for bernoulli rv sigma is less than for normal # so it\u0026#39;s better to increase N to get similar power N *= int((sigma / p) ** 2) for _ in range(n_iterations): if metric == \u0026#34;normal\u0026#34;: x = np.random.normal(mu, sigma, N) y = np.random.normal(mu+eff, sigma, N) else: x = np.random.choice(a=[0, 1], size=N, replace=True, p=[1 - p, p]) y = np.random.choice(a=[0, 1], size=N, replace=True, p=[1 - p*(1+eff), p*(1+eff)]) size = np.arange(1, N + 1) diff = (np.cumsum(y) / size) - (np.cumsum(x) / size) test = gavi.AlwaysValidInference(size=size, sigma2=sigma**2, estimate=diff, alpha=alpha) itermittent_analyses = np.linspace(N/10, N, 10).astype(int) - 1 z_score = diff[itermittent_analyses] / np.sqrt(2 * sigma ** 2 / size[itermittent_analyses]) result[\u0026#39;No_Seq\u0026#39;].append(N if z_score[-1] \u0026gt; norm.ppf(1 - alpha) else None) if sampling == \u0026#34;accurate\u0026#34;: result[\u0026#39;GAVI\u0026#39;].append(stops_at(test.GAVI(), size)) result[\u0026#39;mSPRT\u0026#39;].append(stops_at(test.mSPRT(), size)) result[\u0026#39;StatSig_SPRT\u0026#39;].append(stops_at(test.StatSig_SPRT(), size)) result[\u0026#39;StatSig_v1\u0026#39;].append(stops_at(test.statsig_alpha_corrected_v1(), size)) result[\u0026#39;GST_linear\u0026#39;].append(stops_at(z_score \u0026gt; gst_linear, size[itermittent_analyses])) result[\u0026#39;GST_quadratic\u0026#39;].append(stops_at(z_score \u0026gt; gst_quadratic, size[itermittent_analyses])) result[\u0026#39;GST_cubic\u0026#39;].append(stops_at(z_score \u0026gt; gst_cubic, size[itermittent_analyses])) elif sampling == \u0026#34;undersampled\u0026#34;: result[\u0026#39;GAVI\u0026#39;].append(stops_at(test.GAVI(phi=N*7/5), size)) # undersampling is the case, when the effect is larger than expected # so let\u0026#39;s say effect ~ 7/5 times larger, 4 * (5/7)^2 ~ 2 result[\u0026#39;mSPRT\u0026#39;].append(stops_at(test.mSPRT(phi=2 * sigma**2 / diff**2), size)) result[\u0026#39;StatSig_SPRT\u0026#39;].append(stops_at(test.StatSig_SPRT(), size)) result[\u0026#39;StatSig_v1\u0026#39;].append(stops_at(test.statsig_alpha_corrected_v1(N=N*7/5), size)) result[\u0026#39;GST_linear\u0026#39;].append(stops_at(z_score \u0026gt; gst_linear_undersampled, size[itermittent_analyses])) result[\u0026#39;GST_quadratic\u0026#39;].append(stops_at(z_score \u0026gt; gst_quadratic_undersampled, size[itermittent_analyses])) result[\u0026#39;GST_cubic\u0026#39;].append(stops_at(z_score \u0026gt; gst_cubic_undersampled, size[itermittent_analyses])) elif sampling == \u0026#34;oversampled\u0026#34;: result[\u0026#39;GAVI\u0026#39;].append(stops_at(test.GAVI(phi=N*7/10), size)) # oversmapling is the case, when the effect is lower than expected # so let\u0026#39;s say effect ~ 7/10 times lower, 4 * (7/10)^2 ~ 8 result[\u0026#39;mSPRT\u0026#39;].append(stops_at(test.mSPRT(phi=8 * sigma**2 / diff**2), size)) result[\u0026#39;StatSig_SPRT\u0026#39;].append(stops_at(test.StatSig_SPRT(), size)) result[\u0026#39;StatSig_v1\u0026#39;].append(stops_at(test.statsig_alpha_corrected_v1(N=N*7/10), size)) result[\u0026#39;GST_linear\u0026#39;].append(stops_at(z_score \u0026gt; gst_linear_oversampled, size[itermittent_analyses])) result[\u0026#39;GST_quadratic\u0026#39;].append(stops_at(z_score \u0026gt; gst_quadratic_oversampled, size[itermittent_analyses])) result[\u0026#39;GST_cubic\u0026#39;].append(stops_at(z_score \u0026gt; gst_cubic_oversampled, size[itermittent_analyses])) else: raise ValueError(\u0026#34;Unknown sampling method\u0026#34;) # remove StatSig_v1 from Power comparison if not aa_test: result.pop(\u0026#39;StatSig_v1\u0026#39;) df = pd.DataFrame(result).agg([\u0026#34;count\u0026#34;, \u0026#34;median\u0026#34;]).T.assign( PositiveRate=lambda x: (x[\u0026#34;count\u0026#34;] / n_iterations).round(3) ).assign( SampleSize=lambda x: x[\u0026#34;median\u0026#34;].astype(int) )[[\u0026#34;PositiveRate\u0026#34;, \u0026#34;SampleSize\u0026#34;]] return df def plot_positive_rate( df: pd.DataFrame, aa_test: bool=True, sampling: str=None ): fig = go.Figure() if aa_test: error_const = round(3 * (alpha * (1 - alpha) / n_iterations) ** 0.5, 3) else: error_array = round(3 * (df[\u0026#34;PositiveRate\u0026#34;] * (1 - df[\u0026#34;PositiveRate\u0026#34;]) / n_iterations) ** 0.5, 3) fig.add_trace(go.Bar( x=df.index, y=df[\u0026#34;PositiveRate\u0026#34;], marker_color=next(palette), error_y=dict(type=\u0026#39;constant\u0026#39;, value=error_const) if aa_test else dict(type=\u0026#39;data\u0026#39;, array=error_array), )) if aa_test: fig.add_hline( y=0.05, line_dash=\u0026#34;dot\u0026#34;, annotation_text=\u0026#34;designed Type I error rate\u0026#34;, annotation_position=\u0026#34;top right\u0026#34; ) title = ( f\u0026#34;{\u0026#39;Correctness\u0026#39; if aa_test else \u0026#39;Power\u0026#39;} of\u0026#34; f\u0026#34;{\u0026#39; \u0026#39; + sampling if sampling else \u0026#39;\u0026#39;} Sequential Testing Design\u0026#34; ) fig.update_layout( yaxis_title=f\u0026#34;{str(not aa_test)} Positive Rate\u0026#34;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: title, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) fig.write_json(f\u0026#34;{title.replace(\u0026#39; \u0026#39;, \u0026#39;-\u0026#39;).lower()}.json\u0026#34;) fig.show() Continuous Variable As you can see for GST bounds are pre-calculated for the necessary intermittent analyses number that were expected to and in fact take place. We calculate bounds for 10 intermittent analyses scenario, in addition considering over- and under- sampling designs.\nCode\rgst_linear_undersampled = gatsby.GST(actual=10, expected=14, iuse=3, phi=1, alpha=alpha) gst_quadratic_undersampled = gatsby.GST(actual=10, expected=14, iuse=3, phi=2, alpha=alpha) gst_cubic_undersampled = gatsby.GST(actual=10, expected=14, iuse=3, phi=3, alpha=alpha) gst_linear_oversampled = gatsby.GST(actual=10, expected=7, iuse=3, phi=1, alpha=alpha) gst_quadratic_oversampled = gatsby.GST(actual=10, expected=7, iuse=3, phi=2, alpha=alpha) gst_cubic_oversampled = gatsby.GST(actual=10, expected=7, iuse=3, phi=3, alpha=alpha) False Positives Code\rdf = monte_carlo(aa_test=True) df PositiveRate SampleSize No_Seq 0.051 500 GAVI 0.018 221 mSPRT 0.048 38 StatSig_SPRT 0.026 43 StatSig_v1 0.074 421 GST_linear 0.051 300 GST_quadratic 0.050 400 GST_cubic 0.051 400 Code\rplot_positive_rate(df, aa_test=True) As it immediately comes clear: StatSig v1 correction was a flaw, all the other methods are targeting $\\alpha$ as needed, however out of AVI it\u0026rsquo;s only mSPRT that gives high enough level, the rest of them make fewer false positives what usually is a sign of lower statistical power, we will see it later.\nCode\rmonte_carlo(aa_test=True, sampling=\u0026#34;undersampled\u0026#34;) PositiveRate SampleSize No_Seq 0.050 500 GAVI 0.014 253 mSPRT 0.043 38 StatSig_SPRT 0.026 41 StatSig_v1 0.015 451 GST_linear 0.047 350 GST_quadratic 0.046 500 GST_cubic 0.045 500 Code\rdf = monte_carlo(aa_test=True, sampling=\u0026#34;oversampled\u0026#34;) df PositiveRate SampleSize No_Seq 0.051 500 GAVI 0.021 189 mSPRT 0.046 35 StatSig_SPRT 0.027 36 StatSig_v1 0.187 378 GST_linear 0.065 250 GST_quadratic 0.072 300 GST_cubic 0.077 350 Code\rplot_positive_rate(df, aa_test=True, sampling=\u0026#34;oversampled\u0026#34;) Over-sampling is a tough cookie for GST, in such a case GST doesn\u0026rsquo;t work correctly, it inflates Type I error, so it\u0026rsquo;s important to note the difference here between AVI and GST, the latter one is not designed to handle over-sampling StatSig distinguished itself: their v1 version suffers more than any other method form both under- and over- sampling, while on the other flip their SPRT implementation is totally resistant to under- and over- sampling and if identifies the positive, it does it quickly, most likely it will be underpowered though. As of now mSPRT seems to be the best choice as it identifies the differences so fast and just a little less often than it should. True Positives It\u0026rsquo;s time to compare the power of different methods, I\u0026rsquo;m not going to consider StatSig Alpha corrected version anymore as it\u0026rsquo;s not a valid procedure\nCode\rdf = monte_carlo(aa_test=False) df PositiveRate SampleSize No_Seq 0.474 500 GAVI 0.222 285 mSPRT 0.268 202 StatSig_SPRT 0.188 230 GST_linear 0.409 300 GST_quadratic 0.445 350 GST_cubic 0.459 400 Code\rplot_positive_rate(df, aa_test=False) Code\rdf = monte_carlo(aa_test=False, sampling=\u0026#34;undersampled\u0026#34;) df PositiveRate SampleSize No_Seq 0.478 500 GAVI 0.211 306 mSPRT 0.253 210 StatSig_SPRT 0.190 233 GST_linear 0.425 350 GST_quadratic 0.449 450 GST_cubic 0.455 500 Code\rplot_positive_rate(df, aa_test=False, sampling=\u0026#34;undersampled\u0026#34;) Code\rmonte_carlo(aa_test=False, sampling=\u0026#34;oversampled\u0026#34;) PositiveRate SampleSize No_Seq 0.476 500 GAVI 0.236 268 mSPRT 0.263 208 StatSig_SPRT 0.190 233 GST_linear 0.443 300 GST_quadratic 0.496 300 GST_cubic 0.519 350 So, as it comes from bar chart and tables:\nall AVI (including over-sampled options) are way weaker than even under-sampled GST, so power-wise GST is an unconditional winner it\u0026rsquo;s appealing that for under-sampled GST the power has just a subtle decline, and even then only for strict spending function (Cubic), providing an increase for permissive spending function (Linear) Although if AVI rejects null hypothesis it does quicker (the required Sample Size is smaller) than GST on average Conversion Rate In addition to continuous measure, let\u0026rsquo;s consider ratio variable, how the methods work with conversions\nFalse Positives Code\rdf = monte_carlo(aa_test=True, metric=\u0026#34;choice\u0026#34;) df PositiveRate SampleSize No_Seq 0.050 4500 GAVI 0.018 1913 mSPRT 0.072 88 StatSig_SPRT 0.044 60 StatSig_v1 0.074 3796 GST_linear 0.050 2700 GST_quadratic 0.049 3600 GST_cubic 0.049 3600 Code\rplot_positive_rate(df, aa_test=True, sampling=\u0026#34;ratio\u0026#34;) As you see, in addition to StatSig v1 Alpha Correction which is again an outsider, mSPRT approximation is not good enough for Bernoulli random variable, for conversions it\u0026rsquo;s another approach that shall be applied, savvi package might come handy here as it the main purpose the that library - to work with inhomogeneous Bernoulli or Poisson process. Alternatively, you may use sequential_p_value function from gavi module of seqabpy, it\u0026rsquo;s a valid procedure following the algorithm defined by M. Lindon and A. Malek in Anytime-Valid Inference For Multinomial Count Data (2022), could be a little less powerful though than savvi implementation that follows even more recent articles.\nCode\rexpected_probs = [0.5, 0.5] # it\u0026#39;s an asymptotic algorithm, so only numerators are compared # assuming the denominators of convesrion are similar like in fair A/B test actual_counts = [156, 212] print(f\u0026#34;AVI p-value for Conversion: {gavi.sequential_p_value(actual_counts, expected_probs):.3f}\u0026#34;) AVI p-value for Conversion: 0.075\rCode\rdf = monte_carlo(aa_test=True, metric=\u0026#34;choice\u0026#34;, sampling=\u0026#34;oversampled\u0026#34;) df PositiveRate SampleSize No_Seq 0.051 4500 GAVI 0.022 1620 mSPRT 0.069 85 StatSig_SPRT 0.045 61 StatSig_v1 0.189 3377 GST_linear 0.064 2250 GST_quadratic 0.072 2700 GST_cubic 0.076 3150 Code\rplot_positive_rate(df, aa_test=True, sampling=\u0026#34;oversampled ratio\u0026#34;) This chart above is just to assure you, that for conversions over-sampled GST doesn\u0026rsquo;t work neither, I can\u0026rsquo;t help but prove that GST in oversampling design is a flaw, while yet much better than Statsig v1 Alpha Corrections.\nTrue Positives Let\u0026rsquo;s take a brief look at the power comparison for a couple different effect sizes\nCode\rdf = monte_carlo(aa_test=False, metric=\u0026#34;choice\u0026#34;, effect_size=0.10) df PositiveRate SampleSize No_Seq 0.480 4500 GAVI 0.240 2482 mSPRT 0.310 1441 StatSig_SPRT 0.226 1702 GST_linear 0.420 2700 GST_quadratic 0.449 3150 GST_cubic 0.460 3600 Code\rplot_positive_rate(df, aa_test=False, sampling=\u0026#34;ratio\u0026#34;) Code\rdf = monte_carlo(aa_test=False, metric=\u0026#34;choice\u0026#34;, effect_size=0.2) df PositiveRate SampleSize No_Seq 0.930 4500 GAVI 0.767 2127 mSPRT 0.796 1593 StatSig_SPRT 0.718 1923 GST_linear 0.898 2250 GST_quadratic 0.915 2250 GST_cubic 0.921 2700 Code\rplot_positive_rate(df, aa_test=False, sampling=\u0026#34;strong effect\u0026#34;) With the growing effect size the relative difference in power is getting lower, but you can check that with any kind of reasonable effect size, GST outperforms AVI and what is more even for conversion variable, where mSPRT method doesn\u0026rsquo;t really control Type I error rate, it\u0026rsquo;s less powerful than GST after all.\nConclusion Generally speaking, I\u0026rsquo;d rather say that GST is yet the best framework for sequential testing, despite all the recent publications on cutting-edge AVI variations.\nHowever, I have to make a clause: while AVI is noticeably less powerful, it\u0026rsquo;s perfect to work in a streaming manner for guardrail metrics, while GST is better for target metrics within you AB test.\nüí° Practical Tip\nCombining these methodologies you may set up robust Sequential Testing framework, gaining from both: quick detection of major deterioration in your product with AVI and reliable uplifts discoveries in your decisive metrics with the most powerful GST procedure.\nAnother important point is to be conscious about the choice of the specific version of the algorithms that you will use.\nFor instance running an under-sampled experiments where GST with strict alpha spending functions, like Cubic, applied is less preferable, under-sampling works better with permissive spending functions, as well as over-sampling with Cubic spending is worse as it inflates $\\alpha$ more.\nüí° General Rule\nThe more permissive spending function is the faster effect is identified, but the less power at the end of experiment is achieved, what is especially striking for less substantial effect sizes.\nRounding this extensive blog-post up, here are the recommendations on choosing the sequential testing framework wrapped up into a single decision tree:\nReferences seqabpy is an open source library that is perfect for GST and AVI in Python. In addition to implemented functionality it contains all the referenced original papers in functions\u0026rsquo; docstrings, so you may get acquainted with the original works.\nThere were similar posts made by Booking and Spotify, however they do not share implementation details, hence you may read it to deepen the understanding, but barely can apply it in practice:\nChoosing a Sequential Testing Framework by Spotify Sequential Testing at Booking ","permalink":"https://npodlozhniy.github.io/posts/sequential-testing/","summary":"How GST is often misinterpreted and why its genuine version is way better than AVI?","title":"Sequential Testing Guide"},{"content":"Contingency Tests Overview Intro\nIn the world of statistical analysis, contingency tests play a crucial role in examining the relationship between two categorical variables. These tests are essential tools for researchers across various disciplines, enabling them to determine whether there is a significant correlation between the variables of interest.\nReal-world Relevance\nTo illustrate the practical significance of contingency tests, let\u0026rsquo;s consider a real-world scenario: imagine a market research team is investigating the relationship between customer satisfaction (a few levels e.g.¬†Satisfied, Neutral, Dissatisfied) and the type of product purchased (there are multiple products) from an online marketplace. They collect data from a limited sample of customers who recently made purchases building a contingency table. By applying a contingency test, such as Fisher\u0026rsquo;s exact or Chi-squared test, researchers can determine whether customer satisfaction and the type of product purchased are connected.\nFocus\nThis notebook embarks on a journey to explore the subtleties of Fisher\u0026rsquo;s exact vs.¬†Chi-squared tests application, delving into Fisher\u0026rsquo;s implementation nuances, and performance characteristics. A comparative analysis of two implementations of the Fisher\u0026rsquo;s exact test is covered: one crafted in pure Python and the other leveraging the statistical package of R through the rpy2 library. Furthermore, we\u0026rsquo;ll scrutinize the performance and accuracy of both approaches, comparing their results and dissecting their respective advantages.\nBackground: Fisher\u0026rsquo;s Exact and Chi-Squared Contingency Tables and Statistical Independence\nContingency tables serve as the foundation for these tests, presenting the observed frequencies of different combinations of categories for the two or more variables. By analyzing the distribution of frequencies within the table, contingency tests help to assess whether the observed patterns are likely due to chance or reflect a genuine interconnection between the variables.\nCode\rtable = [[1, 24, 5], [5, 20, 7], [14, 11, 7], [11, 14, 8], [10, 10, 10], [12, 12, 12]] Fisher\u0026rsquo;s Exact Fisher\u0026rsquo;s Exact Test, a non-parametric statistical test, plays a pivotal role in hypothesis testing for categorical data. This test is particularly valuable when dealing with small sample sizes, where the assumptions of the chi-squared test are violated.\nDerivation of the Hypergeometric Distribution\nThe hypergeometric distribution arises from a scenario involving sampling without replacement from a finite population containing two types of objects: \u0026ldquo;successes\u0026rdquo; and \u0026ldquo;failures.\u0026rdquo; In the context of contingency tables, these objects correspond to the different categories of the two variables being analyzed.\nConsider a population of size N, containing K objects classified as \u0026ldquo;successes\u0026rdquo; and N-K objects classified as \u0026ldquo;failures.\u0026rdquo; We draw a sample of size n without replacement from this population. The hypergeometric distribution describes the probability of obtaining exactly k successes in the sample.\nThe Hypergeometric Distribution Formula\nThe probability mass function of the hypergeometric distribution is given by:\n$$P(X=k) = \\frac{\\binom{K}{k} \\binom{N-K}{n-k}}{\\binom{N}{n}}$$\nwhere:\nN: Total population size K: Number of successes in the population n: Sample size k: Number of successes in the sample Fisher\u0026rsquo;s Exact Test: Applying the Hypergeometric Distribution\nFisher\u0026rsquo;s Exact Test leverages the hypergeometric distribution to calculate the exact probability of observing a given contingency table, or one more extreme, assuming the null hypothesis of independence between the variables. This exact probability is then used to assess the statistical significance of the observed association.\nChi-Squared The Chi-squared test is another widely used method for analyzing contingency tables to determine whether there is a significant connection between categorical variables. It relies on a statistical approach based on the Chi-squared distribution.\nChi-Squared Statistic: Measuring the Difference\nThe Chi-squared test calculates a test statistic, denoted by $\\chi^2$, which quantifies the difference between the observed frequencies in the contingency table and the frequencies expected under the null hypothesis of independence. The null hypothesis assumes that there is no association between the variables, meaning that the observed frequencies should be close to the expected frequencies.\nTesting the Null Hypothesis\nThe calculated chi-squared statistic is then compared to the chi-squared distribution with degrees of freedom determined by the dimensions of the contingency table. If the table has $N \\times M$ size then degrees of freedom = $(N-1)(M-1)$\nAssumptions and Limitations\nBoth of these procedures, like any statistical test, operates under certain assumptions which are crucial for ensuring the validity of the test\u0026rsquo;s results. Their common requirements are:\nCategorical Data: The variables being analyzed must be categorical, meaning they can be divided into distinct categories or groups. Independent Observations: The observations in the contingency table should be independent of each other. This means that the outcome of one observation should not influence the outcome of another observation. In addition each of them has a third extra requirement\nFisher: Fixed Margins The row and column totals in the contingency table are considered fixed. This implies that the sample sizes for each category are predetermined.\nChi-squared: Sample sizes The expected cell frequencies should be sufficiently large for the chi-squared approximation to be valid\nApparently namely the third condition for each is the most challenging.\nThe third assumption for Fisher is quite strict and is not usually satisfied in practice. There are other representatives of exact test\u0026rsquo;s family that are free of this requirement like Boschloo\u0026rsquo;s or Barnard\u0026rsquo;s tests, although they are much more computationally expensive, as they require the nuisance parameters estimation and it\u0026rsquo;s not feasible to implement them for the tables larger than 2x2. So the performance is the main issue of the exact tests and if it\u0026rsquo;s the case then Chi-squared test is advised to be applied instead.\nFor Chi-squared violations of the third assumption can lead to inaccurate results. In such cases, Fisher\u0026rsquo;s exact test is often preferred due to its ability to handle small sample sizes and sparse tables where the chi-squared test\u0026rsquo;s approximations may not hold true.\nFisher\u0026rsquo;s Exact Test Implementation Pythonic Fisher NxM As long as widely used python packages for statistics like scipy or statsmodels don\u0026rsquo;t furnish Fisher\u0026rsquo;s exact test for tables larger than 2x2, here is the author\u0026rsquo;s pure Pythonic implementation for this procedure, to get more details follow the function documentation below.\nCode\rimport math def _pvalue(func: object, shape: tuple=(2,2)) -\u0026gt; str: print(f\u0026#34;p-value: {func([row[:shape[1]] for row in table[:shape[0]]]):.5f}\u0026#34;) def NxM_Fisher_exact_test(table: list[list]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Performs Fisher\u0026#39;s exact test for a contingency table of an arbitrary size. Parameters ---------- table: list[list] contigency matrix M x N Returns ------- p-value: float \u0026#34;\u0026#34;\u0026#34; num_rows = len(table) num_cols = len(table[0]) row_sums = [sum(row) for row in table] col_sums = [sum(table[i][j] for i in range(num_rows)) for j in range(num_cols)] log_p_constant = ( sum(math.lgamma(x + 1) for x in row_sums) + sum(math.lgamma(y + 1) for y in col_sums) - math.lgamma(sum(row_sums) + 1) ) def calculate_log_probability(matrix): \u0026#34;\u0026#34;\u0026#34; Calculates the log-probability of a contingency table n x m. Fisher\u0026#39;s statistic under the truthful null hypothesis has a hypergeometric distribution of the numbers in the cells of the table. Therefore the probability of the contingency table follows hypergeometric probability mass function $C^K_k * C^N-K_n-k / C^N_n$ So, simplifying it\u0026#39;s clear that the probability follows: the product of factorials of total row and total columns counts divided by the total count factorial and factorials of each cell count. row_1! x..x row_n! x col_1! x..x col_m! / (cell_11! x..x cell_nm! x total!) 1. As the gamma function satisfies: gamma(n + 1) = n! and it\u0026#39;s computationally more stable- it\u0026#39;s used instead of factorials. 2. Making the computations more stable I\u0026#39;m switching from product to sum using logarithmic probability. \u0026#34;\u0026#34;\u0026#34; return log_p_constant - sum( math.lgamma(cell + 1) for row in matrix for cell in row ) log_p_obs = calculate_log_probability(table) p_value = 0 def dfs(matrix: list[list], row_id, col_id, tol=1e-10): \u0026#34;\u0026#34;\u0026#34; Recursive deep-first search function Generates all possible contingency tables and calculates their log-probability adding up those, that are at least as extreme as the observed contingency table, to the total p-value Args: matrix: A list of lists representing the contingency table row_id: Row index up to which the table is already filled col_id: Column index up to which the table is already filled tol: Maximum absolute log-probability comparison error Returns: None \u0026#34;\u0026#34;\u0026#34; nonlocal p_value # Copy is necessary to make recursion working table = [row.copy() for row in matrix] # Stopping condition - only the last row and column are left if row_id == num_rows - 1 and col_id == num_cols - 1: for i in range(row_id): # fill last column table[i][col_id] = row_sums[i] - sum(table[i][:col_id]) for j in range(col_id): # fill last row table[row_id][j] = col_sums[j] - sum(table[i][j] for i in range(row_id)) bottom_right_cell = row_sums[row_id] - sum(table[row_id][:col_id]) if bottom_right_cell \u0026lt; 0: # Non-reliable table, all cells must be non-negative return else: table[row_id][col_id] = bottom_right_cell log_p = calculate_log_probability(table) if log_p \u0026lt;= log_p_obs + tol: p_value += math.exp(log_p) return # Fill the table until the Stopping condition isn\u0026#39;t met else: remaining_row_sum = row_sums[row_id] - sum(table[row_id]) remaining_col_sum = col_sums[col_id] - sum(table[i][col_id] for i in range(num_rows)) for k in range(min(remaining_row_sum, remaining_col_sum) + 1): table[row_id][col_id] = k if row_id == num_rows - 2 and col_id == num_cols - 2: dfs(table, row_id + 1, col_id + 1, tol=tol) elif row_id == num_rows - 2: dfs(table, 0, col_id + 1, tol=tol) else: dfs(table, row_id + 1, col_id, tol=tol) dfs(matrix=[[0] * num_cols for _ in range(num_rows)], row_id=0, col_id=0) return p_value While this exact test above is a precise solution, it does have limitations related to the computational intensity of the test, especially when dealing with large contingency tables. As the table size increases, the number of possible arrangements of data grows exponentially, making the calculations more time-consuming.\n_pvalue(NxM_Fisher_exact_test, shape=(3, 2)) p-value: 0.00014\rR Fisher NxM Another option that is to use rpy bridge from Python to R, this function works for an arbitrary shape of contingency table and unfortunately doesn\u0026rsquo;t have alternatives in Python.\nCode\rimport numpy as np import rpy2.robjects.numpy2ri from rpy2.robjects.packages import importr def R_fisher_exact_test(table: list[list]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Performs exact Fisher\u0026#39;s test using R Parameters ---------- table: list[list] contigency matrix M x N Returns ------- p-value: float \u0026#34;\u0026#34;\u0026#34; # Enable automatic conversion between NumPy and R arrays rpy2.robjects.numpy2ri.activate() # Import necessary R package stats = importr(\u0026#39;stats\u0026#39;) # Perform Fisher\u0026#39;s test using the R function with more memory to get p-value result = stats.fisher_test(np.array(table), workspace = 2e9) # Extract the p-value p_value = result[0][0] return p_value Note that the rpy2 package has native dependencies, what in particular means that, installed R accompanied by the corresponding libraries is required.\n_pvalue(R_fisher_exact_test, shape=(3, 2)) p-value: 0.00014\rAlgorithmic Differences: Python vs R While both the Python and R implementations ultimately calculate the p-value for Fisher\u0026rsquo;s Exact Test, they employ distinct algorithms under the hood, each with its own strengths and weaknesses. Understanding these differences is crucial for selecting the most appropriate implementation for a given scenario.\nPython: utilizes a recursive algorithm to enumerate all possible contingency tables that could arise under the null hypothesis. This approach, while conceptually straightforward, can become computationally expensive.\nR: in contrast, it leverages optimized algorithms and data structures that are specifically designed for efficient calculation of Fisher\u0026rsquo;s Exact Test. These algorithms, often implemented in compiled languages, take advantage of advanced numerical techniques and data representations to minimize computational overhead.\nPerformance Comparison Trade-offs and Considerations:\nThe choice between the Python and R implementations depends on the specific needs of the analysis. For smaller tables, the Python implementation may suffice, offering ease of understanding and implementation. However, as the table size increases, the computational advantages of the R implementation become more pronounced.\nI suggest we generate random contingency tables with dimensions ranging from 2x2 to 5x5, representing a diverse range of scenarios encountered in real-world applications. For each table size, we will measure the execution time required by both the Python and R implementations to calculate the p-value.\nCode\r%%time _pvalue(NxM_Fisher_exact_test, shape=(3, 2)) _pvalue(NxM_Fisher_exact_test, shape=(4, 2)) _pvalue(NxM_Fisher_exact_test, shape=(3, 3)) p-value: 0.00014\rp-value: 0.00012\rp-value: 0.00085\rCPU times: user 745 ms, sys: 7.96 ms, total: 753 ms\rWall time: 756 ms\rCode\r%%time _pvalue(R_fisher_exact_test, shape=(3, 2)) _pvalue(R_fisher_exact_test, shape=(4, 2)) _pvalue(R_fisher_exact_test, shape=(3, 3)) p-value: 0.00014\rp-value: 0.00012\rp-value: 0.00085\rCPU times: user 2.6 s, sys: 219 ms, total: 2.82 s\rWall time: 2.82 s\rCode\r%%time _pvalue(R_fisher_exact_test, shape=(4, 3)) p-value: 0.00149\rCPU times: user 1.46 s, sys: 97 ms, total: 1.55 s\rWall time: 1.88 s\rCode\r%%time _pvalue(NxM_Fisher_exact_test, shape=(4, 3)) p-value: 0.00149\rCPU times: user 8min, sys: 921 ms, total: 8min 1s\rWall time: 8min 5s\rFor sizes more than 2 x 5 and 3 x 3, R package function can significantly outperform the Python counterpart, whereas with tables of less size pure Python function is shining.\nThe results of the benchmarks revealed a clear trend: Python implementation is beaten in terms of execution time for larger tables. As the table dimensions increased, the performance gap between the two implementations is widened drastically. This observation aligns with the algorithmic differences discussed earlier, where the optimized algorithms and data structures employed by the R implementation proved to be more efficient.\nAs a rule of thumb I propose to apply R if $N \\times M \u0026gt; 10$, otherwise Python is preferable and what is more - it doesn\u0026rsquo;t have any dependencies on non-native packages\nAccuracy Comparison Along with the performance let\u0026rsquo;s assure that the p-values generated by both methods are equivalent\nCode\rfrom scipy.stats import multinomial from statsmodels.stats.proportion import proportion_confint P = np.arange(.1, .35, .05) n = 40 rv = multinomial(n, P) np.random.seed(2024) tol = 1e-5 alpha = 0.05 n_iterations = 100 for shape in [(2, 2), (2, 4), (3, 3)]: false_positives = 0 for i in range(n_iterations): contingency_table = rv.rvs(shape[0]) p_value_py = NxM_Fisher_exact_test([row[:shape[1]] for row in contingency_table]) p_value_r = R_fisher_exact_test([row[:shape[1]] for row in contingency_table]) if abs(p_value_py - p_value_r) \u0026gt; tol: print(f\u0026#34;Different p-values! Python: {p_value_py}, R: {p_value_r}\u0026#34;) break elif p_value_py \u0026lt;= alpha: false_positives += 1 l, r = proportion_confint(count=false_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#34;shape: {shape}, false positives: {false_positives/n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#34;) shape: (2, 2), false positives: 0.020 ¬± 0.026\rshape: (2, 4), false positives: 0.030 ¬± 0.030\rshape: (3, 3), false positives: 0.030 ¬± 0.030\rSo, it\u0026rsquo;s clear from multiple iterations for different tables and sizes that there is no a single case of different p-values, so the equivalence is practically evident. In addition I\u0026rsquo;ve checked Type I error level, it\u0026rsquo;s well below the bound of 5%, which means that ideologically the criterions are valid.\nPerformance Analysis: A Comparative Benchmark As the textbooks say, Fisher\u0026rsquo;s Exact Test stands out as a particularly versatile option when dealing with small sample sizes or sparse contingency tables. It provides accurate p-values, even when the assumptions of other commonly used test, such as the chi-squared test, might be violated. This benefit makes Fisher\u0026rsquo;s Exact test an invaluable method when working with limited data or situations where the chi-squared test\u0026rsquo;s approximation is not applicable.\nI offer you a procedure to challenge these statements, namely to call the power of the exact test out, when chi-squared assumptions are not satisfied. First we will check the correctness of these two methods and then the power.\nCode\rdef chi_squared_challenge( shape: tuple = (2, 2), n_iterations: int=1_000, alpha: float=0.05, aa_test: bool=True ) -\u0026gt; None: for i in range(1 + len(rv.rvs()[0]) - shape[1]): fisher_positives = 0 chi2_positives = 0 chi2_yates_positives = 0 zero_expected_count = 0 less_than_5 = 0 less_than_10 = 0 for _ in range(n_iterations): contingency_table = rv.rvs(shape[0])[:, i:i+shape[1]] if not aa_test: contingency_table[0] = contingency_table[0] ** 2 if np.min(contingency_table) == 0: zero_expected_count += 1 continue less_than_5 += np.max(np.array(contingency_table) \u0026lt; 5) less_than_10 += np.max(np.array(contingency_table) \u0026lt; 10) if shape == (2, 2): p_value_fisher = fisher_exact(contingency_table).pvalue p_value_chi2_yates = chi2_contingency(contingency_table, correction=True).pvalue if p_value_chi2_yates \u0026lt;= alpha: chi2_yates_positives += 1 else: p_value_fisher = NxM_Fisher_exact_test(contingency_table) p_value_chi2 = chi2_contingency(contingency_table, correction=False).pvalue if p_value_chi2 \u0026lt;= alpha: chi2_positives += 1 if p_value_fisher \u0026lt;= alpha: fisher_positives += 1 valid_tables = n_iterations - zero_expected_count print( f\u0026#34;\\nIf out of {valid_tables} valid {shape[0]}x{shape[1]} tables \u0026#34; f\u0026#34;(w/o zero expected count) number of tables with less than:\u0026#34; f\u0026#34;\\n - 5 elements in any cell is {less_than_5}\u0026#34; f\u0026#34;\\n - 10 elements in any cell is {less_than_10}\u0026#34; f\u0026#34;\\n Then p-values are:\u0026#34; ) l, r = proportion_confint(count=fisher_positives, nobs=valid_tables, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#34;Fisher positives: {fisher_positives/valid_tables:.3f} ¬± {(r - l) / 2:.3f}\u0026#34;) l, r = proportion_confint(count=chi2_positives, nobs=valid_tables, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#34;Chi2 positives: {chi2_positives/valid_tables:.3f} ¬± {(r - l) / 2:.3f}\u0026#34;) if not shape == (2, 2): continue l, r = proportion_confint(count=chi2_yates_positives, nobs=valid_tables, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#34;Chi2 Yates positives: {chi2_yates_positives/valid_tables:.3f} ¬± {(r - l) / 2:.3f}\u0026#34;) According to frequently encountered requirements in the literature regarding expected cell counts for chi-squared test application, a common rule is at least 5 (some requires 10) in all cells of 2x2 table, and 5 or more in 80% of cells in larger tables, but no cells with zero expected count. Furthermore, when the assumption for 2x2 table is not met, Yates\u0026rsquo;s correction is applied.\nNow, we will check the feasibility of these conditions for 2x2 tables first.\nCorrectness Code\rnp.random.seed(26) chi_squared_challenge(aa_test=True) If out of 969 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 915\r- 10 elements in any cell is 969\rThen p-values are:\rFisher positives: 0.019 ¬± 0.007\rChi2 positives: 0.043 ¬± 0.011\rChi2 Yates positives: 0.011 ¬± 0.006\rIf out of 993 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 566\r- 10 elements in any cell is 993\rThen p-values are:\rFisher positives: 0.024 ¬± 0.008\rChi2 positives: 0.050 ¬± 0.011\rChi2 Yates positives: 0.015 ¬± 0.006\rIf out of 1000 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 178\r- 10 elements in any cell is 993\rThen p-values are:\rFisher positives: 0.032 ¬± 0.009\rChi2 positives: 0.052 ¬± 0.012\rChi2 Yates positives: 0.022 ¬± 0.008\rIf out of 1000 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 27\r- 10 elements in any cell is 831\rThen p-values are:\rFisher positives: 0.033 ¬± 0.009\rChi2 positives: 0.049 ¬± 0.011\rChi2 Yates positives: 0.022 ¬± 0.008\rPower Code\rnp.random.seed(26) chi_squared_challenge(aa_test=False) If out of 969 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 785\r- 10 elements in any cell is 969\rThen p-values are:\rFisher positives: 0.300 ¬± 0.024\rChi2 positives: 0.359 ¬± 0.025\rChi2 Yates positives: 0.265 ¬± 0.023\rIf out of 993 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 373\r- 10 elements in any cell is 987\rThen p-values are:\rFisher positives: 0.325 ¬± 0.024\rChi2 positives: 0.370 ¬± 0.025\rChi2 Yates positives: 0.295 ¬± 0.024\rIf out of 1000 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 90\r- 10 elements in any cell is 897\rThen p-values are:\rFisher positives: 0.334 ¬± 0.025\rChi2 positives: 0.363 ¬± 0.025\rChi2 Yates positives: 0.305 ¬± 0.024\rIf out of 1000 valid 2x2 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 9\r- 10 elements in any cell is 586\rThen p-values are:\rFisher positives: 0.351 ¬± 0.025\rChi2 positives: 0.387 ¬± 0.025\rChi2 Yates positives: 0.327 ¬± 0.024\rSurprise! In this example it\u0026rsquo;s shown there is no need for Yates nor for Fisher\u0026rsquo;s exact test at all! Chi-squared test doesn\u0026rsquo;t inflate the number of Type I errors and keep the power at least as high as it\u0026rsquo;s for the exact test regardless of the number of cells with low frequencies.\nJFYI: Some other exact tests might be applied instead of Fisher\u0026rsquo;s test, e.g.¬†Boschloo\u0026rsquo;s test provides higher power but it\u0026rsquo;s a) much slower and b) yet worse than a plain chi-squared, you may prove it on your own as an exercise. Hint: there is a function boschloo_exact in scipy\nOkay, it\u0026rsquo; clear with 2x2, but what if the table is getting bigger?\nCode\rnp.random.seed(26) chi_squared_challenge(shape=(2, 4), aa_test=False) If out of 969 valid 2x4 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 804\r- 10 elements in any cell is 969\rThen p-values are:\rFisher positives: 0.685 ¬± 0.025\rChi2 positives: 0.688 ¬± 0.024\rIf out of 993 valid 2x4 tables (w/o zero expected count) number of tables with less than:\r- 5 elements in any cell is 388\r- 10 elements in any cell is 993\rThen p-values are:\rFisher positives: 0.666 ¬± 0.025\rChi2 positives: 0.672 ¬± 0.024\rThe power values are not statistically distinguishable, so chi-squared is still the winner as it\u0026rsquo;s much simpler in calculations and for the data model that I specified it seems that it can handle small table sizes - it takes low values at least as good as Fisher\u0026rsquo;s test.\nMonte-Carlo Simulation Results Interpretation I\u0026rsquo;d like to make an extra note that Monte-Carlo simulations can provide valuable insights into the performance and behavior of statistical tests, but it\u0026rsquo;s essential to interpret their results with caution and awareness of their limitations.\nWhile simulations can mimic real-world scenarios, they are inherently limited by the assumptions and parameters used in their design. They may not capture the full complexity of real-world data and may not be generalizable to all situations. Therefore, it\u0026rsquo;s crucial to consider the specific context and limitations of the simulation when interpreting its results.\nSaying that I must admit that I don\u0026rsquo;t have an intention to prove that there is no need for exact tests in any experiment design, I\u0026rsquo;d rather invite you to challenge your data and your experiments set up specifics, as there is a chance that you will find that chi-squared test is all you need for contingency experiments.\nJustification for Fisher\u0026rsquo;s Exact over Chi-Squared Even when the chi-squared test appears to perform well in Monte-Carlo simulations, there are compelling theoretical and practical reasons to prefer Fisher\u0026rsquo;s Exact Test in specific situations. Understanding these justifications is crucial for making informed decisions about which test to apply.\nSparse Table: Chi-squared test relies on approximations that may not hold true when dealing with sparse contingency tables. Sample Size: Chi-squared test is based on the asymptotic distribution of the test statistic, which assumes that the sample size is large. Effect Size: Chi-squared may be less sensitive in detecting the small effect size, because the approximation may not be as accurate. So, once again: in order to guarantee that you don\u0026rsquo;t have a need for exact tests in your data model setting, you must consciously simulate your data distributions (especially when it comes to sparse tables, small sample sizes and small effect sizes) and then make a decision, the process that I presented here is based on the data my team is exposed to most and hopefully it might be easily simulated with Multinomial distribution.\nGeneral Pipeline Finally I\u0026rsquo;d like to offer you a full pipeline on how to organize contingency tests efficiently: when Fisher\u0026rsquo;s exact test shall be applied and when Chi-squared is just enough.\nAs you know, exact tests could take time and what I want to achieve is to have a control over the time that I allocate to the function execution.\nThere are a few ways to implement timeouts, my favourite one is leveraging multiprocessing capabilities, however it\u0026rsquo;s not always the case that you can run a subprocess under your main process in production, so another concise way to apply timeouts will be shown via func_timeout library.\nConcurrent Execution Simple decorator to pass the output from the subprocess into main process.\nCode\rfrom typing import Optional from functools import wraps from multiprocessing import Queue, Process def subprocess_output(procedure: object, queue: Optional[Queue]=None) -\u0026gt; object: @wraps(procedure) def wrapper(*args, **kwargs): p_value = procedure(*args, **kwargs) queue.put(p_value) return p_value return wrapper def concurrent_test( method: object, table: list[list], name: str=\u0026#34;NxM Fisher\u0026#39;s exact test\u0026#34;, timeout: int=10, ) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Runs the given method in a separate process with a timeout. If the process takes longer than the timeout, it is terminated. The result is returned from the main process. Parameters ---------- method: object The method to be run in a separate process. table: list[list] Contingency matrix M x N name: str Process name timeout: int Time limit for subprocess execution Returns ------- p-value: float \u0026#34;\u0026#34;\u0026#34; queue = Queue() procedure = subprocess_output(method, queue) p = Process( target=procedure, args=(table,), name=name ) p.start() p.join(timeout=timeout) p.terminate() if p.exitcode is not None: p_value = queue.get() return p_value Timeout Execution Handy function that is a good solution if a timeout is the only thing you want to get from a subprocess\nCode\r# pip install func-timeout from func_timeout import func_timeout, FunctionTimedOut def timeout_test( method: object, table: np.ndarray, timeout: int=10, ) -\u0026gt; float: try: p_value = func_timeout(timeout, method, args=(table,)) except FunctionTimedOut: p_value = None return p_value By the way: there is no need for timeout when running scipy Fisher\u0026rsquo;s exact test, so it\u0026rsquo;s applied only to those methods analyzed in the chapter above.\nCode\r%%time fisher_exact(np.array([[10000, 4000], [12000, 5000]])) CPU times: total: 0 ns\rWall time: 14 ms\rSignificanceResult(statistic=1.0416666666666667, pvalue=0.10488212218194087)\rUniversal procedure Logic Code\rfrom scipy.stats import chi2_contingency, fisher_exact def _contingency_test(table: np.ndarray, criterion: str, timeout: int) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; Performs a test of independence of variables in a contingency table Parameters ---------- table: np.ndarray Contingency matrix M x N criterion: str Test to be performed timeout: int Time limit for Fisher\u0026#39;s exact test Returns ------- dict: { \u0026#34;method\u0026#34;: str, \u0026#34;p-value\u0026#34;: float, \u0026#34;error\u0026#34;: str (optional) } \u0026#34;\u0026#34;\u0026#34; if criterion not in {\u0026#34;chi-squared\u0026#34;, \u0026#34;fisher-exact\u0026#34;, \u0026#34;textbook\u0026#34;}: raise ValueError( \u0026#34;Incorrect type of criterion, \u0026#34; \u0026#34;should be one of the following: \u0026#39;chi-squared\u0026#39;, \u0026#39;fisher-exact\u0026#39;, \u0026#39;textbook\u0026#39;\u0026#34; ) # No timeout if \u0026#34;fisher-exact\u0026#34; criterion is set timeout = timeout + 10_000 * (criterion == \u0026#34;fisher-exact\u0026#34;) result = dict.fromkeys([\u0026#34;method\u0026#34;, \u0026#34;p-value\u0026#34;]) try: if criterion == \u0026#34;textbook\u0026#34; and table.shape == (2, 2) and (table \u0026gt;= 10).all(): result[\u0026#34;method\u0026#34;] = \u0026#34;2x2 Pearson\u0026#39;s chi-squared test with Yates\u0026#34; test = chi2_contingency(table, correction=True) result[\u0026#34;p-value\u0026#34;] = test.pvalue elif criterion != \u0026#34;chi-squared\u0026#34; and table.shape == (2, 2): result[\u0026#34;method\u0026#34;] = \u0026#34;2x2 Fisher\u0026#39;s exact test in Python\u0026#34; test = fisher_exact(table) result[\u0026#34;p-value\u0026#34;] = test.pvalue elif criterion == \u0026#34;chi-squared\u0026#34; or (criterion == \u0026#34;textbook\u0026#34; and np.sum(table \u0026gt;= 5) \u0026gt;= np.size(table) * 0.80): result[\u0026#34;method\u0026#34;] = \u0026#34;NxM Pearson\u0026#39;s chi-squared test w/o Yates\u0026#34; test = chi2_contingency(table, correction=False) result[\u0026#34;p-value\u0026#34;] = test.pvalue else: # try Exact fisher test if doesn\u0026#39;t take too much if np.size(table) \u0026gt; 10: name = \u0026#34;NxM Fisher\u0026#39;s exact test in R\u0026#34; p_value = timeout_test( R_fisher_exact_test, table, timeout ) else: name = \u0026#34;NxM Fisher\u0026#39;s exact test in Python\u0026#34; p_value = timeout_test( NxM_Fisher_exact_test, table, timeout ) result[\u0026#34;method\u0026#34;] = name if p_value: result[\u0026#34;p-value\u0026#34;] = p_value else: result[\u0026#34;method\u0026#34;] = \u0026#34; \u0026#34;.join([ result[\u0026#34;method\u0026#34;], \u0026#34;timed out.\u0026#34;, \u0026#34;NxM Pearson\u0026#39;s chi-squared test approximation applied\u0026#34; ]) test = chi2_contingency(table, correction=False) result[\u0026#34;p-value\u0026#34;] = test.pvalue except Exception as error: result[\u0026#34;error\u0026#34;] = f\u0026#34;{error}\u0026#34; return result Here is a quick example of how it works with the identified table\nCode\rt = np.array([row[:2] for row in table[:5]]) _contingency_test(t, \u0026#39;textbook\u0026#39;, 5) {'method': \u0026quot;NxM Pearson's chi-squared test w/o Yates\u0026quot;,\r'p-value': 0.00032439327665678783}\rInput validation Adding a validation is an important step to prevent end-users from inference the function in the wrong way\nCode\rdef _validate_input(table: list[list]) -\u0026gt; np.array: try: array = np.array(table) except ValueError: raise ValueError( \u0026#34;Contingency table\u0026#39;s rows must be of equal length.\u0026#34; ) try: array = array.astype(dtype=int) except ValueError: raise ValueError( \u0026#34;All cells must contain integer numbers.\u0026#34; ) if array.ndim != 2: raise ValueError( \u0026#34;Contigency table must be a 2-dimensional array.\u0026#34; ) if (array \u0026lt; 0).any(): raise ValueError( \u0026#34;All cells must contain non-negative numbers.\u0026#34; ) if array.shape[0] == 2 and np.max(np.min(array, axis=1)) == 0: raise ValueError( \u0026#34;There are cells with zero expected count. \u0026#34; \u0026#34;Expectations must contain only positive numbers.\u0026#34; ) return array Put it all together Code\rdef general_contingency_test(table: list[list], criterion=\u0026#39;textbook\u0026#39;, timeout: int=10) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; Performs a test of independence of variables in a contingency table Parameters ---------- table: list[list] matrix M x N, where M is the number of compared groups and N is the set of measures timeout: int Time limit for Fisher\u0026#39;s exact test, if the calculation takes longer chi-squared test is applied instead Returns ------- dict: { \u0026#34;method\u0026#34;: str, \u0026#34;p-value\u0026#34;: float, \u0026#34;error\u0026#34;: str (optional) } \u0026#34;\u0026#34;\u0026#34; return _contingency_test(_validate_input(table), criterion, timeout) Now when we have a general procedure, let\u0026rsquo;s take a look at a few examples of the inference\nCode\rgeneral_contingency_test(table, criterion=\u0026#39;chi-squared\u0026#39;) {'method': \u0026quot;NxM Pearson's chi-squared test w/o Yates\u0026quot;,\r'p-value': 0.0020940807559433087}\rCode\rgeneral_contingency_test([[1, 2, 3, 5, 6, 100, 2000], [4, 5, 6, 7, 8, 150, 1000]], timeout=10) {'method': \u0026quot;NxM Fisher's exact test in R timed out. NxM Pearson's chi-squared test approximation applied\u0026quot;,\r'p-value': 5.060441099877772e-17}\rThe logic wrapped into the library Good news for you, you don\u0026rsquo;t need to repeat all the code that was shared above as it\u0026rsquo;s already a part of the public Python package podlozhnyy_module that comes hande every time you have data analysis assignments at work.\nKey Features of the Library:\nAutomatic Test Selection: By default, the library automatically selects the most appropriate test based on textbook rules, considering factors such as sample size and expected cell frequencies. This intelligent selection process ensures the validity and accuracy of the results. Flexibility and Control: Users have the flexibility to override the automatic selection and force the library to apply a specific test if desired. This feature is particularly useful when researchers have prior knowledge or preferences regarding the test to be used. User-Friendly Interface: The library\u0026rsquo;s interface is designed to be intuitive and easy to use, enabling researchers to effortlessly perform contingency tests without extensive coding or technical expertise. # !pip install podlozhnyy-module==2.6-alpha import podlozhnyy_module as pm Library\u0026rsquo;s application is as simple as the following command\npm.contingency.general_contingency_test(table[:5], criterion=\u0026#39;fisher-exact\u0026#39;, timeout=1) {'method': \u0026quot;NxM Fisher's exact test in R\u0026quot;, 'p-value': 0.0011288225617825118}\rpm.contingency.general_contingency_test(table[:3], criterion=\u0026#39;fisher-exact\u0026#39;, timeout=1) {'method': \u0026quot;NxM Fisher's exact test in Python\u0026quot;,\r'p-value': 0.0008451539443552633}\rConclusion: A Unified Framework for Contingency Tests This notebook has explored various aspects of contingency tests, with a particular focus on Fisher\u0026rsquo;s Exact Test comparison to Chi-squared test. As a part of the journey we have contrasted the Python and R implementations of the Fisher\u0026rsquo;s test, highlighting their strengths and weaknesses in terms of performance, accuracy, and computational considerations.\nThe powerful framework of Monte-Carlo simulations is provided to enable the readers to simulate their data and ultimately apply proper testing techniques in their field providing the accurate guidance to the business.\npodlozhnyy_module library: a flexible solution\nThe code presented in this notebook has been thoughtfully integrated into the podlozhnyy_module library, offering a flexible and user-friendly solution for conducting contingency tests. This library empowers users to select the most appropriate test based on textbook rules or to override the default behavior and force the application of a specific test, such as Fisher\u0026rsquo;s Exact or the Chi-squared test.\n","permalink":"https://npodlozhniy.github.io/posts/contingency-test/","summary":"Why Fisher\u0026rsquo;s Exact test is the relic of the past and Chi-squared is all the way","title":"Mastering Homogeneity Hypothesis testing"},{"content":"\rBackground Hey, some time passed since the last post, where I described why it\u0026rsquo;s must to apply Dunnett\u0026rsquo;s correction for multivariate testing. This time I\u0026rsquo;m eager to open the topic of maturing metrics, particularly conversions, how to properly analyse the experiment with such a target, what are the options and which one is applied at HomeBuddy where we strive to squeeze maximum from every experiment.\nPrerequisites As usual the code snippets that are used for demo are written in Python, so to thoroughly follow the content, it\u0026rsquo;s better to have some exposure to the language and its main packages, although I\u0026rsquo;m adding extensive comments, so even if you\u0026rsquo;re not into programming you can grasp the pith anyway.\nDisclaimer: The actual notebook was written on Python 3.11.4 and to keep the results reproducible here is the list of particular packages that are being used specified with their versions.\nCode\rpip install --quiet --upgrade numpy==1.25.2 pandas==2.0.3 scipy==1.11.2 statsmodels==0.14.0 podlozhnyy_module==2.5.3 Data We need more data! This time the data simulation process is a little tricky than in case of a plain conversion as soon as we need to specify the maturation curve.\nIf you are not familiar with the concept of a maturation let me provide you with an example: imagine as a target in an AB experiment you have a deep conversion from a visit to a final goal, it may be an order for an arbitrary e-commerce web-site or the first payment in a mobile game.\nWhat all these deep actions usually have in common is that they are revenue-generating events which makes them a good proxy to measure the success of the experiment, from the other flip they take time to happen, depending on the specifics of the business, this could be even days or weeks.\nIn the context of HomeBuddy this might be an appointment or a deal that is our north star, as this is the reaper point where both sides (lead and contractor) reached their primary goal.\nIn order to avoid revealing the cards, I suggest we use an arbitrary maturation curve that is not inherited from our data, however it\u0026rsquo;s a pretty true-to-life example of how metric adds up to the final value.\nMaturation curves are usually might be described to a very high extent with Pareto distribution, which was initially developed to define a wealth\u0026rsquo;s distribution in the society, however the theory underneath is going way beyond widely known Pareto 80-20 principle as in essence it\u0026rsquo;s a parametric distribution that is good to define highly skewed data samples with a long heavy tail.\nIf you\u0026rsquo;re interested in challenging your metric using the materials from the post, I suggest you give a shot to ParetoApprox function from my module that basically finds the best Pareto distribution parameters to fit the supplied array of the data, and voila you have a random variable that describes your metric and therefore you can do an infinite sampling and verify hypotheses about the applied techniques.\nFor this post let\u0026rsquo;s stop at one specific and quite realistic distribution of the metric maturation by days.\nCode\rimport numpy as np import pandas as pd import podlozhnyy_module as pm from tqdm.notebook import tqdm What\u0026rsquo;s important is barely we can expect that all visitors finally will reach the goal and hence while those, who will, can be described with Pareto, the major part of visitors will never convert and hence the true distribution of visitors is the sampling between never converted visitors and the rest of them, whose conversion day is distributed by Pareto.\nHopefully I\u0026rsquo;ve already created such an \u0026ldquo;Extended\u0026rdquo; Pareto distribution and you can define a metric of such a nature right out of the box: ParetoExtended in addition to Pareto specific alpha, loc and scale params is taking default_proba and default_value which means what ultimate percentage of visitors would never convert and which particular value should I assign to them, it\u0026rsquo;s an arbitrary (usually negative) number that stands on the left of X axis from the main Pareto distribution.\nCode\rf = pm.pareto.ParetoExtended( alpha=1.161, default_value=0, default_proba=0.5 ) As an example here is the metric where the ultimate conversion is a half of the users (and the other half is never converted) and their conversions happen accordingly to a textbook Pareto principle 80-20\nCode\rfrom matplotlib import pyplot as plt plt.style.use(\u0026#39;dark_background\u0026#39;) def plot_maturating_curve(f: pm.pareto.ParetoExtended, xmin=0, ymin=None) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Plot maturating curve for the conversion that has given distribution \u0026#34;\u0026#34;\u0026#34; xarr = np.linspace(xmin, 24, 100) cdf = [f.cdf(x) for x in xarr] weeks_x = range(6, 24, 7) weeks_cdf = [f.cdf(x) for x in weeks_x] plt.plot(xarr, cdf, \u0026#39;y-\u0026#39;, linewidth=2) plt.scatter(x=weeks_x, y=weeks_cdf, marker=\u0026#39;o\u0026#39;, s=36, c=\u0026#39;y\u0026#39;) plt.xlabel(\u0026#39;Day No.\u0026#39;) plt.ylabel(\u0026#39;Cumulative Share of Conversions\u0026#39;) plt.title(\u0026#39;Maturating Curve\u0026#39;) plt.xlim([min(xarr), max(xarr) + 1]) plt.ylim([ymin if ymin else f._zero_p, 1.0]) plt.grid() plt.show() plot_maturating_curve(f, xmin=1) The line start from 0.5 because it\u0026rsquo;s defined with default_proba that only a half of users will have a conversion after all, for the half of them there will be no conversion at all, so they reach the maturatity on the first day.\nCode\r# eval: false sample = f.rvs(100_000) # 80th percentile of Pareto part is 90th percentile of declared random variable # as half of values are set to zero with default_proba percentile80 = np.percentile(sample, 90) print(f\u0026#34;20% of population possess {sum(sample[sample \u0026gt;= percentile80]) / sum(sample):.1%} of wealth\u0026#34;) 20% of population possess 75.2% of wealth\rSo, as you can see everything works as expected, and in addition you have flexibility for sampling as non-converted users are embedded\nPower Research The focus of the article is to explore what happens to the power of an experiment when a maturating conversion is not treated properly\nSample Size First of all let\u0026rsquo;s set a few introductory parameters and move on to the sample\u0026rsquo;s size calculation, that we have to provide in order to achieve 80% power. We explore a deep conversion, so the base value is a matter of tenths of a percent, in addition we will employ multivariate testing approach as it\u0026rsquo;s usually the case.\nCode\rfrom podlozhnyy_module.abtesting import ZDunnett from statsmodels.stats.proportion import proportion_confint CONVERSION = 0.004 N_GROUPS = 3 DURATION_DAYS = 21 USERS_PER_DAY = 10_000 EFFECT_SIZE = 0.12 from scipy.stats import norm, binom def min_sample_size(mde: float, var: float, alpha: float=0.05, power: float=0.8): \u0026#34;\u0026#34;\u0026#34; General-purpose calculator for T-test sample size. As long as a variance is supplied as a prameter, it can be accountant for different variations of a classical test. Args: mde: absolute value of a minimal detectable effect var: variance of leveraged T-statistic alpha: Type I error rate power: 1 - Type II error rate Returns: int, minimum size of each sample \u0026#34;\u0026#34;\u0026#34; effect_size = mde / var ** 0.5 return round(2 * ((norm.ppf(1 - alpha/2) + norm.ppf(power)) / effect_size) ** 2) For an accurate size calculation let\u0026rsquo;s get a proper estimate of the variance from a simulation of a future experiment. The conversions number is considered a binomial random variable, like there is no maturity, it\u0026rsquo;s equal to a size estimation on the basis of the historical value of your conversion.\nCode\rinput = dict() input[\u0026#34;names\u0026#34;] = [chr(ord(\u0026#34;A\u0026#34;) + i) for i in range(N_GROUPS)] input[\u0026#34;variant\u0026#34;] = input[\u0026#34;names\u0026#34;][-1] input[\u0026#34;base\u0026#34;] = [DURATION_DAYS * USERS_PER_DAY] * N_GROUPS input[\u0026#34;target\u0026#34;] = [binom(n=DURATION_DAYS * USERS_PER_DAY, p=CONVERSION).rvs() for _ in range(N_GROUPS)] test = ZDunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) test.groups_results() {'variant': ['B', 'C'],\r'statistic': array([-0.71881574, -0.47094824]),\r'p-value': array([0.69330407, 0.85181339])}\rCode\rmin_sample_size(mde=EFFECT_SIZE * CONVERSION, var=test.variance, alpha=0.1, power=0.8) 207983\rOf course there is some variance in this variance calculation, although it\u0026rsquo;s essentially enough to carry an experiment for 21 day with 10K users per day to get 80% power, given the rest of input parameters and if you wait an infinite amount of time after the sampling is finished and consequently take into account every single conversion that happens sooner or later.\nMonte-Carlo Let\u0026rsquo;s design a procedure to look at the power in case of early stopping for different time frames and as a good habit it\u0026rsquo;s always better to ensure the correctness of the criterion beforehand - so will measure that FWER is controlled accordingly as well.\nCode\r# it can by any negative number, that works just like a signal of the lack of conversion DEFAULT = -10 It is proposed to use some arbitrary extended Pareto distribution, that has a few characteristics\n75% of conversions happen during the first week 90% of conversions happen within 3 weeks in total percentage of conversions is equal to CONVERSION Code\rdef distribution(conv_proba: float) -\u0026gt; pm.pareto.ParetoExtended: \u0026#34;\u0026#34;\u0026#34; Returns a possible distribution of a maturating metric Args: conv_proba: ultimate share of conversions equals to: 1 - the share of users who will not convert \u0026#34;\u0026#34;\u0026#34; return pm.pareto.ParetoExtended( alpha=1.136, loc=-4.475, scale=2.981, default_value=DEFAULT, default_proba=1-conv_proba, ) f = distribution(1.0) plot_maturating_curve(f, xmin=0, ymin=0.4) for idx, val in enumerate([f.cdf(x) for x in range(6, 22, 7)]): print(f\u0026#34;{val:.0%} of conversions happen during {1 + idx} week{\u0026#39;s\u0026#39; if idx \u0026gt; 0 else \u0026#39;\u0026#39;}\u0026#34;) 76% of conversions happen during 1 week\r87% of conversions happen during 2 weeks\r91% of conversions happen during 3 weeks\rIn addition we need a sample generator that would count the conversions that happen only before we draw a conclusion. Method generate_sample would vary throughout the notebook: the first implementation reflects the idea of drawing a conclusion right after we stopped the experiment.\nAs usual Monte-Carlo process comes handy when you want to carry some \u0026ldquo;meta\u0026rdquo; experiments. Simplifying the code, Monte Carlo procedure is wrapped into a function with all the necessary parameters supplied as arguments.\nCode\rdef generate_sample( conv_proba: float, exp_duration: int, users_per_day: int, ) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Build a sample of bernoulli random variable where 1 = conversion happened during the time of an experiment 0 = conversion wouldn\u0026#39;t happen at all or only after we stopped and draw a conclusion The output is the sum of the sample = total number of conversions Args: conv_proba: ultimate share of conversions exp_duration: experiment duration (days) users_per_day: number of users received per day Returns: int: number of registered conversions \u0026#34;\u0026#34;\u0026#34; rv = distribution(conv_proba) conversions = 0 for day in range(exp_duration): data = rv.rvs(users_per_day) conversions += sum((data \u0026gt; DEFAULT) \u0026amp; (data \u0026lt;= day)) return conversions def monte_carlo( aa_test: bool = True, verbose: bool = True, n_iterations: int = 1_000, exp_duration: int = DURATION_DAYS, users_per_day: int = USERS_PER_DAY, p_general: float = CONVERSION, n_groups: int = N_GROUPS, effect_size: float = EFFECT_SIZE, alpha: float = 0.10, **kwargs, ) -\u0026gt; tuple: input = dict.fromkeys([\u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;]) input[\u0026#34;names\u0026#34;] = [chr(ord(\u0026#34;A\u0026#34;) + i) for i in range(n_groups)] input[\u0026#34;variant\u0026#34;] = input[\u0026#34;names\u0026#34;][-1] dunnett_positives = 0 z_positives = 0 for i in range(n_iterations): input[\u0026#34;base\u0026#34;] = [exp_duration * users_per_day] * n_groups input[\u0026#34;target\u0026#34;] = [ generate_sample( p_general, exp_duration, users_per_day, **kwargs ) for _ in range(n_groups - 1) ] input[\u0026#34;target\u0026#34;] += [ generate_sample( p_general * (1 + effect_size * (1 - aa_test)), exp_duration, users_per_day, **kwargs ) ] dunnett_test = ZDunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) dunnett_p_value = dunnett_test.groups_results(alternative=\u0026#34;two-sided\u0026#34;)[\u0026#34;p-value\u0026#34;] if aa_test: if max(dunnett_p_value \u0026lt;= alpha): dunnett_positives += 1 else: if isinstance(dunnett_p_value, np.ndarray) and dunnett_p_value[-1] \u0026lt;= alpha: dunnett_positives += 1 elif isinstance(dunnett_p_value, np.float64) and dunnett_p_value \u0026lt;= alpha: dunnett_positives += 1 dl, dr = proportion_confint(count=dunnett_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) if verbose: buffer_window = kwargs.get(\u0026#34;extra_waiting_days\u0026#34;) baking_window = kwargs.get(\u0026#34;baking_window\u0026#34;) print ( f\u0026#34;{\u0026#39;FPR \u0026#39; if aa_test else f\u0026#39;TPR of {effect_size:.0%} effect size {chr(10)}\u0026#39;}\u0026#34; f\u0026#34;for {exp_duration}-day\u0026#39;s experiment\u0026#34; f\u0026#34;{f\u0026#39;{chr(10)}with {buffer_window} buffer days in the end\u0026#39; if buffer_window else \u0026#39;\u0026#39;}\u0026#34; f\u0026#34;{f\u0026#39;{chr(10)}with baking window of {baking_window} days\u0026#39; if baking_window else \u0026#39;\u0026#39;}\u0026#34; f\u0026#34;{chr(10)}where {users_per_day:,} users are exposed per day:{chr(10)}\u0026#34; f\u0026#34; {dunnett_positives / n_iterations:.3f} ¬± {(dr - dl) / 2:.3f}\u0026#34; ) return [dl, dunnett_positives / n_iterations, dr] Correctness It\u0026rsquo;s never an excess to double check that the criterion that is used works as expected.\nCode\r%%time np.random.seed(2024) _ = monte_carlo() FPR for 21-day's experiment\rwhere 10,000 users are exposed per day:\r0.101 ¬± 0.016\rCPU times: total: 8min\rWall time: 8min 5s\rAlpha is way between the bounds for Type I error rate, so we\u0026rsquo;re free to go ahead.\nPower In case of no pause after the latest visitor exposure - the power falls off to 60%, just a reminder that it supposed to be 80%, insane!\nCode\r%%time np.random.seed(2024) _ = monte_carlo(aa_test=False) TPR of 12% effect size for 21-day's experiment\rwhere 10,000 users are exposed per day:\r0.568 ¬± 0.026\rCPU times: total: 7min 29s\rWall time: 7min 32s\rLet\u0026rsquo;s explore what happens to the power if we are not in a hurry with conclusions.\nCode\rdef generate_sample( conv_proba: float, exp_duration: int, users_per_day: int, extra_waiting_days: int, ) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Extra Args: extra_waiting_days: how many days we wait until drawing a conclusion \u0026#34;\u0026#34;\u0026#34; rv = distribution(conv_proba) conversions = 0 for day in range(exp_duration): data = rv.rvs(users_per_day) conversions += sum((data \u0026gt; DEFAULT) \u0026amp; (data \u0026lt;= day + extra_waiting_days)) return conversions A new way to generate a sample provides and extra opportunity to select how many days we\u0026rsquo;re ready to wait until the experiment is finished. Be careful as the code below takes time!\nCode\r%%time np.random.seed(2024) power_with_waiting = [] for to_wait_for in tqdm(range(1, 22, 2)): result = monte_carlo(aa_test=False, verbose=False, extra_waiting_days=to_wait_for) power_with_waiting.append(result) CPU times: total: 1h 17min 19s\rWall time: 1h 17min 40s\rCode\r%%time np.random.seed(2024) power_with_waiting_4weeks = [] for to_wait_for in tqdm(range(1, 22, 2)): result = monte_carlo(aa_test=False, verbose=False, exp_duration=28, effect_size=0.10, extra_waiting_days=to_wait_for) power_with_waiting_4weeks.append(result) CPU times: total: 1h 37min 48s\rWall time: 1h 37min 56s\rThe chart makes it clear how important it is - not to rush with conclusions, in parenthesis, in everyday life it works exactly the same! We lose about 10 percentage points of the power if we are not ready to wait yet another week.\nCode\rimport plotly.express as px import plotly.graph_objs as go def hex2rgba(hex, alpha): \u0026#34;\u0026#34;\u0026#34; Convert plotly hex colors to rgb and enables transparency adjustment \u0026#34;\u0026#34;\u0026#34; col_hex = hex.lstrip(\u0026#39;#\u0026#39;) col_rgb = tuple(int(col_hex[i : i + 2], 16) for i in (0, 2, 4)) col_rgb += (alpha,) return \u0026#39;rgba\u0026#39; + str(col_rgb) def get_new_color(colors): while True: for color in colors: yield color colors_list = px.colors.qualitative.Plotly rgba_colors = [hex2rgba(color, alpha=0.5) for color in colors_list] palette = get_new_color(rgba_colors) def add_chart(figure, data, title=None, showtitle=False): x = list(range(1, 22, 2)) color = next(palette) figure.add_trace( go.Scatter( name=title if title else \u0026#34;Point Estimate\u0026#34;, x=x, y=[v[1] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(color=color), showlegend=showtitle, ), ) figure.add_trace( go.Scatter( name=\u0026#39;Upper Bound\u0026#39;, x=x, y=[v[2] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(width=0), marker=dict(color=\u0026#34;#444\u0026#34;), hovertemplate=\u0026#34;%{y:.3f}\u0026#34;, showlegend=False, ), ) figure.add_trace( go.Scatter( name=\u0026#39;Lower Bound\u0026#39;, x=x, y=[v[0] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(width=0), marker=dict(color=\u0026#34;#444\u0026#34;), hovertemplate=\u0026#34;%{y:.3f}\u0026#34;, showlegend=False, fillcolor=color, fill=\u0026#39;tonexty\u0026#39;, ), ) figure = go.Figure() add_chart(figure, power_with_waiting) figure.update_xaxes( title_text=\u0026#34;Days passed since the exposure was stopped until a conclusion was made\u0026#34; ) figure.update_layout( yaxis_title=\u0026#39;True Positive Rate\u0026#39;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: \u0026#39;Power of criterion\u0026#39;, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) figure.show() Essentially, it\u0026rsquo;s a mediocre example, when it comes to the real experiments we usually have more hidden variables that impact metrics maturation: working days and hours, holidays, etc. To show you the dependency on the day of the week let me add a basic weekly seasonality. In simple words a delay to maturation of the metric during the weekend is added, that applies a sense of a business process that can\u0026rsquo;t be done on a weekend so conversion happens only during the working week. Of course it\u0026rsquo;s intensely simplified example that only takes into account the first week of the maturation and doesn\u0026rsquo;t account for users variation per day, the result speaks for itself though.\nCode\rdef generate_sample( conv_proba: float, exp_duration: int, users_per_day: int, extra_waiting_days: int, weekday_exp_start: int, ) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Extra Args: weekday_exp_start: number of a weekday in ISO format From 1 through 7, where Monday = 1 and so on \u0026#34;\u0026#34;\u0026#34; rv = distribution(conv_proba) conversions = 0 for day in range(exp_duration): data = rv.rvs(users_per_day) if weekday_exp_start and (weekday_exp_start + day) % 6 == 0: conversions += sum((data \u0026gt; DEFAULT) \u0026amp; (data + 2 \u0026lt;= day + extra_waiting_days)) elif weekday_exp_start and (weekday_exp_start + day) % 7 == 0: conversions += sum((data \u0026gt; DEFAULT) \u0026amp; (data + 1 \u0026lt;= day + extra_waiting_days)) else: conversions += sum((data \u0026gt; DEFAULT) \u0026amp; (data \u0026lt;= day + extra_waiting_days)) return conversions As a rule of thumb the experiments are run during the working week only and are analyzed during the working week, usually if one was run on the night from Monday to Tuesday it will be stopped on the corresponding night a few weeks later and analyzed on the next day. I suggest we reproducing this set up and assume we draw a conclusion at the next day after stopping (for simplicity you can imagine that we run all experiments at midnight). How the power of an experiment depends on a day of the week when it was run?\nCode\r%%time np.random.seed(2024) weekday_power = [] for day_of_week in tqdm(range(1, 6)): result = monte_carlo(aa_test=False, verbose=False, extra_waiting_days=0, weekday_exp_start=day_of_week) weekday_power.append(result) CPU times: total: 33min 27s\rWall time: 33min 28s\rCode\rimport plotly.graph_objects as go fig = go.Figure() fig.add_trace(go.Bar( name=\u0026#39;TPR\u0026#39;, x=[\u0026#39;Mon\u0026#39;, \u0026#39;Tue\u0026#39;, \u0026#39;Wed\u0026#39;, \u0026#39;Thu\u0026#39;, \u0026#39;Fri\u0026#39;], y=[v[1] for v in weekday_power], marker_color=next(palette), error_y=dict(type=\u0026#39;data\u0026#39;, array=[round(v[1] - v[0], 3) for v in weekday_power]), )) fig.update_layout( yaxis_title=\u0026#39;True Positive Rate\u0026#39;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: \u0026#39;Power of criterion\u0026#39;, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) fig.show() As expected, worst day of the week scenario is Monday, interesting what happens to the power if we\u0026rsquo;re ready to wait with analysis for an extra week until the next Monday?\nCode\rnp.random.seed(2024) _ = monte_carlo(aa_test=False, extra_waiting_days=7, weekday_exp_start=1) TPR of 12% effect size for 21-day's experiment\rwith 7 buffer days in the end\rwhere 10,000 users are exposed per day:\r0.623 ¬± 0.025\rIt improves the power and levels the weekday effect out; it\u0026rsquo;s what I suggest you take away: any extra dependency complicates the process and may decrease the power, although to wait for an extra week to let the metric reach its maturation appears to be a solution anyway.\nBaking Window Often it\u0026rsquo;s suggested to use a baking window concept: so to register a conversion during the defined time frame respectively for every cohort, it\u0026rsquo;s a viable option and always a go-to technique. Let\u0026rsquo;s compare such a \u0026ldquo;cohort based\u0026rdquo; approach power against the basis \u0026ldquo;count everything that\u0026rsquo;s converted so far\u0026rdquo;.\nCode\rdef generate_sample( conv_proba: float, exp_duration: int, users_per_day: int, baking_window: int, ) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Extra Args: baking_window: only count a conversion if it happens during this number of days \u0026#34;\u0026#34;\u0026#34; rv = distribution(conv_proba) data = rv.rvs(users_per_day * exp_duration) conversions = sum((data \u0026gt; DEFAULT) \u0026amp; (data \u0026lt; baking_window)) return conversions We need to recalculate sample size as it would be different because now the ultimate conversion is lower, we only take into account specific time frame for every cohort.\nCode\rinput = dict() input[\u0026#34;names\u0026#34;] = [chr(ord(\u0026#34;A\u0026#34;) + i) for i in range(N_GROUPS)] input[\u0026#34;variant\u0026#34;] = input[\u0026#34;names\u0026#34;][-1] input[\u0026#34;base\u0026#34;] = [DURATION_DAYS * USERS_PER_DAY] * N_GROUPS input[\u0026#34;target\u0026#34;] = [generate_sample(CONVERSION, DURATION_DAYS, USERS_PER_DAY, baking_window=7) for _ in range(N_GROUPS)] test = ZDunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) test.groups_results() {'variant': ['B', 'C'],\r'statistic': array([0.77838058, 0.30579237]),\r'p-value': array([0.65239556, 0.93402278])}\rCode\rmin_sample_size(mde=EFFECT_SIZE * np.mean(test.p_samples / test.n_samples), var=test.variance) 344708\rAs you can see more days of sampling are required for this technique, followed by those after the end of experiment when we wait for a convergence of a conversion.\nCode\rnp.random.seed(2024) _ = monte_carlo(aa_test=False, baking_window=7) TPR of 12% effect size for 21-day's experiment\rwith baking window of 7 days\rwhere 10,000 users are exposed per day:\r0.594 ¬± 0.026\rIf no more days provided - then the power is rather low, although for more extensive experiment it\u0026rsquo;s mush higher.\nCode\rnp.random.seed(2024) _ = monte_carlo(aa_test=False, exp_duration=28, baking_window=7) TPR of 12% effect size for 28-day's experiment\rwith baking window of 7 days\rwhere 10,000 users are exposed per day:\r0.712 ¬± 0.024\rSo, it\u0026rsquo;s interesting which strategy outperforms another one, so let\u0026rsquo;s make a comparison for different time spans.\nthe first one it\u0026rsquo;s just how many days we wait until the end of experiment before the analysis with the second one in addition baking window is applied, so we only count a conversion within a specific window that is exact same width like the break after the end of the experiment Code\r%%time np.random.seed(2024) power_baking_window = [] for to_wait_for in tqdm(range(1, 22, 2)): result = monte_carlo(aa_test=False, verbose=False, baking_window=to_wait_for) power_baking_window.append(result) CPU times: total: 1h 16min 21s\rWall time: 1h 16min 40s\rCode\rfigure = go.Figure() add_chart(figure, power_with_waiting, title=\u0026#34;w/o baking window\u0026#34;, showtitle=True) add_chart(figure, power_baking_window, title=\u0026#34;with baking window\u0026#34;, showtitle=True) figure.update_xaxes( title_text=\u0026#34;Days passed since the exposure was stopped (doubles up as baking window width)\u0026#34; ) figure.update_layout( yaxis_title=\u0026#39;True Positive Rate\u0026#39;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: \u0026#39;Power of criterion (3 weeks experiment)\u0026#39;, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) figure.show() So, for the analyzed set up it\u0026rsquo;s obvious that the basic approach is a winner and uniformly no less powerful. It provides a higher power by a wide margin if you wait just a few days after the experiment while if you have a forbearance to wait longer the difference is not significant.\nCode\r%%time np.random.seed(2024) power_baking_window_4weeks = [] for to_wait_for in tqdm(range(1, 22, 2)): result = monte_carlo(aa_test=False, verbose=False, exp_duration=28, effect_size=0.10, baking_window=to_wait_for) power_baking_window_4weeks.append(result) CPU times: total: 1h 43min 17s\rWall time: 1h 43min 22s\rOne last thing that is important to be mentioned is how this tendency is affected by a duration of an experiment. It\u0026rsquo;s quite obvious that the longer experiment takes the less significant will be an effect of waiting (under the conditions of the same target metric maturation curve). Here is the comparison similar to one above but for 4-weeks experiment with a little less effect size, so it shall show close power numbers.\nCode\rfigure = go.Figure() add_chart(figure, power_with_waiting_4weeks, title=\u0026#34;w/o baking window\u0026#34;, showtitle=True) add_chart(figure, power_baking_window_4weeks, title=\u0026#34;with baking window\u0026#34;, showtitle=True) figure.update_xaxes( title_text=\u0026#34;Days passed since the exposure was stopped (doubles up as baking window width)\u0026#34; ) figure.update_layout( yaxis_title=\u0026#39;True Positive Rate\u0026#39;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: \u0026#39;Power of –°riterion (4 weeks experiment)\u0026#39;, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) figure.show() So, two observations:\nit seems that no matter what is the experiment\u0026rsquo;s duration the strategy where all events are counted works better in case of a short baking window waiting at least for a week after the experiment is over for more conversions to happen is must for short experiments only Conclusion Now you know, how important it\u0026rsquo;s to give your metric time to mature and don\u0026rsquo;t rush with the conclusions for \u0026ldquo;gray\u0026rdquo; experiments, even if you didn\u0026rsquo;t reach statistically significant result immediately after you stopped the exposure to the experiment, it doesn\u0026rsquo;t mean that you would finally get it a little later.\nThe effect is especially great for the shorter experiments, within your data specific the time frames may strongly vary although this rule is an invariant, the longer you run it the less power you lose when drawing a conclusion right after the experiment\u0026rsquo;s finish. However it totally makes sense to give some time to the recent users to have their conversions if you want to take them into account in the same way as those who came earlier, otherwise the latest users performance is neglected and the conclusion relies on those who came earlier mostly.\nThe final thought is that not every time well-known techniques fits your personal set up, in our case it was easy to show that movement to a baking window can only either (or even both simultaneously) drop the power down or push the experiment duration up, so the simpler default approach works uniformly better.\n","permalink":"https://npodlozhniy.github.io/posts/maturing-conversions/","summary":"Why you need to be patient and when cohort-wise approach is a flaw","title":"Maturing Conversions in AB-experiment"},{"content":"Background We at HomeBuddy run a various number of AB tests to improve customer journey. A big part of efforts is allocated to onboarding funnel, hence the main metrics are conversions throughout this funnel. Usually we design multivariate tests with a few testing groups reflecting slightly different variations (often in terms of actual design) of the business hypothesis behind. No matter how you run the experiments you want to get an accurate and powerful procedure, that\u0026rsquo;s why we use Dunnett\u0026rsquo;s correction for all of the experiments where we have to maximize the power of AB engine. Are you curious what it is?\nPrerequisites It\u0026rsquo;s expected that the reader has an experience with Python and its main libraries for data analysis. The actual notebook was written on Python 3.11.4 and to keep the results reproducible here is the list of particular packages that are being used in the article specified with their versions.\nCode\rpip install --quiet --upgrade numpy==1.25.2 scipy==1.11.2 statsmodels==0.14.0 podlozhnyy_module==2.4b0 Problem Definition Imagine we want to optimize an onboarding funnel of an arbitrary product applying a new business idea. We don\u0026rsquo;t want to rely on expert assessment only and hence opt for designing an AB test first. The target metric is an abstract conversion and we carry out a classical AB test with one treatment group.\nIt\u0026rsquo;s not a secret that in such a scenario the best practice is a standard Z-test procedure for independent proportions\n$$ Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{P(1 - P)(\\frac{1}{n_1} + \\frac{1}{n_2})}} \\space\\text{,where}\\space P = \\frac{\\hat{p}_1n_1 + \\hat{p}_2n_2}{n_1 + n_2} $$\nUnder the conditions of the truth of the null hypothesis the statistic follows the standard normal distribution\n$$ Z \\stackrel{H_0}{\\sim} \\mathbb{N}(0, 1) $$\nCode\rimport numpy as np from scipy.stats import binom from statsmodels.stats.proportion import proportions_ztest, proportion_confint There are multiple versions of classical Z-test in Python and even though those have its unique selling points this time I apply my own implementation to stay consistent across this article using the same interface for all criteria.\nCode\rfrom podlozhnyy_module.abtesting import Dunnett, Ztest Interface As a brief check that the method is working as expected and to make you acquainted with its interface let\u0026rsquo;s identify a dummy experiment\ninput must consist of at least three lists:\nbase is the basis event that stands as a denominator of conversion target is the goal event - conversion numerator groups are names of the experiment variants In addition variant is specified to explicitly define which group we\u0026rsquo;re focus on (it makes sense in case of multivariate testing)\nCode\rinput = { \u0026#34;base\u0026#34;: [ 480, 437, ], \u0026#34;target\u0026#34;: [ 32, 37, ], \u0026#34;groups\u0026#34;: [ \u0026#34;test\u0026#34;, \u0026#34;control\u0026#34;, ], \u0026#34;variant\u0026#34;: \u0026#34;test\u0026#34;, } Functionality is written leveraging well-known OOP principles, so the experiment is not a function that stands aside, but a class which besides input dictionary gets the keys of the corresponding entities.\nCode\rtest = Ztest(input, base=\u0026#34;base\u0026#34;, target=\u0026#34;target\u0026#34;, groups=\u0026#34;groups\u0026#34;, variant=\u0026#34;variant\u0026#34;) print(f\u0026#34;p-value: {test.variant_pvalue(alternative=\u0026#39;less\u0026#39;):.3f}, Z-statistic: {test.statistic[0]:.3f}\u0026#34;) p-value: 0.151, Z-statistic: -1.032\rFor an on fly verification, the results might be compared to the output of a well known statsmodels package.\nCode\rcount = input[\u0026#34;target\u0026#34;] nobs = input[\u0026#34;base\u0026#34;] stat, pval = proportions_ztest(count, nobs, alternative=\u0026#39;smaller\u0026#39;) print(\u0026#39;p-value: {0:0.3f}, Z-statistic: {1:0.3f}\u0026#39;.format(pval, stat)) p-value: 0.151, Z-statistic: -1.032\rThe numbers coincide and now we\u0026rsquo;re ready to move to the main topic.\nTheory While often basic Z-test procedure is the appropriate option for AB analysis, it doesn\u0026rsquo;t satisfy the genuine requirements for the statistical method when it comes to multiple hypothesis testing. I hope you understand what is the problem in case of having $n$ independent metrics in your test: if you set up acceptable Type I error rate $\\alpha$ for each of them then the total probability to get at least one significant difference (dubbed as FWER) under the conditions of the truth of the null hypothesis (which means no difference between the groups by design) would be not $\\alpha$ but $1 - (1 - \\alpha)^n$ what totally invalidates the procedure.\nThere are two possible scenarios: either we have multiple metrics that we want to compare across the test and control group or we have several testing groups that apply different treatment to customers. Whilst the first problem is more popular by a wide margin and has lots of solutions, the second one is often neglected in the business industry and generally the same procedures are used to solve it. If you apply any type of corrections for multiple testing you\u0026rsquo;re already ahead of 80% of teams that don\u0026rsquo;t, although what I want to show you is that the second scenario must be treated differently to extract more insights from your experiments.\nDunnet\u0026rsquo;s test is a multiple comparison procedure developed specifically to compare each of a number of treatment groups to a control one, extended original paper was released in 1964. Dunnett\u0026rsquo;s test is not a set aside procedure but more like an extension of monumental Student\u0026rsquo;s T-test, for a specific design where every treatment group is compared to a single control one, which is leveraged to take into account the dependencies between comparisons. In case of proportions it\u0026rsquo;s a Z-test, as long as we don\u0026rsquo;t need to estimate variance for binomial distribution in addition to the probability of success.\nThe main trick is to calculate variance in a different way, just to remind you in case of a standard procedure statistic looks like\n$$ Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{S\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} $$\nwhere $S$ is a standard error estimate comprises the squared root of variance of a combined sample $\\sigma^2 = P(1 - P)$\nDunnett\u0026rsquo;s test statistic looks exactly the same for every treatment group with the only difference laying under how variance is calculated. In general in case of $n$ treatment groups and one control group $i=0$ of observations $(X_0, .. X_{N_i})$ with $N_i$ size of each, the formula is the following:\n$$ S^{2} = \\frac{\\sum_{i=0}^{n}S_i^2}{df} = \\frac{\\sum_{i=0}^{n}\\sum_{j=1}^{N_i}(X_{ij} - \\bar{X_i})^2}{df} = \\frac{\\sum_{i=0}^{n}\\sum_{j=1}^{N_i}X_{ij}^2 - n\\bar{X_i}^2}{df} $$\nwhere $df$ is degrees of freedom\n$$ df = \\sum_{i=0}^{n}N_{i} - (n + 1) $$\nFor proportions this \u0026ldquo;pooled\u0026rdquo; variance simplifies even further as long as the rest part of calculations is exactly the same\n$$ S^{2} = \\frac{\\sum_{i=0}^{n}{N_i\\bar{p_i}(1 - \\bar{p_i})}}{df} $$\nCanonical AB test First of all, to guarantee the accuracy we should challenge this specific criterion against a classical one in case when both of them are applicable - standard AB test. What is good about proportions is that we can easily simulate the data in the blink of an eye, so we\u0026rsquo;re setting up a simulation and employ Monte Carlo process to check two things:\ncorrectness - the criterion should meet the identified confidence level, which means that in case of AA comparison we should get Type I error in $\\alpha$ percent of experiments power - as it comes from the theory in case of only two groups Dunnett\u0026rsquo;s test is equal to a classical Z-test, so shall we call the implementation out? In addition to a point estimate of False Positive Rate I suggest building a 90% confidence interval to be more precise in the comparisons\nCode\rdef generate_sample(size: int, prob: float) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Return the number of target events \u0026#34;\u0026#34;\u0026#34; return binom(n=size, p=prob).rvs() Correctness in AA Code\rnp.random.seed(2024) alpha = 0.05 n_iterations = 1000 p_general = 0.1 input = dict.fromkeys([\u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;]) input[\u0026#34;names\u0026#34;] = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] input[\u0026#34;variant\u0026#34;] = \u0026#34;B\u0026#34; for size in map(int, [1e2, 1e3, 1e4]): dunnett_positives = 0 z_positives = 0 for i in range(n_iterations): input[\u0026#34;base\u0026#34;] = [size, size] input[\u0026#34;target\u0026#34;] = [generate_sample(size, p_general), generate_sample(size, p_general)] dunnett_test = Dunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) z_test = Ztest(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) dunnett_p_value = dunnett_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) z_p_value = z_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) if dunnett_p_value \u0026lt;= alpha: dunnett_positives += 1 if z_p_value \u0026lt;= alpha: z_positives += 1 print(f\u0026#39;FPR for sample size {size}\u0026#39;) l, r = proportion_confint(count=dunnett_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Dunnet: {dunnett_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) l, r = proportion_confint(count=z_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Z-test: {z_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) print() FPR for sample size 100\rDunnet: 0.048 ¬± 0.011\rZ-test: 0.048 ¬± 0.011\rFPR for sample size 1000\rDunnet: 0.050 ¬± 0.011\rZ-test: 0.050 ¬± 0.011\rFPR for sample size 10000\rDunnet: 0.051 ¬± 0.011\rZ-test: 0.051 ¬± 0.011\rAmazing! It seems that 0.05 every time lies in the 90% confidence interval for FPR and hence the criterion is valid and moreover the numbers are exactly the same, it\u0026rsquo;s what was expected and now let\u0026rsquo;s check the power too.\nPower in AB Code\rnp.random.seed(2024) alpha = 0.05 n_iterations = 1000 p_general = 0.10 effect_size = 0.2 input = dict.fromkeys([\u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;]) input[\u0026#34;names\u0026#34;] = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] input[\u0026#34;variant\u0026#34;] = \u0026#34;B\u0026#34; for size in map(int, [1e2, 1e3, 1e4]): dunnett_positives = 0 z_positives = 0 for i in range(n_iterations): input[\u0026#34;base\u0026#34;] = [size, size] input[\u0026#34;target\u0026#34;] = [generate_sample(size, p_general), generate_sample(size, p_general * (1 + effect_size))] dunnett_test = Dunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) z_test = Ztest(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) dunnett_p_value = dunnett_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) z_p_value = z_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) if dunnett_p_value \u0026lt;= alpha: dunnett_positives += 1 if z_p_value \u0026lt;= alpha: z_positives += 1 print(f\u0026#39;TPR of {effect_size:.0%} effect size for sample size {size}\u0026#39;) l, r = proportion_confint(count=dunnett_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Dunnet: {dunnett_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) l, r = proportion_confint(count=z_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Z-test: {z_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) print() TPR of 20% effect size for sample size 100\rDunnet: 0.085 ¬± 0.015\rZ-test: 0.085 ¬± 0.015\rTPR of 20% effect size for sample size 1000\rDunnet: 0.306 ¬± 0.024\rZ-test: 0.306 ¬± 0.024\rTPR of 20% effect size for sample size 10000\rDunnet: 0.992 ¬± 0.005\rZ-test: 0.992 ¬± 0.005\rWe are well on our way - the numbers are exactly the same, which means that in the case of 2 groups, Dunnett\u0026rsquo;s test is at least as powerful as a standard procedure. It\u0026rsquo;s time to challenge it in a way it\u0026rsquo;s supposed to be used: meet multivariate testing!\nMultivariate ABC Monte Carlo Now we will track how the criterion controls not a single FPR, but family-wise error rate (FWER) and what is more in order to continue the comparison with a classical Z-test the latter needs to have Bonferroni correction applied otherwise it wouldn\u0026rsquo;t properly control FWER.\nCode\rdef fwe(x: np.ndarray, alpha: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Indicates either at least one of null hypotheses is rejected \u0026#34;\u0026#34;\u0026#34; return max(x \u0026lt;= alpha) def bonferroni_fwe(x: np.ndarray, alpha: float, n: int) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Bonferroni correction for FWER, you can think of as it\u0026#39;s Bonferroni-Holm procedure Because to have a False Positive result it\u0026#39;s necessary and sufficient that at least one of `n` p-values doesn\u0026#39;t exceed `alpha` / `n` \u0026#34;\u0026#34;\u0026#34; return fwe(x, alpha / n) To simplify the code in the future Monte Carlo procedure is wrapped into a function with all the necessary parameters supplied as arguments.\nCode\rdef monte_carlo( aa_test: bool = True, verbose: bool = True, n_iterations: int = 1000, group_size: int = 100, n_groups: int = 3, p_general: float = 0.1, effect_size: float = 0.2, alpha: float = 0.05, ) -\u0026gt; dict: input = dict.fromkeys([\u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;]) input[\u0026#34;names\u0026#34;] = [chr(ord(\u0026#34;A\u0026#34;) + i) for i in range(n_groups)] input[\u0026#34;variant\u0026#34;] = input[\u0026#34;names\u0026#34;][-1] dunnett_positives = 0 z_positives = 0 for i in range(n_iterations): input[\u0026#34;base\u0026#34;] = [group_size] * n_groups input[\u0026#34;target\u0026#34;] = [generate_sample(group_size, p_general) for _ in range(n_groups - 1)] input[\u0026#34;target\u0026#34;] += [generate_sample(group_size, p_general * (1 + effect_size * (1 - aa_test)))] dunnett_test = Dunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) z_test = Ztest(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) dunnett_p_value = dunnett_test.groups_results(alternative=\u0026#34;two-sided\u0026#34;)[\u0026#34;p-value\u0026#34;] z_p_value = z_test.groups_results(alternative=\u0026#34;two-sided\u0026#34;)[\u0026#34;p-value\u0026#34;] if aa_test: if fwe(dunnett_p_value, alpha): dunnett_positives += 1 if bonferroni_fwe(z_p_value, alpha, 2): z_positives += 1 else: if isinstance(dunnett_p_value, np.ndarray) and dunnett_p_value[-1] \u0026lt;= alpha: dunnett_positives += 1 elif isinstance(dunnett_p_value, np.float64) and dunnett_p_value \u0026lt;= alpha: dunnett_positives += 1 if sidak_holm(z_p_value, alpha)[-1]: z_positives += 1 dl, dr = proportion_confint(count=dunnett_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) zl, zr = proportion_confint(count=z_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) if verbose: print ( f\u0026#34;{\u0026#39;FPR\u0026#39; if aa_test else f\u0026#39;TPR of {effect_size:.0%} effect size\u0026#39;} for sample size {group_size}\\n\u0026#34; f\u0026#34; - Dunnett: {dunnett_positives / n_iterations:.3f} ¬± {(dr - dl) / 2:.3f}\\n\u0026#34; f\u0026#34; - Z-Test: {z_positives / n_iterations:.3f} ¬± {(zr - zl) / 2:.3f}\\n\u0026#34; ) return {\u0026#34;dunnett\u0026#34;: [dl, dunnett_positives / n_iterations, dr], \u0026#34;z-test\u0026#34;: [zl, z_positives / n_iterations, zr]} Correctness Validity first, let\u0026rsquo;s check the ability to control FWER at predefined $\\alpha$ level.\nCode\rnp.random.seed(2024) for size in [1e2, 5e2, 1e3, 5e3]: _ = monte_carlo(aa_test=True, group_size=int(size)) FPR for sample size 100\r- Dunnett: 0.045 ¬± 0.011\r- Z-Test: 0.042 ¬± 0.010\rFPR for sample size 500\r- Dunnett: 0.048 ¬± 0.011\r- Z-Test: 0.045 ¬± 0.011\rFPR for sample size 1000\r- Dunnett: 0.063 ¬± 0.013\r- Z-Test: 0.056 ¬± 0.012\rFPR for sample size 5000\r- Dunnett: 0.050 ¬± 0.011\r- Z-Test: 0.046 ¬± 0.011\rSuper cool, both methods: Dunnett\u0026rsquo;s Test without any corrections and Z-Test with Bonferroni-Holm correction control FWER correctly.\nPower Now it\u0026rsquo;s the time to define a full-fledged step-down procedure for multivariate testing. Despite the fact that its shortened version works well to define FWER it doesn\u0026rsquo;t when it comes to a power analysis. I prefer Sidak-Holm procedure as it\u0026rsquo;s known as the most powerful procedure that controls FWER, however as long as sample size is increased the difference from Bonferroni-Holm is hardly noticeable.\nCode\rdef sidak_holm(p_values: np.ndarray, alpha: float) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Step down Sidak-Holm procedure If the statistics are jointly independent, no procedure can be constructed to control FWER that is more powerful than the Sidak-Holm method \u0026#34;\u0026#34;\u0026#34; m = p_values.size adjusted_alpha = np.array([1 - (1 - alpha) ** (1 / (m - i + 1)) for i in range(1, m + 1)]) sorted_indexes = np.argsort(p_values) sorted_pvalues = p_values[sorted_indexes] first_reject = (list(sorted_pvalues \u0026lt;= adjusted_alpha) + [False]).index(False) result = np.array([True] * first_reject + [False] * (m - first_reject)) return result[np.argsort(sorted_indexes)] For the power test I offer to use two treatment groups and a single control where in one of the treatments an effect of 20% uplift is added. So the null hypothesis should be rejected and True Positive Rate is measured - is the share of rejected hypotheses among the number of iterations.\nCode\rnp.random.seed(2024) for size in [1e2, 5e2, 1e3, 5e3]: _ = monte_carlo(aa_test=False, group_size=int(size)) TPR of 20% effect size for sample size 100\r- Dunnett: 0.051 ¬± 0.011\r- Z-Test: 0.043 ¬± 0.011\rTPR of 20% effect size for sample size 500\r- Dunnett: 0.104 ¬± 0.016\r- Z-Test: 0.092 ¬± 0.015\rTPR of 20% effect size for sample size 1000\r- Dunnett: 0.265 ¬± 0.023\r- Z-Test: 0.240 ¬± 0.022\rTPR of 20% effect size for sample size 5000\r- Dunnett: 0.855 ¬± 0.018\r- Z-Test: 0.842 ¬± 0.019\rThe results are promising! The power of Dunnett\u0026rsquo;s test every time exceeds the power of Z-test with Sidak-Holm procedure applied. The difference is not significant though, so we can\u0026rsquo;t say for sure that it\u0026rsquo;s better, let\u0026rsquo;s experiment more with parameters and change effect_size\nCode\rnp.random.seed(2024) for effect_size in [0.1, 0.2, 0.3]: _ = monte_carlo(aa_test=False, group_size=3000, effect_size=effect_size) TPR of 10% effect size for sample size 3000\r- Dunnett: 0.163 ¬± 0.019\r- Z-Test: 0.157 ¬± 0.019\rTPR of 20% effect size for sample size 3000\r- Dunnett: 0.589 ¬± 0.026\r- Z-Test: 0.569 ¬± 0.026\rTPR of 30% effect size for sample size 3000\r- Dunnett: 0.927 ¬± 0.014\r- Z-Test: 0.914 ¬± 0.015\rIt\u0026rsquo;s exciting, the result remains the same, and if we know that Sidak-Holm is the most powerful method that controls FWER for the general use case, we see now that Dunnett\u0026rsquo;s at least not worse. Finally, the most appealing variable is the number of treatment groups, let\u0026rsquo;s vary it too.\nCode\rnp.random.seed(2024) for n_groups in [3, 5, 7]: _ = monte_carlo(aa_test=False, group_size=3000, n_groups=n_groups) TPR of 20% effect size for sample size 3000\r- Dunnett: 0.608 ¬± 0.025\r- Z-Test: 0.601 ¬± 0.025\rTPR of 20% effect size for sample size 3000\r- Dunnett: 0.544 ¬± 0.026\r- Z-Test: 0.506 ¬± 0.026\rTPR of 20% effect size for sample size 3000\r- Dunnett: 0.464 ¬± 0.026\r- Z-Test: 0.424 ¬± 0.026\rNow we\u0026rsquo;ve nailed it! The number of groups is what affects the bottom line. The more groups are in the experiment - the more powerful Dunnett\u0026rsquo;s Correction than Sidak-Holm. So, let\u0026rsquo;s build a title image for this article that illustrates how Dunnett\u0026rsquo;s test outperforms the well-known step-down procedure as the number of treatment groups increases.\nCode\rfrom tqdm.notebook import tqdm np.random.seed(2024) ztest_values = [] dunnett_values = [] for n_groups in tqdm(range(2, 16)): result = monte_carlo(aa_test=False, verbose=False, group_size=3000, n_groups=n_groups) dunnett_values.append(result[\u0026#34;dunnett\u0026#34;]) ztest_values.append(result[\u0026#34;z-test\u0026#34;]) Plotly is used for interactive visualization, hover over the image to see details.\nCode\rimport plotly.express as px import plotly.graph_objs as go def hex2rgba(hex, alpha): \u0026#34;\u0026#34;\u0026#34; Convert plotly hex colors to rgb and enables transparency adjustment \u0026#34;\u0026#34;\u0026#34; col_hex = hex.lstrip(\u0026#39;#\u0026#39;) col_rgb = tuple(int(col_hex[i : i + 2], 16) for i in (0, 2, 4)) col_rgb += (alpha,) return \u0026#39;rgba\u0026#39; + str(col_rgb) def get_new_color(colors): while True: for color in colors: yield color colors_list = px.colors.qualitative.Plotly rgba_colors = [hex2rgba(color, alpha=0.5) for color in colors_list] palette = get_new_color(rgba_colors) def add_chart(figure, data, title): x = list(range(1, 15)) color = next(palette) figure.add_trace( go.Scatter( name=title, x=x, y=[v[1] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(color=color), ), ) figure.add_trace( go.Scatter( name=\u0026#39;Upper Bound\u0026#39;, x=x, y=[v[2] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(width=0), marker=dict(color=\u0026#34;#444\u0026#34;), hovertemplate=\u0026#34;%{y:.3f}\u0026#34;, showlegend=False, ), ) figure.add_trace( go.Scatter( name=\u0026#39;Lower Bound\u0026#39;, x=x, y=[v[0] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(width=0), marker=dict(color=\u0026#34;#444\u0026#34;), hovertemplate=\u0026#34;%{y:.3f}\u0026#34;, showlegend=False, fillcolor=color, fill=\u0026#39;tonexty\u0026#39;, ), ) figure = go.Figure() add_chart(figure, dunnett_values, \u0026#34;Dunnnett\u0026#39;s Correction\u0026#34;) add_chart(figure, ztest_values, \u0026#39;Z-Test with Sidak-Holm\u0026#39;) figure.update_xaxes( title_text=\u0026#34;Number of Treatment Groups\u0026#34; ) figure.update_layout( yaxis_title=\u0026#39;True Positive Rate\u0026#39;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: \u0026#39;Power of –°riterion\u0026#39;, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) figure.show() Conclusion It was shown that when the experiment design satisfies the premises of Dunnett\u0026rsquo;s Test applicability (only $n$ comparisons of $n$ test groups against a single control) at least in a specific case of conversion metrics, Dunnett\u0026rsquo;s correction is more powerful that the standard step-down procedures like Sidak-Holm.\nWhile Dunnet\u0026rsquo;s correction is a definite winner it doesn\u0026rsquo;t mean that Sidak-Holm is abandoned from now on in our team, the proper design would be to use Dunnett\u0026rsquo;s correction first for multivariate testing and Sidak-Holm procedure must be applied on top if there are multiple metrics to compare between the groups, which is often the case.\nAdditional Information Original Paper from Journal of the American Statistical Association Multivariate Testing - Best Practices Dunnett\u0026rsquo;s Correction in Analytics ToolKit Multiple comparisons problem ","permalink":"https://npodlozhniy.github.io/posts/dunnett-correction/","summary":"Improve your multivariate testing framework power and roll out the experiments much faster","title":"Dunnett's Correction for ABC testing"},{"content":"\rBackground Every company has always taken pride in providing excellent customer service, therefore it\u0026rsquo;s crucial as part of ongoing improvements for product to collect and analyse feedback after each customer interaction on support channels\nA top-notch analyst in Hi-Tech industry should be in capable of handle at least base feedback analysis quickly and efficiently and going through this article you will be introduced with the necessary concepts and eventually given the base guide on how to unlock the true power of text data\nWell, no more words, let\u0026rsquo;s have a look on how\nDisclaimer: I\u0026rsquo;m not pretending on having this article as an exhaustive everything you need notebook for analyzing customer feedback, but it\u0026rsquo;s the steps I usually follow while working on customers text data and on the basis of my previous experience - 80% of your needs is covered here\nPrerequisites It\u0026rsquo;s expected that the reader has an experience with Python and it\u0026rsquo;s main Data Analysis libraries\nThe actual notebook was written on Python 3.7.9 and to keep the results reproducible here is the list of particular packages that are being used in the article specified with their versions\nCode\rnumpy==1.21.6 pandas==1.3.5 pandas-profiling==3.1.0 matplotlib==3.4.2 unidecode==1.3.7 nltk==3.6.2 sklearn==0.24.2 wordcloud==1.9.2 shap==0.42.1 transformers==4.30.2 gensim==4.0.1 Methodology During this notebook the feedback which comes in the form of ratings (from 1-5) and textual comments is considered. The general purpose is to dive deeper into this feedback, identify common themes, if certain issues lead to more negative feedback than others and understand areas of improvement\nCode\rimport numpy as np import pandas as pd import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from matplotlib import pyplot as plt %matplotlib inline As a general rule the analysis should be organized in a top-down manner, simple exploration and low-hanging fruits first and sophisticated approach after only if you really need more in-depth expertise. According to the above the analysis will be organized in the three steps:\nData Mining - simple unsupervised data exploration to have a general idea of the data nature Sentiment Analysis - search on what matters the most which reasons drive review to be positive or negative Topic Modelling - main feedback themes extraction, improvement focuses identification Analysis Process Data Mining Basic EDA First of all - let\u0026rsquo;s have a fluent look at the suggested data, pandas-profiling is a very useful tool to perform basic and boring Exploratory Data Analysis in a minute. View Data Profile by clicking here\nCode\rfrom pandas_profiling import ProfileReport df = pd.read_csv(\u0026#39;feedback-data-sample.csv\u0026#39;, index_col=0) profile = ProfileReport( df, minimal=True, dark_mode=True, title=\u0026#34;Feedback Data Report\u0026#34;, ) profile.to_file(\u0026#34;data-profile.html\u0026#34;) Well, what are the main data patterns\nthe dataset constitutes 1276 tickets with customer feedback consisted of csat_score and comment ticket_id - is a primary key, given that we don\u0026rsquo;t have timestamp column and it\u0026rsquo;s looks like bigint not a random hash, it\u0026rsquo;s better to sort by it beforehand, to be sure we are not predicting the past on the future features all the reviews are in English language, good for us, we can forget about additional translators and this column in general csat_score has only two different values 0 and 4 stars, and the majority of reviews are positive, well not much, but it\u0026rsquo;s even easier to transform it into a bool target and work with it further comment has a few NULL values, let\u0026rsquo;s keep it in mind from the simple frequentist analysis it\u0026rsquo;s clear that tokens not and very might be informative, so don\u0026rsquo;t forget to exclude them from stop words list Code\rdf = df.sort_index().drop(columns={\u0026#39;language\u0026#39;}) def define_sentiment(rating: float) -\u0026gt; int: if rating \u0026lt; 3: return -1 # negative sentiment elif rating \u0026gt; 3: return 1 # positive sentiment else: return 0 # neutral sentiment df[\u0026#39;sentiment\u0026#39;] = df[\u0026#39;csat_score\u0026#39;].apply(define_sentiment) Text cleaning Barely the role of data cleaning might be underestimated, it\u0026rsquo;s an incredibly important step, if this is skipped the rest of analysis doesn\u0026rsquo;t make any sense then.\nMain cleaning that should be applied:\nremove all the symbols and keep only words remove redundant short words remove general language words transliterate unicode symbols to ascii lowercase The next step is tokenization: the are two main approaches here:\nstemming - fast process of removing prefixes and suffixes to give a word a short form, that might not be a dictionary word though lemmatization - finds meaningful word representation from dictionary and depends on the part-of-speech To summarize, the stemming is just searching for a common ground between words and cutting ends then and therefore it takes less time whereas lemmatization provides better results by performing a specific morphological analysis and produces a real word which is extremely important for some human-interactive applications\nSounds like if the resources are not a problem it\u0026rsquo;s better to use lemmatization by default, but there is an opinion that Stemming works efficiently for some specific tasks like: spam classification and feedback sentiment classification, given that it\u0026rsquo;s the case, let\u0026rsquo;s apply both and take a choice in the end\nCode\rimport re import nltk # If the code below doesn\u0026#39;t work - download add-ons first # nltk.download([\u0026#39;stopwords\u0026#39;, \u0026#39;wordnet\u0026#39;]) from nltk.corpus import stopwords stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) # this words might be useful, better to retain for now for word in [\u0026#39;very\u0026#39;, \u0026#39;not\u0026#39;]: stop_words.remove(word) from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() # better version of the Porter Stemmer from nltk.stem.snowball import SnowballStemmer stemmer = SnowballStemmer(language=\u0026#34;english\u0026#34;) for example in [\u0026#39;programming\u0026#39;, \u0026#39;quickly\u0026#39;, \u0026#39;very\u0026#39;]: print(f\u0026#34;Example: {example}\u0026#34;) print(f\u0026#34; - Lemma: {lemmatizer.lemmatize(example, pos=\u0026#39;v\u0026#39;)}\u0026#34;) print(f\u0026#34; - Stem: {stemmer.stem(example)}\u0026#34;) Example: programming\r- Lemma: program\r- Stem: program\rExample: quickly\r- Lemma: quickly\r- Stem: quick\rExample: very\r- Lemma: very\r- Stem: veri\rUnicode transliteration is needed, given that the data contains non-ascii symbols\nCode\rfrom unidecode import unidecode df[df.comments.notnull()][ df[df.comments.notnull()].comments != df[df.comments.notnull()].comments.apply(unidecode) ].comments.sample(5) ticket_id\r43532202076873 Just asking me to show what I‚Äôm doing to give ...\r43532202076331 I haven‚Äôt had a reply\r43532202117219 All good üëç thanks\r43532202073174 Chat went silent. After talking to someone the...\r43532202202546 Hi Val√©ria,\\nIt still does not work. There mus...\rName: comments, dtype: object\rPutting it all together\nCode\rdef clean_data(x, tokenizer, black_list=stop_words): \u0026#34;\u0026#34;\u0026#34; The method removes from a sentence `x` - punctuation \u0026amp; digits - too short words (less than 3 letters) - unicode symbols (translate to ascii) - words from `black_list` Return lowercased and tokenized text \u0026#34;\u0026#34;\u0026#34; words = re.findall(\u0026#39;\\w{3,}\u0026#39;, re.sub(\u0026#39;[^a-z√Ä-√ø ]\u0026#39;, \u0026#39; \u0026#39;, str(x).lower() if x is not np.NaN else \u0026#39;\u0026#39;)) tokens = [tokenize(unidecode(word), tokenizer) for word in words] return \u0026#39; \u0026#39;.join([word for word in tokens if word not in black_list]) def tokenize(x: str, tokenizer) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Applies either stemming or lemmatization to a token `x` \u0026#34;\u0026#34;\u0026#34; if hasattr(tokenizer, \u0026#39;lemmatize\u0026#39;): return tokenizer.lemmatize(x, pos=\u0026#39;v\u0026#39;) elif hasattr(tokenizer, \u0026#39;stem\u0026#39;): return tokenizer.stem(x) else: raise ValueError(\u0026#34;tokenizer should be either Lemmatizer or Stemmer\u0026#34;) df[\u0026#34;lemma_text\u0026#34;] = df.comments.apply(clean_data, args=(lemmatizer,)) df[\u0026#34;stem_text\u0026#34;] = df.comments.apply(clean_data, args=(stemmer,)) Words Frequency There are many different ways how to tackle the visual text analysis, the popular and convenient way is Word Clouds where the size of the word reflects its frequency within the given text\nP.S. In any data mining initiative, it is a good idea to retain some portion of the data to validate your final findings, so let\u0026rsquo;s create a holdout piece of data to adhere true-to-life approach\nCode\rfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( df[[\u0026#34;lemma_text\u0026#34;, \u0026#34;stem_text\u0026#34;]], df[\u0026#39;sentiment\u0026#39;], train_size=1000, shuffle=False ) from collections import Counter def create_ngrams(tokens: list, n: int): ngrams = zip(*[tokens[idx:] for idx in range(n)]) return [\u0026#34; \u0026#34;.join(sorted(ngram)) for ngram in ngrams] def frequent_ngrams(documents: list, n: int = 1): \u0026#34;\u0026#34;\u0026#34; Use .most_common(top_n) to get top n ngrams \u0026#34;\u0026#34;\u0026#34; ngrams = [] if n == 1: ngrams = \u0026#34; \u0026#34;.join(list(documents)).split() elif n \u0026gt;= 2: for tokens in documents: ngrams.extend(create_ngrams(tokens.split(), n)) else: raise ValueError(\u0026#34;n for n-grams should be a positive number\u0026#34;) return Counter(ngrams) import wordcloud def make_word_cloud(text, stop_words=None): plt.figure(figsize=(12, 9)) kwargs = { \u0026#39;width\u0026#39;: 1600, \u0026#39;height\u0026#39;: 900, \u0026#39;min_font_size\u0026#39;: 10 } if isinstance(text, str): word_cloud = wordcloud.WordCloud(stopwords=stop_words, **kwargs).generate_from_text(text) elif isinstance(text, list) or isinstance(text, np.ndarray): word_cloud = wordcloud.WordCloud(stopwords=stop_words, **kwargs).generate(\u0026#34; \u0026#34;.join(text)) else: if stop_words: text = {word: value for word, value in text.items() if word not in stop_words} word_cloud = wordcloud.WordCloud(**kwargs).generate_from_frequencies(text) plt.imshow(word_cloud) plt.axis(\u0026#34;off\u0026#34;) plt.show() First of all, as promised different tokenizers should be compared\nCode\rmake_word_cloud(frequent_ngrams(X_train[\u0026#34;stem_text\u0026#34;], 1)) Code\rmake_word_cloud(frequent_ngrams(X_train[\u0026#34;lemma_text\u0026#34;], 1)) Well, at first glance it looks like both tokenizers work very similarly, one noticeable difference is that stemmer treats word pairs like help and helpful or quick and quickly as one token and actually it might be wrong, imagine if we encounter helpful more in positive sentence and help in negative, then they shouldn\u0026rsquo;t be united\nCode\rfor word in [\u0026#34;help\u0026#34;, \u0026#34;helpful\u0026#34;]: score = df.loc[ df[\u0026#34;comments\u0026#34;].apply( lambda x: f\u0026#34; {word} \u0026#34; in x if x is not np.nan else False ), \u0026#34;sentiment\u0026#34; ].mean() print(f\u0026#34;for `{word}` average sentiment score = {score:.2f}\u0026#34;) for `help` average sentiment score = 0.31\rfor `helpful` average sentiment score = 0.95\rIndeed, that is the case, so let\u0026rsquo;s end up with traditional lemmatizer and take a look at word clouds separately for different sentiments\nCode\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026lt; 0, \u0026#34;lemma_text\u0026#34;], 1)) Code\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026gt; 0, \u0026#34;lemma_text\u0026#34;], 1)) Good news, they are very different in essence, in positive reviews customers use gratitude words more like thank and helpful on the other hand in negative sentences customer highlight that the issue still not resolved. In addition it might be useful to take a look at popular collocations\nCode\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026gt; 0, \u0026#34;lemma_text\u0026#34;], 2)) Code\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026lt; 0, \u0026#34;lemma_text\u0026#34;], 3)) N-grams appear to be very informative:\nin positive sentences, based on bigrams, customers say that the response was quick, problem was solved and the support was very helpful in negative sentences, on the basis of trigrams, customers claim that the issue/problem not resolved sometimes the add yet or still to fully express their dissatisfaction To summarize, the basic approach has already given some meaningful insights and the hope that the reviews might be classified automatically quite well and themes can be modelled then\nUnsupervised TF-IDF It\u0026rsquo;s critical to reduce feature number otherwise X-matrix will be too sparse, and the clusterization will fail to give a substantial result, actually only quite often words should be taken into account\nCode\rfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score from sklearn.cluster import KMeans min_word_counts = range(1, 100, 2) n_features, scores = [], [] for min_word_count in min_word_counts: tfidf = TfidfVectorizer(min_df=min_word_count) X = tfidf.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) n_features.append(X.shape[1]) km = KMeans( n_clusters=2, init=\u0026#39;k-means++\u0026#39;, max_iter=300, random_state=20231020 ) km.fit(X) score = accuracy_score(2 * km.predict(X) - 1, y_train) scores.append( max(score, 1 - score) ) plt.style.use(\u0026#39;ggplot\u0026#39;) fig, ax1 = plt.subplots(figsize=(12, 6)) ax2 = ax1.twinx() ax2.bar(min_word_counts, n_features, color=\u0026#34;b\u0026#34;, width=1.5, alpha=0.33) ax1.plot(min_word_counts, scores, \u0026#34;g--\u0026#34;, linewidth=2) ax1.set_ylabel(\u0026#39;Accuracy, %\u0026#39;, color=\u0026#34;g\u0026#34;) ax1.set_xlabel(\u0026#39;Minimal Word Frequency, #\u0026#39;) ax2.set_ylabel(\u0026#39;N Features, #\u0026#39;, color=\u0026#34;b\u0026#34;) plt.title(\u0026#39;K-Means Clusterization\u0026#39;) plt.show() From the chart is clear that the larger number of features doesn\u0026rsquo;t lead to the Accuracy increase, from the principal of maximum Accuracy the optimal number of features might be 69 for example, let\u0026rsquo;s fix it and track which features the model decided to consider\nCode\rtfidf = TfidfVectorizer(min_df=69) X = tfidf.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) words = np.array(tfidf.get_feature_names()) print(f\u0026#34;Number of features: {X.shape[1]}, namely: \u0026#34;) print(*words) Number of features: 14, namely: answer get help helpful issue not problem quick resolve response solve still thank very\rThe clusterization which is based just on 14 words! gives an accuracy higher than 75%, but it\u0026rsquo;s the result only valid for the train sample, which was used to identify min_df hyperparameter, so to have an unbiased estimation test sample should be considered here\nCode\rkm = KMeans( n_clusters=2, init=\u0026#39;k-means++\u0026#39;, max_iter=300, random_state=20231020 ) km.fit(X) centroids_important_indexes = km.cluster_centers_.argsort()[:,::-1] for idx in range(km.get_params()[\u0026#39;n_clusters\u0026#39;]): print(\u0026#34;Cluster No.\u0026#34;, idx, *words[centroids_important_indexes[idx, :7]]) Cluster No. 0 very issue thank helpful quick resolve response\rCluster No. 1 not issue resolve still solve problem get\rWell, looks like Cluster-0 caught positive feedback and Cluster-1 negative, then given that target is the value from [-1, 1] set, to define an accuracy some transformation must be put in place first\nCode\rpredictions_train = 2 * -km.predict(X) + 1 print(f\u0026#34;Accuracy of K-Means train sample = {accuracy_score(predictions_train, y_train):.1%}\u0026#34;) Accuracy of K-Means train sample = 76.4%\rCode\rpredictions_test = 2 * -km.predict( tfidf.transform( X_test[\u0026#34;lemma_text\u0026#34;] ) ) + 1 print(f\u0026#34;Accuracy of K-Means test sample = {accuracy_score(predictions_test, y_test):.1%}\u0026#34;) Accuracy of K-Means test sample = 74.6%\rSplendid, this toy example gives a clue that it\u0026rsquo;s pretty good approach when you need to classify your customer\u0026rsquo;s feedback while you don\u0026rsquo;t have any ratings (only text comments). Unsupervised approach based on TF-IDF Vectorizer and K-Means gives a nice baseline, but fortunately, it\u0026rsquo;s not our case let\u0026rsquo;s go ahead and use rating set by a customer in addition to texts to reach the text data potential\nSentiment Analysis The goal of this part of the article is to enhance the unsupervised model and build a powerful classifier to eventually understand key drivers for review to be positive or negative from features extraction\nThere are 2 main ways of doing Semantic Analysis:\ntrain your own model using the available data use pre-trained deep learning models and fine-tune them if needed for this particular text specific Both approaches is considered below\nCustom Regression Model Training of the custom model will be held in 3 steps (again?)\nModel architecture selection Selected model training and cross-validation Feature analysis and general evaluation Well, by the model term stands combination of Vectorizer, which transform text data into a vector representation and Classifier that is training on these vectors to predict sentiment\nOut of vectorizers we are going to try both most popular options: Classic Counter and TF-iDF, for classificators let\u0026rsquo;s search among classic linear, tree-based and in addition naive bias method, which might be extremely useful for text classification\nIn addition as it was shown during unsupervised analysis, barely all the words should be taken into account to build a substantial model and it alleviates the learning process also, therefore SVD application for feature space reduction will be considered\nCode\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.svm import LinearSVC from sklearn.decomposition import TruncatedSVD from sklearn.model_selection import cross_val_score from sklearn.pipeline import Pipeline from typing import List, Union, Optional def model(vectorizer, classifier, transformer=None): if transformer: return Pipeline([ (\u0026#34;vectorizer\u0026#34;, vectorizer), (\u0026#34;transformer\u0026#34;, transformer), (\u0026#34;classifier\u0026#34;, classifier) ]) else: return Pipeline([ (\u0026#34;vectorizer\u0026#34;, vectorizer), (\u0026#34;classifier\u0026#34;, classifier) ]) def get_entity_name(entity: Union[List[object], Optional[object]]) -\u0026gt; str: if not isinstance(entity, List): entity = [entity] return [re.sub(\u0026#34;\u0026gt;|\u0026#39;\u0026#34;, \u0026#39;\u0026#39;, str(e)).split(\u0026#34;.\u0026#34;)[-1] for e in entity if e] Using vanilla classifiers let\u0026rsquo;s define which one suits the data better from scratch and reveal hyperparameters on the validation basis after for this candidate\nCode\rnp.random.seed(20231024) for vectorizer in [CountVectorizer, TfidfVectorizer]: for classifier in [LogisticRegression, SGDClassifier, RandomForestClassifier, LinearSVC, MultinomialNB]: transformers = [None] if vectorizer == CountVectorizer: transformers.append(TfidfTransformer()) if classifier != MultinomialNB: transformers.extend([ TruncatedSVD(n_components=100), TruncatedSVD(n_components=10), ]) for transformer in transformers: print(get_entity_name([vectorizer, classifier, transformer]), end=\u0026#34;: \u0026#34;) score = cross_val_score( model(vectorizer(), classifier(), transformer), X_train[\u0026#34;lemma_text\u0026#34;], y_train, cv=5, scoring=\u0026#39;f1\u0026#39; ).mean() print(f\u0026#34;f1-score: {score:.1%}\u0026#34;) ['CountVectorizer', 'LogisticRegression']: f1-score: 90.2%\r['CountVectorizer', 'LogisticRegression', 'TfidfTransformer()']: f1-score: 90.5%\r['CountVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=100)']: f1-score: 88.7%\r['CountVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=10)']: f1-score: 87.5%\r['CountVectorizer', 'SGDClassifier']: f1-score: 88.0%\r['CountVectorizer', 'SGDClassifier', 'TfidfTransformer()']: f1-score: 89.5%\r['CountVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.0%\r['CountVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 84.7%\r['CountVectorizer', 'RandomForestClassifier']: f1-score: 90.2%\r['CountVectorizer', 'RandomForestClassifier', 'TfidfTransformer()']: f1-score: 90.1%\r['CountVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 89.1%\r['CountVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 88.3%\r['CountVectorizer', 'LinearSVC']: f1-score: 89.3%\r['CountVectorizer', 'LinearSVC', 'TfidfTransformer()']: f1-score: 90.3%\r['CountVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=100)']: f1-score: 89.2%\r['CountVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=10)']: f1-score: 87.3%\r['CountVectorizer', 'MultinomialNB']: f1-score: 90.6%\r['CountVectorizer', 'MultinomialNB', 'TfidfTransformer()']: f1-score: 90.2%\r['TfidfVectorizer', 'LogisticRegression']: f1-score: 90.5%\r['TfidfVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=100)']: f1-score: 89.7%\r['TfidfVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=10)']: f1-score: 86.7%\r['TfidfVectorizer', 'SGDClassifier']: f1-score: 89.4%\r['TfidfVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.4%\r['TfidfVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 83.5%\r['TfidfVectorizer', 'RandomForestClassifier']: f1-score: 89.9%\r['TfidfVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.9%\r['TfidfVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 88.0%\r['TfidfVectorizer', 'LinearSVC']: f1-score: 90.3%\r['TfidfVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=100)']: f1-score: 90.7%\r['TfidfVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=10)']: f1-score: 87.4%\r['TfidfVectorizer', 'MultinomialNB']: f1-score: 90.2%\rWinners:\nTfidfVectorizer \u0026amp; LinearSVC \u0026amp; TruncatedSVD CountVectorizer \u0026amp; MultinomialNB TfidfVectorizer \u0026amp; LogisticRegression On the basis of above analysis, the better approach will be to go with the first option because:\nthere is a clear rationale to apply tf-idf over usual counter for the majority of text analysis task regression is more flexible model than Naive Bayes and highly likely that after cross-validation it can accomplish even higher accuracy reducing the feature space makes sense as it was for unsupervised learning Code\rfrom sklearn.model_selection import GridSearchCV clf = model( vectorizer=TfidfVectorizer(), classifier=LinearSVC(random_state=20231020), transformer=TruncatedSVD() ) param_grid = { \u0026#34;vectorizer__max_df\u0026#34;: [1.0, 0.15, 0.10], \u0026#34;vectorizer__min_df\u0026#34;: [1, 2, 3], \u0026#34;vectorizer__ngram_range\u0026#34;: [(1, 1), (1, 2), (1, 3)], \u0026#34;classifier__C\u0026#34;: [0.75, 1, 1.25], \u0026#34;transformer__n_components\u0026#34;: [100, 500, 1000] } search = GridSearchCV(clf, param_grid, cv=3) search.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(\u0026#34;Best parameter (CV score = %0.3f):\u0026#34; % search.best_score_) for key, value in search.best_params_.items(): print(f\u0026#34;{key}: {value}\u0026#34;) Best parameter (CV score = 0.897):\rclassifier__C: 1\rclassifier__loss: hinge\rtransformer__n_components: 1000\rvectorizer__max_df: 1.0\rvectorizer__min_df: 1\rvectorizer__ngram_range: (1, 2)\rCode\r# If you want to see all the results of the scoring use the following code # for param, score in zip( # search.cv_results_[\u0026#39;params\u0026#39;], # search.cv_results_[\u0026#39;mean_test_score\u0026#39;] # ): # print(param, score) The majority of parameters remains as by default, although some of them changed, it\u0026rsquo;s interesting that only bigrams are useful, trigrams on contrary to what we saw before from dummy analysis don\u0026rsquo;t give any additional info to regression from grid-search point of view, from given experience it\u0026rsquo;s better to retain them\nWell, given the parameters are all defined, let\u0026rsquo;s set them up and take closer look at the trained model\nCode\rfrom sklearn.metrics import classification_report clf = model( vectorizer=TfidfVectorizer(ngram_range=(1, 3)), classifier=LinearSVC(random_state=20231020), transformer=TruncatedSVD(n_components=1000), ) clf.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(classification_report(y_test, clf.predict(X_test[\u0026#34;lemma_text\u0026#34;]))) precision recall f1-score support\r-1 0.90 0.86 0.88 111\r1 0.91 0.94 0.92 165\raccuracy 0.91 276\rmacro avg 0.91 0.90 0.90 276\rweighted avg 0.91 0.91 0.91 276\rIf we just out of curiosity take a look at another candidate - Naive Bayes, then the results are\nCode\rclf = model( vectorizer=CountVectorizer(ngram_range=(1, 3)), classifier=MultinomialNB(), ) clf.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(classification_report(y_test, clf.predict(X_test[\u0026#34;lemma_text\u0026#34;]))) precision recall f1-score support\r-1 0.90 0.82 0.86 111\r1 0.89 0.94 0.91 165\raccuracy 0.89 276\rmacro avg 0.89 0.88 0.89 276\rweighted avg 0.89 0.89 0.89 276\rWell, regression works a bit better and it\u0026rsquo;s a pretty powerful classificator, but what is really needed is feature exploration, which words have been determined by the model sentiment and what do customers really appreciate or complain about. In order to evaluate features easily SVD step is skipped here, it doesn\u0026rsquo;t inflict tangible damage on model quality but simplifies analysis a lot\nCode\rimport shap vectorizer=TfidfVectorizer(ngram_range=(1, 3)) classifier=LinearSVC(random_state=20231020) X_train_vec = vectorizer.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) classifier.fit(X_train_vec, y_train) explainer = shap.Explainer( classifier, X_train_vec, feature_names=vectorizer.get_feature_names() ) shap_values = explainer(X_train_vec) shap.plots.beeswarm(shap_values, max_display=30, plot_size=(12, 8)) Well, the results almost don\u0026rsquo;t reveal any new pattens:\nclients like quick and clear answered questions, they are thankful for fast and friendly support and of course solved problem is everything clients dislike: if have no response (reply) and if the problem still not resolved (yet), in general they don\u0026rsquo;t like to wait for getting a solution However, there are some new words here - account, pleo and sonja, and there is an easy way to check how the model works for a particular review\nP.S. if visualization doesn\u0026rsquo;t work run shap.initjs() first\nCode\rfor word in[\u0026#39;account\u0026#39;, \u0026#39;pleo\u0026#39;, \u0026#39;sonja\u0026#39;]: print(word, *X_train[\u0026#34;lemma_text\u0026#34;].apply(lambda x: word in x).values.argsort()[-3:]) account 154 375 104\rpleo 386 532 242\rsonja 826 943 190\rCode\ridx = 154 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Negative\rText: my email account is still not connected and it does not work. tried a couple of times to reconnect\rCode\ridx = 242 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Positive\rText: Fast answer and a perfect answer because the function I was looking for was in Pleo :D\rCode\ridx = 826 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Positive\rText: Quick and thorough answer by support agent Sonja!\rFrom several examples it comes that:\naccount word usually means a general problem with account pleo appears in formal reviews, mostly with some claim sonja seems to be a chat bot agent and it gets positive reviews Pretrained neural network Well, we got some new insights using custom model approach, let\u0026rsquo;s see whether the modern NN architecture will be able to unlock even more meaningful take-away\u0026rsquo;s without additional training\nCode\rfrom transformers import pipeline sentiment_transformer_model = pipeline( task=\u0026#34;sentiment-analysis\u0026#34;, model=\u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34;, return_all_scores=True ) scoring_results = sentiment_transformer_model(X_test[\u0026#34;lemma_text\u0026#34;].to_list()) The drawback of this approach becomes obvious as soon as you start applying it, even inference is taking a lot of time, fingers crossed it\u0026rsquo;s worths it, let\u0026rsquo;s look at the classification quality\nCode\rdef twitter_roberta_predict(scoring_output): scoring_output.sort(key=lambda x: x[\u0026#39;score\u0026#39;], reverse=True) prediction = scoring_output[scoring_output[0][\u0026#39;label\u0026#39;] == \u0026#39;neutral\u0026#39;][\u0026#39;label\u0026#39;] if prediction == \u0026#34;positive\u0026#34;: return 1 elif prediction == \u0026#34;negative\u0026#34;: return -1 else: raise ValueError(\u0026#34;unexpected scoring results\u0026#34;) sentiment_transformer_predictions = [twitter_roberta_predict(scoring) for scoring in scoring_results] print( classification_report(y_test, sentiment_transformer_predictions) ) precision recall f1-score support\r-1 0.19 0.23 0.21 111\r1 0.38 0.32 0.35 165\raccuracy 0.29 276\rmacro avg 0.29 0.28 0.28 276\rweighted avg 0.31 0.29 0.29 276\rIt\u0026rsquo;s kind of expected, given that we use the model which was trained and fine tuned on the texts which have a bit different nature (tweets). To unlock the true power of neural network approach it should be fine tuned to reflect the particular data specific and if you don\u0026rsquo;t have the sufficient amount of data - it should be a red flag not to wasting time with too comprehensive models\nAnyway, let\u0026rsquo;s take a look on how the language model works before pigeonholing it, here we are just some examples, to get a better summary a larger portion of the dataset is needed\nCode\rexplainer = shap.Explainer(sentiment_transformer_model) shap_values = explainer( X_train.loc[X_train[\u0026#34;lemma_text\u0026#34;].apply(lambda x: len(x.split()) \u0026gt; 5), \u0026#34;lemma_text\u0026#34;].sample(3), silent=True ) shap.plots.text(shap_values) ","permalink":"https://npodlozhniy.github.io/posts/feedback-analysis/","summary":"Background Every company has always taken pride in providing excellent customer service, therefore it\u0026rsquo;s crucial as part of ongoing improvements for product to collect and analyse feedback after each customer interaction on support channels\nA top-notch analyst in Hi-Tech industry should be in capable of handle at least base feedback analysis quickly and efficiently and going through this article you will be introduced with the necessary concepts and eventually given the base guide on how to unlock the true power of text data","title":"Customer Feedback Analysis in Python"},{"content":"\rIntro It\u0026rsquo;s a short yet handy guide on how to analyze time series data in a few clicks using modern Python libraries.\nTech stack for this guide consists of\nduckdb and pandas packages for data processing Facebook\u0026rsquo;s prophet for data exploration and forecasting plotly for data visualization Code\rimport duckdb as db import pandas as pd import numpy as np orders = pd.read_csv(\u0026#39;orders_data.csv\u0026#39;) orders.sample() Created Date Country City Restaurant ID Restaurant Name Order State Cancel Reason Cuisine Platform Products in Order Order Value ‚Ç¨ (Gross) Delivery Fee Delivery Time Order ID 44255 10.01.2020 Portugal Lisbon 6167 H3 Armaz√©ns do Chiado delivered NaN Mediterranean ios 1 ‚Ç¨9.55 0.0 14.7 379143844 Are there any discernible patterns or seasonality present? Usually it\u0026rsquo;s good to start any time series analysis with tackling four key questions:\nIs there any trend? Is there seasonality, if so which type: additive or multiplicative? Is there any explicit change points, which we should consider? Are there outliers? - Get rid of them! Data If you never experienced duckdb before, I totally recommend you to try as it boosts your efficiency allowing simultaneous usage of best features from Python and SQL\nCode\rdf = db.query(\u0026#34;\u0026#34;\u0026#34; SELECT strptime(\u0026#34;Created Date\u0026#34;, \u0026#39;%d.%m.%Y\u0026#39;) as date ,\u0026#34;City\u0026#34; as city ,count(*) as orders FROM orders o WHERE \u0026#34;Order State\u0026#34; = \u0026#39;delivered\u0026#39; GROUP BY 1, 2 \u0026#34;\u0026#34;\u0026#34;).to_df() pivot = df.pivot(index=\u0026#39;date\u0026#39;, columns=\u0026#39;city\u0026#39;, values=\u0026#39;orders\u0026#39;) pivot.head() city Accra Lisbon date 2020-01-01 124 882 2020-01-02 126 1104 2020-01-03 76 1110 2020-01-04 169 784 2020-01-05 198 788 Code\rprint(f\u0026#34;We have data for {pivot.shape[0]} consecutive days\u0026#34;) We have data for 59 consecutive days\rLibraries I\u0026rsquo;m using my own time series model wrappers to explore and forecast the data, they are based upon popular libraries from big tech companies, in this particular case, on Facebook Prophet\nCode\rimport datetime import re from typing import Optional, List, Tuple from plotly import express, graph_objects from plotly.subplots import make_subplots from prophet import Prophet as FP from prophet.utilities import regressor_coefficients from sklearn.preprocessing import MinMaxScaler Code\rclass ModelWrapper: \u0026#34;\u0026#34;\u0026#34; Custom Wrapper for popular models for TimeSeries analysis, including but not limited to: - Google Causal Impact https://pypi.org/project/causalimpact/ - Facebook Prophet https://facebook.github.io/prophet/ Parameters ---------- df: DataFrame containg target variable with `y` name and time frame variable with `date` name start_date: the start date of experiment with the format YYYY-MM-DD test_days: the number of last days in which the experiment was run end_date: the end date of experiment with the format YYYY-MM-DD date: timeframe column name y: target column name Attributes ---------- explore: the chart exploring pre-experiment correlations show: visualization of the trained model, please notet that you need to run the model first \u0026#34;\u0026#34;\u0026#34; def __init__( self, df: pd.DataFrame, start_date: Optional[str] = None, test_days: Optional[int] = None, end_date: Optional[str] = None, date: str = \u0026#34;date\u0026#34;, y: str = \u0026#34;y\u0026#34; ) -\u0026gt; None: self.df = df.copy() self.date = date self.y = y if start_date: self.start_dt = datetime.datetime.strptime(start_date, \u0026#34;%Y-%m-%d\u0026#34;).date() elif test_days: if pd.core.dtypes.common.is_datetime64_dtype(self.df[self.date].dtype): self.start_dt = self.df[self.date].max().date() - datetime.timedelta(days=test_days) else: self.start_dt = datetime.datetime.strptime(self.df[self.date].max(), \u0026#34;%Y-%m-%d\u0026#34;).date() - datetime.timedelta(days=test_days) else: raise ValueError(\u0026#34;You must specify start_date or test_days variable\u0026#34;) self.end_dt = datetime.datetime.strptime(end_date, \u0026#34;%Y-%m-%d\u0026#34;).date() if end_date else None self.x = list(self.df.columns.difference([self.date, self.y]).values) self._preprocess() def _preprocess(self): self.df[self.date] = pd.to_datetime(self.df[self.date]).dt.date self.df = self.df[[self.y, self.date] + self.x].set_index(self.date).sort_index() for column in self.df.columns[self.df.dtypes == \u0026#34;object\u0026#34;]: try: self.df[column] = self.df[column].astype(float) except ValueError: raise ValueError(\u0026#34;All DataFrame columns except Date must be numeric\u0026#34;) @staticmethod def _save(figure, title: str) -\u0026gt; None: figure.write_html( re.sub(\u0026#39;[-:\u0026lt;\u0026gt;|\\/\\*\\?\\\u0026#34;\\\\\\\\ ]+\u0026#39;, \u0026#39;_\u0026#39;, title.lower()) + \u0026#34;.html\u0026#34; ) def explore( self, scale: bool = True, title: str = \u0026#34;pre-experiment correlation\u0026#34;, x_label: Optional[str] = None, y_label: Optional[str] = None, width: int = 900, height: int = 600, save: bool = False, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Plot the dynamic of pre-experiment correlation Parameters ---------- scale: whether the samples should be scaled to [0, 1] interval title: the title of the chart x_label: label for X axis y_label: label for Y axis width: the width of the chart height: the height of the chart save: whether you want to save the chart as HTML \u0026#34;\u0026#34;\u0026#34; data = self.df.copy() if scale: scaler = MinMaxScaler(feature_range=(0, 1)) data[data.columns] = scaler.fit_transform(data) corr = data[data.index \u0026lt; self.start_dt].corr().iloc[0, 1:] data = pd.melt( data, value_vars=list(data.columns), var_name=\u0026#34;variable\u0026#34;, value_name=\u0026#34;value\u0026#34;, ignore_index=False, ) chart_title = title + f\u0026#34;\u0026#34;\u0026#34;\u0026lt;br\u0026gt;\u0026lt;sup\u0026gt;{\u0026#39;, \u0026#39;.join( [f\u0026#34;{feature}: {round(value, 2)}\u0026#34; for feature, value in zip(self.x, corr)] )}\u0026lt;/sup\u0026gt;\u0026lt;/br\u0026gt;\u0026#34;\u0026#34;\u0026#34; figure = express.line( data, x=data.index, y=\u0026#34;value\u0026#34;, color=\u0026#34;variable\u0026#34;, height=height, width=width, title=chart_title, ) figure.add_vline(x=self.start_dt, line_width=2, line_dash=\u0026#34;dash\u0026#34;, line_color=\u0026#34;white\u0026#34;) if self.end_dt: figure.add_vline(x=self.end_dt, line_width=2, line_dash=\u0026#34;dash\u0026#34;, line_color=\u0026#34;white\u0026#34;) figure.update_traces(hovertemplate=\u0026#34;%{y}\u0026#34;) figure.update_xaxes(title_text=x_label if x_label else \u0026#34;Date\u0026#34;) figure.update_yaxes(title_text=y_label if y_label else \u0026#34;Scaled Axis\u0026#34; if scale else \u0026#34;Original Axis\u0026#34;) figure.update_layout( title={ \u0026#34;x\u0026#34;: 0.5, }, legend={ \u0026#34;x\u0026#34;: 0.05, \u0026#34;y\u0026#34;: 1.05, \u0026#34;orientation\u0026#34;: \u0026#34;h\u0026#34;, \u0026#34;title\u0026#34;: None }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, xaxis=dict(hoverformat=\u0026#34;%a, %b %d, %Y\u0026#34;), ) if save: self._save(figure, title) figure.show() @staticmethod def _add_chart( figure, data: pd.DataFrame, titles: list, y: str, name: str, row: int, actual: bool = False, y_label: Optional[str] = None, ) -\u0026gt; None: figure.add_trace( graph_objects.Scatter( x=data.index, y=data[y], name=name, hovertemplate=\u0026#34;%{y}\u0026#34;, line={\u0026#34;color\u0026#34;: \u0026#34;white\u0026#34;}, legendgroup=f\u0026#34;{row}\u0026#34;, legendgrouptitle={\u0026#34;text\u0026#34;: titles[row]}, connectgaps=True, ), row=row, col=1, ) figure.add_trace( graph_objects.Scatter( x=data.index, y=data[y + \u0026#34;_upper\u0026#34;], name=\u0026#34;Upper bound\u0026#34;, hovertemplate=\u0026#34;%{y}\u0026#34;, line={\u0026#34;color\u0026#34;: \u0026#34;deepskyblue\u0026#34;, \u0026#34;width\u0026#34;: 0.5}, legendgroup=f\u0026#34;{row}\u0026#34;, connectgaps=True, ), row=row, col=1, ) figure.add_trace( graph_objects.Scatter( x=data.index, y=data[y + \u0026#34;_lower\u0026#34;], name=\u0026#34;Lower bound\u0026#34;, hovertemplate=\u0026#34;%{y}\u0026#34;, fill=\u0026#34;tonexty\u0026#34;, line={\u0026#34;color\u0026#34;: \u0026#34;deepskyblue\u0026#34;, \u0026#34;width\u0026#34;: 0.5}, legendgroup=f\u0026#34;{row}\u0026#34;, connectgaps=True, ), row=row, col=1, ) figure.update_yaxes(title_text=\u0026#34;\u0026#34; if not y_label else \u0026#34;% Effect\u0026#34; if row == 4 else y_label) if actual: figure.add_trace( graph_objects.Scatter( x=data.index, y=data[\u0026#34;response\u0026#34;], name=\u0026#34;Actual\u0026#34;, hovertemplate=\u0026#34;%{y}\u0026#34;, line={\u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;}, legendgroup=f\u0026#34;{row}\u0026#34;, connectgaps=True, ), row=row, col=1, ) else: figure.add_hline(y=0, line_width=1, line_color=\u0026#34;white\u0026#34;, row=row, col=1) def show( self, keep_n_prior_days: Optional[int] = None, title: str = \u0026#34;Causal Impact\u0026#34;, x_label: Optional[str] = None, y_label: Optional[str] = None, width: int = 900, height: int = 600, save: bool = False, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Plot the trained model results Parameters ---------- keep_n_prior_days: specify the exact number of pre-experiment days you want to keep, skip if you want to show all the time frame title: the title of the chart x_label: label for X axis y_label: label for Y axis width: the width of the chart height: the height of the chart save: whether you want to save the chart as HTML \u0026#34;\u0026#34;\u0026#34; try: if keep_n_prior_days: data = self.result[ self.result.index \u0026gt; self.start_dt - datetime.timedelta(days=keep_n_prior_days) ] else: data = self.result.iloc[1:] except AttributeError: raise AttributeError(\u0026#34;To show the results run the model first, use .run() method\u0026#34;) titles = [ \u0026#34;Model Overview\u0026#34;, \u0026#34;Actual vs Expected\u0026#34;, \u0026#34;Effect Size: Actual - Expected\u0026#34;, \u0026#34;Cumulative Effect\u0026#34;, \u0026#34;Effect Relative to Expected\u0026#34;, ] if isinstance(self, CausalImpact): figure = make_subplots( rows=4, cols=1, shared_xaxes=True, subplot_titles=titles[1:] ) for y, name, row in zip( [\u0026#34;point_pred\u0026#34;, \u0026#34;point_effect\u0026#34;, \u0026#34;cum_effect\u0026#34;, \u0026#34;rel_effect\u0026#34;], [\u0026#34;Expected Values\u0026#34;, \u0026#34;Effect Size\u0026#34;, \u0026#34;Cumulative Effect\u0026#34;, \u0026#34;Relative Effect\u0026#34;], range(1, 5) ): self._add_chart( figure, data, titles, y=y, name=name, row=row, actual=(row == 1), y_label=y_label ) elif isinstance(self, Prophet): row = 1 figure = make_subplots() self._add_chart( figure, data, titles, y=\u0026#34;yhat\u0026#34;, name=\u0026#34;Expected Value\u0026#34;, row=row, actual=True, y_label=y_label ) figure.update_xaxes(title_text=x_label if x_label else \u0026#34;Date\u0026#34;, row=row, col=1) figure.add_vline(x=self.start_dt, line_width=2, line_dash=\u0026#34;dash\u0026#34;, line_color=\u0026#34;white\u0026#34;) if self.end_dt: figure.add_vline(x=self.end_dt, line_width=2, line_dash=\u0026#34;dash\u0026#34;, line_color=\u0026#34;white\u0026#34;) figure.update_layout( title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: title, }, width=width, height=height, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, legend={ \u0026#34;x\u0026#34;: 0.0, \u0026#34;y\u0026#34;: -0.2, \u0026#34;orientation\u0026#34;: \u0026#34;h\u0026#34;, \u0026#34;groupclick\u0026#34;: \u0026#34;toggleitem\u0026#34;, \u0026#34;traceorder\u0026#34;: \u0026#34;grouped\u0026#34;, }, xaxis=dict(hoverformat=\u0026#34;%a, %b %d, %Y\u0026#34;), ) if save: self._save(figure, title) figure.show() Code\rclass CausalImpact(ModelWrapper): def run( self, nseasons: int = 7, season_duration: int = 1, alpha: float = 0.05, **kwargs ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Run causal impact analysis Parameters ---------- nseasons: Period of the seasonal components. In order to include a seasonal component, set this to a whole number greater than 1. For example, if the data represent daily observations, use 7 for a day-of-week component. This interface currently only supports up to one seasonal component. season_duration: Duration of each season, i.e., number of data points each season spans. For example, to add a day-of-week component to data with daily granularity, use model_args = list(nseasons = 7, season_duration = 1). To add a day-of-week component to data with hourly granularity, set model_args = list(nseasons = 7, season_duration = 24). alpha : Desired tail-area probability for posterior intervals. Defaults to 0.05, which will produce central 95% intervals Other Parameters ---------------- **kwargs : model_args variables, available options: ndraws: number of MCMC samples to draw. More samples lead to more accurate inferences. Defaults to 1000. nburn: number of burn in samples. This specifies how many of the initial samples will be discarded. defaults to 10% of ndraws. standardize_data: whether to standardize all columns of the data before fitting the model. This is equivalent to an empirical Bayes approach to setting the priors. It ensures that results are invariant to linear transformations of the data. prior_level_sd: prior standard deviation of the Gaussian random walk of the local level. Expressed in terms of data standard deviations. Defaults to 0.01. A typical choice for well-behaved and stable datasets with low residual volatility after regressing out known predictors. When in doubt, a safer option is to use 0.1, as validated on synthetic data, although this may sometimes give rise to unrealistically wide prediction intervals. dynamic_regression: whether to include time-varying regression coefficients. In combination with a time-varying local trend or even a time-varying local level, this often leads to overspecification, in which case a static regression is safer. Defaults to FALSE. \u0026#34;\u0026#34;\u0026#34; data = self.df.copy().reset_index() prior = [ data.index.min(), int(data[data[self.date] \u0026lt; self.start_dt].index.max()) ] posterior = [ int(data[data[self.date] \u0026gt;= (self.end_dt if self.end_dt else self.start_dt)].index.min()), data.index.max() ] data.drop(columns=[self.date], inplace=True) self.ci = CI( data, prior, posterior, model_args={ \u0026#34;nseasons\u0026#34;: nseasons, \u0026#34;season_duration\u0026#34;: season_duration, **kwargs }, alpha=alpha, ) self.ci.run() def summary(self, format: str = \u0026#34;summary\u0026#34;) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Print the summary for Causal Impact Analysis model Parameters ---------- format: can be \u0026#39;summary\u0026#39; to return a table or \u0026#39;report\u0026#39; to return a natural language description \u0026#34;\u0026#34;\u0026#34; try: self.ci.summary(format) except AttributeError: raise AttributeError(\u0026#34;To get the summary run the model first, use .run() method\u0026#34;) self.result = self.ci.inferences.set_index(self.df.index) for suffix in [\u0026#34;\u0026#34;, \u0026#34;_lower\u0026#34;, \u0026#34;_upper\u0026#34;]: self.result[\u0026#34;rel_effect\u0026#34; + suffix] = self.result[\u0026#34;point_effect\u0026#34; + suffix] / self.result[\u0026#34;point_pred\u0026#34;] Code\rclass Prophet(ModelWrapper): def run( self, growth: str = \u0026#34;linear\u0026#34;, weekly_seasonality: bool = True, monthly_seasonality: bool = True, yearly_seasonality: bool = True, seasonality_mode: str = \u0026#34;additive\u0026#34;, country_holidays: Optional[str] = None, outliers: Optional[List[Tuple[str]]] = None, floor: Optional[int] = None, cap: Optional[int] = None, alpha: float = 0.05, **kwargs, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Run time-series forecasting Parameters ---------- growth: String \u0026#39;linear\u0026#39;, \u0026#39;logistic\u0026#39; or \u0026#39;flat\u0026#39; to specify a linear, logistic or flat trend. weekly_seasonality: Fit weekly seasonality. monthly_seasonality: Fit monthly seasonality. yearly_seasonality: Fit yearly seasonality. seasonality_mode: \u0026#39;additive\u0026#39; (default) or \u0026#39;multiplicative\u0026#39;. country_holidays: country code (e.g. \u0026#39;RU\u0026#39;) of the country whose holiday are to be considered outliers: list of time intervals (start date, end date) with the format YYYY-MM-DD where there are outliers floor: minimum allowed value for the target variable. It\u0026#39;s particulary useful with \u0026#34;logistic\u0026#34; growth type cap: maximum allowed value for the target variable. It\u0026#39;s particulary useful with \u0026#34;logistic\u0026#34; growth type alpha: 1 - width of the uncertainty intervals provided for the forecast. Other Parameters ---------------- **kwargs : model_args variables, to reveal the whole list follow the Prophet documentation, for example: n_changepoints: Number of potential changepoints to include. Not used if input `changepoints` is supplied. If `changepoints` is not supplied, then n_changepoints potential changepoints are selected uniformly from the first `changepoint_range` proportion of the history. changepoint_range: Proportion of history in which trend changepoints will be estimated. Defaults to 0.8 for the first 80%. Not used if `changepoints` is specified. changepoint_prior_scale: Parameter modulating the flexibility of the automatic changepoint selection. Large values will allow many changepoints, small values will allow few changepoints. \u0026#34;\u0026#34;\u0026#34; data = self.df.copy().reset_index().rename(columns={self.date: \u0026#34;ds\u0026#34;, self.y: \u0026#34;y\u0026#34;}).sort_values(\u0026#34;ds\u0026#34;) if cap: data[\u0026#34;cap\u0026#34;] = cap if floor: data[\u0026#34;floor\u0026#34;] = floor train, test = data[data[\u0026#34;ds\u0026#34;] \u0026lt; self.start_dt], data[data[\u0026#34;ds\u0026#34;] \u0026gt;= self.start_dt] self.model = FP( growth=growth, weekly_seasonality=weekly_seasonality, yearly_seasonality=yearly_seasonality, seasonality_mode=seasonality_mode, interval_width=1-alpha, **kwargs, ) if monthly_seasonality: self.model.add_seasonality(name=\u0026#34;monthly\u0026#34;, period=30.5, fourier_order=5) if country_holidays: self.model.add_country_holidays(country_name=country_holidays) if outliers: for pair in outliers: train.loc[ (train[\u0026#34;ds\u0026#34;] \u0026gt; datetime.datetime.strptime(pair[0], \u0026#34;%Y-%m-%d\u0026#34;).date()) \u0026amp; (train[\u0026#34;ds\u0026#34;] \u0026lt; datetime.datetime.strptime(pair[1], \u0026#34;%Y-%m-%d\u0026#34;).date()), \u0026#34;y\u0026#34;] = None for feature in self.x: self.model.add_regressor(feature) self.model.fit(train) future = self.model.make_future_dataframe(periods=test.shape[0]) self.result = self.model.predict(future.set_index(\u0026#34;ds\u0026#34;).join(data.set_index(\u0026#34;ds\u0026#34;)).reset_index()) self.result[\u0026#34;ds\u0026#34;] = self.result[\u0026#34;ds\u0026#34;].dt.date self.result = self.result.set_index(\u0026#34;ds\u0026#34;).join(data[[\u0026#34;ds\u0026#34;, \u0026#34;y\u0026#34;]].set_index(\u0026#34;ds\u0026#34;)).rename(columns={\u0026#34;y\u0026#34;: \u0026#34;response\u0026#34;}) def summary( self, width: int = 900, height: int = 600, save: bool = False, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Plot the regressors statistics: Coefficients, Impact and Impact Share The estimated beta coefficient for each regressor roughly represents the increase in prediction value for a unit increase in the regressor value. Note that the coefficients returned are always on the scale of the original data In addition the credible interval for each coefficient is also returned, which can help identify whether each regressor is ‚Äústatistically significant‚Äù. On the basis of `seasonality_mode` the model looks like: Additive: y(t) ~ trend(t) + seasonality(t) + beta * regressor(t) Multiplicative: y(t) ~ trend(t) * ( 1 + seasonality(t) + beta * regressor(t) ) Therefore, the incremental impact are: Additive: increasing the value of the regressor by a unit leads to an increase in y(t) by beta units Multiplicative: increasing the value of the regressor by a unit leads to increase in y(t) by beta * trend(t) units The Impact is the product of incremental impact(t) * regressor(t) and finally, Share is the percentage of absolute Impact Parameters ---------- width: the width of the chart height: the height of the chart save: whether you want to save the chart as HTML \u0026#34;\u0026#34;\u0026#34; try: data = regressor_coefficients(self.model) except AttributeError: raise AttributeError(\u0026#34;To get the summary run the model first, use .run() method\u0026#34;) last_day_data, last_day_result = self.df.iloc[-1, :], self.result.iloc[-1, :] data[\u0026#34;incremental_impact\u0026#34;] = data[\u0026#34;coef\u0026#34;] * data[\u0026#34;regressor_mode\u0026#34;].apply(lambda x: last_day_result[\u0026#34;trend\u0026#34;] if x == \u0026#34;multiplicative\u0026#34; else 1) data[\u0026#34;impact\u0026#34;] = data.apply(lambda x: x[\u0026#34;incremental_impact\u0026#34;] * last_day_data[x[\u0026#34;regressor\u0026#34;]], axis=1) data[\u0026#34;share\u0026#34;] = round(100 * np.abs(data[\u0026#34;impact\u0026#34;]) / np.sum(np.abs(data[\u0026#34;impact\u0026#34;])), 2) def plot_bar(data, y, title): figure = express.bar( data, x=\u0026#34;regressor\u0026#34;, y=y, color=\u0026#34;regressor\u0026#34;, color_discrete_sequence=express.colors.sequential.Jet, ) figure.add_hline(y=0, line_width=1, line_color=\u0026#34;white\u0026#34;) figure.update_xaxes(title_text=None) figure.update_layout( title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: title, }, width=width, height=height, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, showlegend=False, ) if save: self._save(figure, title) figure.show() for y, title in zip([\u0026#34;coef\u0026#34;, \u0026#34;impact\u0026#34;], [\u0026#34;Coefficients\u0026#34;, \u0026#34;Impact\u0026#34;]): plot_bar(data.sort_values(by=\u0026#34;coef\u0026#34;, ascending=False), y, title) pie = express.pie( data.sort_values(by=\u0026#34;coef\u0026#34;, ascending=False), color_discrete_sequence=express.colors.sequential.Jet, values=\u0026#34;share\u0026#34;, names=\u0026#34;regressor\u0026#34;, color=\u0026#34;regressor\u0026#34;, ) pie.update_layout( title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: \u0026#34;Impact Share\u0026#34;, }, width=width, height=height, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, showlegend=False, ) pie.update_traces( marker=dict( line=dict(color=\u0026#34;black\u0026#34;, width=3) ) ) if save: self._save(pie, \u0026#34;Impact Share\u0026#34;) pie.show() Model Sure, you could spend more time trying different models like moving averages and seasonal arima to evaluate and choose better one eventually, but here it\u0026rsquo;s a short way to get pretty good base model, which is fast and robust enough.\nCode\rmodel = Prophet(pivot.reset_index(), y=\u0026#34;Lisbon\u0026#34;, date=\u0026#34;date\u0026#34;, test_days=7) model.explore(scale=True, title=\u0026#34;Correlation: Lisbon vs Accra\u0026#34;, save=True) The default implemented strategy saves plotly figures as html code to be viewable aside of the iPython environment. However if one is interested in more uniform way of storage, here is the function to save html as json without building Plotly figure from scratch.\nCode\rimport plotly import json import re def plotly_html_to_json(name: str) -\u0026gt; None: with open(f\u0026#34;{name}.html\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: html = file.read() call_arg_str = re.findall(r\u0026#34;Plotly\\.newPlot\\((.*)\\)\u0026#34;, html[-2**16:])[0] call_args = json.loads(f\u0026#39;[{call_arg_str}]\u0026#39;) plotly_json = {\u0026#39;data\u0026#39;: call_args[1], \u0026#39;layout\u0026#39;: call_args[2]} figure = plotly.io.from_json(json.dumps(plotly_json)) figure.write_json(f\u0026#34;{name}.json\u0026#34;) Key findings:\nThere is a steadily growing trend for both cities There is different seasonality for cities: additive weekly seasonality for Lisbon (Thursday, Friday - max, Sunday - min) for Accra it\u0026rsquo;s no so explicit, and the pattern is different (the maximum is often reached on weekends, and Friday is min) There are different change points for cities: For Lisbon since the beginning of February the increasing trend was replaced by flatten one For Accra the trend is more subtle, but given the difference in absolute number the confidence is such a trend much lower There are outliers: For Lisbon Feb 26 is enormously higher, the reason could be the national holiday - carnival on Feb 25 In addition Feb 14 - not a public holiday but a nice time to have a romantic dinner at home by candlelight For Accra Feb 7 is enormously lower, could be the object for detailed analysis (a bit covered below) Moreover for Accra Feb 26 is the maximum for Ghana as well, may be some promo took place for both cities And the last point of data should be the minimum for Accra, but anyway looks too low, might be the dataset doesn\u0026rsquo;t include all the orders until the end of the day Outliers Investigation To interpret outliers one of the possible options is to explore Delivery Success rate\nCode\rdb.query(\u0026#34;\u0026#34;\u0026#34; WITH temp AS ( SELECT \u0026#34;Created Date\u0026#34; ,\u0026#34;City\u0026#34; ,\u0026#34;Order State\u0026#34; ,COUNT(\u0026#34;Order ID\u0026#34;) as \u0026#34;Orders Count\u0026#34; FROM orders o WHERE \u0026#34;City\u0026#34; in (\u0026#39;Accra\u0026#39;, \u0026#39;Lisbon\u0026#39;) AND \u0026#34;Created Date\u0026#34; IN ( \u0026#39;06.02.2020\u0026#39;, \u0026#39;07.02.2020\u0026#39;, \u0026#39;08.02.2020\u0026#39;, \u0026#39;26.02.2020\u0026#39;, \u0026#39;27.02.2020\u0026#39;, \u0026#39;28.02.2020\u0026#39; ) GROUP BY ALL ), totals AS ( SELECT \u0026#34;Created Date\u0026#34; ,\u0026#34;City\u0026#34; ,SUM(\u0026#34;Orders Count\u0026#34;) AS \u0026#34;Orders Total\u0026#34; FROM temp GROUP BY 1, 2 ) SELECT temp.*, 100 * ROUND(\u0026#34;Orders Count\u0026#34;/\u0026#34;Orders Total\u0026#34;, 4) AS \u0026#34;Share\u0026#34; FROM temp JOIN totals USING (\u0026#34;Created Date\u0026#34;, \u0026#34;City\u0026#34;) ORDER BY ALL \u0026#34;\u0026#34;\u0026#34;).to_df() Created Date City Order State Orders Count Share 0 06.02.2020 Accra delivered 229 91.60 1 06.02.2020 Accra failed 11 4.40 2 06.02.2020 Accra rejected 10 4.00 3 06.02.2020 Lisbon delivered 1855 98.78 4 06.02.2020 Lisbon failed 1 0.05 5 06.02.2020 Lisbon rejected 22 1.17 6 07.02.2020 Accra delivered 161 86.10 7 07.02.2020 Accra failed 23 12.30 8 07.02.2020 Accra rejected 3 1.60 9 07.02.2020 Lisbon delivered 1725 99.19 10 07.02.2020 Lisbon failed 4 0.23 11 07.02.2020 Lisbon rejected 10 0.58 12 08.02.2020 Accra delivered 264 95.65 13 08.02.2020 Accra failed 4 1.45 14 08.02.2020 Accra rejected 8 2.90 15 08.02.2020 Lisbon delivered 1246 99.12 16 08.02.2020 Lisbon failed 6 0.48 17 08.02.2020 Lisbon rejected 5 0.40 18 26.02.2020 Accra delivered 355 96.47 19 26.02.2020 Accra failed 6 1.63 20 26.02.2020 Accra rejected 7 1.90 21 26.02.2020 Lisbon delivered 2307 98.84 22 26.02.2020 Lisbon failed 8 0.34 23 26.02.2020 Lisbon rejected 19 0.81 24 27.02.2020 Accra delivered 300 92.31 25 27.02.2020 Accra failed 13 4.00 26 27.02.2020 Accra rejected 12 3.69 27 27.02.2020 Lisbon delivered 2201 98.79 28 27.02.2020 Lisbon failed 6 0.27 29 27.02.2020 Lisbon rejected 21 0.94 30 28.02.2020 Accra delivered 206 85.83 31 28.02.2020 Accra failed 17 7.08 32 28.02.2020 Accra rejected 17 7.08 33 28.02.2020 Lisbon delivered 2030 98.88 34 28.02.2020 Lisbon failed 7 0.34 35 28.02.2020 Lisbon rejected 16 0.78 If you\u0026rsquo;re a fan of pandas of course it may be easier to apply similar operations within familiar package, and it even may seem as more Pythonic way to do so, although believe me, once you start using duckdb, it turns out that some operations are just more native to SQL like complex joins and window functions and you naturally perform them faster in SQL; work smarter, not harder - if you\u0026rsquo;re inventing a bicycle writing SQL alike transformations in Pandas, nobody beyond your internal perfectionist will reward it.\nPandas alternative Code\rtemp = orders[ ( (orders[\u0026#34;City\u0026#34;] == \u0026#34;Accra\u0026#34;) | (orders[\u0026#34;City\u0026#34;] == \u0026#34;Lisbon\u0026#34;) ) \u0026amp; ( (orders[\u0026#34;Created Date\u0026#34;] == \u0026#39;06.02.2020\u0026#39;) | (orders[\u0026#34;Created Date\u0026#34;] == \u0026#39;07.02.2020\u0026#39;) | (orders[\u0026#34;Created Date\u0026#34;] == \u0026#39;08.02.2020\u0026#39;) | (orders[\u0026#34;Created Date\u0026#34;] == \u0026#39;26.02.2020\u0026#39;) | (orders[\u0026#34;Created Date\u0026#34;] == \u0026#39;27.02.2020\u0026#39;) | (orders[\u0026#34;Created Date\u0026#34;] == \u0026#39;28.02.2020\u0026#39;) ) ].groupby([\u0026#34;Created Date\u0026#34;, \u0026#34;City\u0026#34;, \u0026#34;Order State\u0026#34;]).agg({\u0026#34;Order ID\u0026#34;: \u0026#34;count\u0026#34;}) temp.join( temp.groupby([\u0026#34;Created Date\u0026#34;, \u0026#34;City\u0026#34;]).sum(), on=[\u0026#34;Created Date\u0026#34;, \u0026#34;City\u0026#34;], rsuffix=\u0026#34; Total\u0026#34; ).apply( lambda x: 100 * round(x[\u0026#34;Order ID\u0026#34;] / x[\u0026#34;Order ID Total\u0026#34;], 4), axis=1 ) Insight: Quick analysis for Feb 7 and for Feb 28 shows that the percentage of delivered orders in Accra is lower than usual while for Lisbon there is no such a drop, that is one of the potential reason of outliers\nMake a forecast of the number of orders expected in the following 4 weeks The correlation between two cities is not that high (Pearson coef is less than 0.5) to leverage it, providing the other country as a predictor\nPortugal (Lisbon) Fit Code\rmodel = Prophet(pivot[\u0026#34;Lisbon\u0026#34;].reset_index(), y=\u0026#34;Lisbon\u0026#34;, date=\u0026#34;date\u0026#34;, test_days=7) Incorporate all the logic described within the previous point:\nlinear trend weekly additive seasonality Carnival, Valentine Day as outliers Last week as a validation time frame\nCode\rmodel.run( growth=\u0026#39;linear\u0026#39;, yearly_seasonality=False, monthly_seasonality=False, seasonality_mode=\u0026#39;additive\u0026#39;, country_holidays=\u0026#39;PT\u0026#39;, outliers=[(\u0026#34;2020-02-13\u0026#34;, \u0026#34;2020-02-15\u0026#34;)] ) model.show( title=\u0026#34;Lisbon\u0026#34;, y_label=\u0026#34;Orders, #\u0026#34;, save=True, ) Summary: model fits well, the last week expectedly exceeds the upper bounds due to the holiday\nPredict Code\rlibon = model.model.make_future_dataframe(periods=36).join(pivot[\u0026#34;Lisbon\u0026#34;], on=\u0026#34;ds\u0026#34;) model = Prophet(libon, y=\u0026#34;Lisbon\u0026#34;, date=\u0026#34;ds\u0026#34;, test_days=28) model.run( growth=\u0026#39;linear\u0026#39;, yearly_seasonality=False, monthly_seasonality=False, seasonality_mode=\u0026#39;additive\u0026#39;, country_holidays=\u0026#39;PT\u0026#39;, outliers=[(\u0026#34;2020-02-13\u0026#34;, \u0026#34;2020-02-15\u0026#34;)] ) model.show( title=\u0026#34;Lisbon Prognosis\u0026#34;, y_label=\u0026#34;Orders, #\u0026#34;, save=True, ) Ghana (Accra) The same algorithm for Accra\nFit Code\rmodel = Prophet(pivot[\u0026#34;Accra\u0026#34;].reset_index(), y=\u0026#34;Accra\u0026#34;, date=\u0026#34;date\u0026#34;, test_days=7) Incorporate all the logic described within the previous point:\nlinear trend weekly additive seasonality Feb 7 \u0026amp; Feb 28 as outliers Last week as a validation time frame\nCode\rmodel.run( growth=\u0026#39;linear\u0026#39;, yearly_seasonality=False, monthly_seasonality=False, seasonality_mode=\u0026#39;additive\u0026#39;, outliers=[(\u0026#34;2020-02-06\u0026#34;, \u0026#34;2020-02-08\u0026#34;), (\u0026#34;2020-02-27\u0026#34;, \u0026#34;2020-02-29\u0026#34;)] ) model.show( title=\u0026#34;Accra\u0026#34;, y_label=\u0026#34;Orders, #\u0026#34;, save=True, ) Summary: model fits well, but the last week exceeds the lower bounds of CI, as it was explored below one of the reason is the drop in the delivery success rate, anyway let\u0026rsquo;s take into account this week for the forecast for March.\nPredict Code\raccra = model.model.make_future_dataframe(periods=36).join(pivot[\u0026#34;Accra\u0026#34;], on=\u0026#34;ds\u0026#34;) model = Prophet(accra, y=\u0026#34;Accra\u0026#34;, date=\u0026#34;ds\u0026#34;, test_days=28) model.run( growth=\u0026#39;linear\u0026#39;, yearly_seasonality=False, monthly_seasonality=False, seasonality_mode=\u0026#39;additive\u0026#39;, outliers=[(\u0026#34;2020-02-06\u0026#34;, \u0026#34;2020-02-08\u0026#34;), (\u0026#34;2020-02-27\u0026#34;, \u0026#34;2020-02-29\u0026#34;)] ) model.show( title=\u0026#34;Accra Prognosis\u0026#34;, y_label=\u0026#34;Orders, #\u0026#34;, save=True, ) Several Notes:\nLisbon: Valentine Day: Feb 14 was unusual day, my initial idea seems to be right based upon the cuisine which was specific to this date. The burgers which is the main cuisine were not so popular this day, but there were spikes for the meals for the group of people and \u0026ldquo;romantic\u0026rdquo; food, namely Japanese (Sushi) \u0026amp; Italian (Pizza) as well as Breakfast, Ice Cream, Desserts Average Check: During the top weekdays in addition to orders number the average order values grows, not only more people order, but each person order more Accra: Delivered Percentage: first of all the delivered percentage for Accra is less than for Lisbon in general (statistical significance is checked below) and the second point is more accurate look at the dynamic, indeed there were some problems on Feb 7 (driver: ios failed) \u0026amp; Feb 28 (driver: android rejected) Delivery Time: there is a positive trend in delivery time, one of the reasons can be the launch of the new suppliers, but it\u0026rsquo;s impossible to check precisely using the given time frame only Conclusion Instead of Summary: If you don\u0026rsquo;t have enough time, to go too much in detail (it doesn\u0026rsquo;t mean that one can solve only well formulated problems, sometimes it only means that a person indeed doesn\u0026rsquo;t have much time) use this high-level approach to explore you data and get immediate insights about your data structure; because after all arbitrarily amount of time can be spent on researching the data, but business never wants to wait.\n","permalink":"https://npodlozhniy.github.io/posts/timeseries-analysis/","summary":"Intro It\u0026rsquo;s a short yet handy guide on how to analyze time series data in a few clicks using modern Python libraries.\nTech stack for this guide consists of\nduckdb and pandas packages for data processing Facebook\u0026rsquo;s prophet for data exploration and forecasting plotly for data visualization Code\rimport duckdb as db import pandas as pd import numpy as np orders = pd.read_csv(\u0026#39;orders_data.csv\u0026#39;) orders.sample() Created Date Country City Restaurant ID Restaurant Name Order State Cancel Reason Cuisine Platform Products in Order Order Value ‚Ç¨ (Gross) Delivery Fee Delivery Time Order ID 44255 10.","title":"Time Series Analysis in Python"}]