<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayesian A/B Test is NOT immune to peeking | Nikita Podlozhniy</title>
<meta name="keywords" content="ab-testing, bayesian-inference, monte-carlo, pymc, sequential-testing">
<meta name="description" content="Practical guide: PyMC examples, hierarchical models, loss functions, HDI, and safe stopping rules">
<meta name="author" content="Nikita Podlozhniy">
<link rel="canonical" href="https://npodlozhniy.github.io/posts/bayesian-test/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://npodlozhniy.github.io/favicons/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://npodlozhniy.github.io/favicons/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://npodlozhniy.github.io/favicons/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://npodlozhniy.github.io/favicons/apple-touch-icon.png">
<link rel="mask-icon" href="https://npodlozhniy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://npodlozhniy.github.io/posts/bayesian-test/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body, 
    {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '\\[', right: '\\]', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false}
              ]
          }
    );"></script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-ESWD18X008"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-ESWD18X008', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Bayesian A/B Test is NOT immune to peeking" />
<meta property="og:description" content="Practical guide: PyMC examples, hierarchical models, loss functions, HDI, and safe stopping rules" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://npodlozhniy.github.io/posts/bayesian-test/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-11-30T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Bayesian A/B Test is NOT immune to peeking"/>
<meta name="twitter:description" content="Practical guide: PyMC examples, hierarchical models, loss functions, HDI, and safe stopping rules"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://npodlozhniy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayesian A/B Test is NOT immune to peeking",
      "item": "https://npodlozhniy.github.io/posts/bayesian-test/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayesian A/B Test is NOT immune to peeking",
  "name": "Bayesian A\/B Test is NOT immune to peeking",
  "description": "Practical guide: PyMC examples, hierarchical models, loss functions, HDI, and safe stopping rules",
  "keywords": [
    "ab-testing", "bayesian-inference", "monte-carlo", "pymc", "sequential-testing"
  ],
  "articleBody": "\rThis interactive notebook demonstrates a concise, pragmatic approach to Bayesian A/B testing using PyMC and analytic Beta-Binomial formulas.\nWhat you‚Äôll find here:\nA minimal, runnable PyMC example to obtain posterior samples. A hierarchical model example for multiple related tests (shrinkage). Analytic Beta-Binomial updates and closed-form PoS / Expected Loss expressions. A Monte Carlo harness comparing frequentist sequential z-tests and several Bayesian stopping rules. TL;DR: Posterior summaries like the Probability of Superiority (PoS) are fantastic for interpretation, but if you stare at them until they cross a threshold (peeking), you will break your error guarantees. üõë If you care about long-run false positives, use decision-theoretic rules (Expected Loss) or precision-aware metrics (HDI).\nü•ß Part 1: A Simple Slice of PyMC Let‚Äôs kick things off with the basics: two variants, A and B, and a binary outcome (conversion: yes/no). Our goal? Use Markov Chains to sample the success probabilities and figure out if A is actually beating B.\nSet up imports Grab PyMC along with must have DS and BI libraries.\nCode\rimport pymc as pm from pymc import Uniform, Bernoulli from matplotlib import pyplot as plt import seaborn as sns import pandas as pd import numpy as np from scipy import stats as sts Simulate observed data Generate sample data: two variants with known conversion rates. In practice, these would be your real observed counts from the experiment.\na_default, b_default = 0.06, 0.04 a_count, b_count = 200, 150 rng = np.random.default_rng(seed=42) a_bernoulli_samples = rng.binomial(n=1, p=a_default, size=a_count) b_bernoulli_samples = rng.binomial(n=1, p=b_default, size=b_count) print( \"Point Estimate\" f\"\\n- A: {a_bernoulli_samples.sum() / a_count :.3f}\" f\"\\n- B: {b_bernoulli_samples.sum() / b_count :.3f}\" ) Point Estimate\r- A: 0.040\r- B: 0.020\rQuick sanity check: plug-in estimates (observed proportions). These will be compared with posterior estimates below.\nDefine the Bayesian model We treat success probabilities as independent random variables. Since we don‚Äôt know much yet, we use a uniform prior (weakly informative - go to for proportions). The observed data (Bernoulli trials) are the likelihood. We also track deterministic difference $= A - B $ , - because that‚Äôs what we actually care about!\nwith pm.Model() as my_model: A_prior = Uniform('A_prior', lower=0, upper=1) B_prior = Uniform('B_prior', lower=0, upper=1) A_observed = Bernoulli('A_observed', p=A_prior, observed=a_bernoulli_samples) B_observed = Bernoulli('B_observed', p=B_prior, observed=b_bernoulli_samples) delta = pm.Deterministic(\"delta\", A_prior - B_prior) Sample from the posterior PyMC unleashes the ‚ÄúNo-U-Turn Sampler‚Äù by default to draw samples from the joint posterior. The tune parameter controls burn-in iterations (discarded); draws are the kept samples used for inference.\nwith my_model: idata = pm.sample(draws=5000, tune=1000, cores=-1) Visualize the posteriors Plot those posterior distributions of each variant and their difference. The vertical black line shows the true (generating) difference; the red line marks zero (no difference). If the posterior difference doesn‚Äôt touch zero, you‚Äôre onto something.\nCode\r# --- Setup Dark Mode --- plt.style.use('dark_background') # Ensure texts are white (sometimes needed depending on Jupyter setup) plt.rcParams.update({ \"text.color\": \"white\", \"axes.labelcolor\": \"white\", \"xtick.color\": \"white\", \"ytick.color\": \"white\" }) fig, axes = plt.subplots(3, 1, figsize=(10, 10), constrained_layout=True) # Neon colors for pop against dark background colors = ['#00FFFF', '#FF00FF', '#32CD32'] # Cyan, Magenta, Lime # --- Plot 1: Posterior P(A) --- ax = axes[0] data_a = idata.posterior['A_prior'].values.ravel() ax.hist(data_a, bins=50, density=True, color=colors[0], alpha=0.7, edgecolor='black', linewidth=1.2, label=\"Posterior $P(A)$\") ax.set_title(\"Posterior Probability of A\", fontsize=14, loc='left', fontweight='bold') ax.legend(loc='upper right', frameon=False) ax.set_ylabel(\"Density\") ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.grid(True, alpha=0.3, linestyle='--') # --- Plot 2: Posterior P(B) --- ax = axes[1] data_b = idata.posterior['B_prior'].values.ravel() ax.hist(data_b, bins=50, density=True, color=colors[1], alpha=0.7, edgecolor='black', linewidth=1.2, label=\"Posterior $P(B)$\") ax.set_title(\"Posterior Probability of B\", fontsize=14, loc='left', fontweight='bold') ax.legend(loc='upper right', frameon=False) ax.set_ylabel(\"Density\") ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.grid(True, alpha=0.3, linestyle='--') # --- Plot 3: Difference --- ax = axes[2] data_delta = idata.posterior['delta'].values.ravel() ax.hist(data_delta, bins=50, density=True, color=colors[2], alpha=0.7, edgecolor='black', linewidth=1.2, label=\"Difference ($A - B$)\") ax.set_title(\"Difference in Probabilities\", fontsize=14, loc='left', fontweight='bold') ax.set_ylabel(\"Density\") ax.set_xlabel(\"Delta Value\") # Vertical Lines ax.axvline(a_default - b_default, color='white', linestyle=':', linewidth=2, label=\"Expected Diff\") ax.axvline(0, color='#FF4444', linestyle='-', linewidth=2, label=\"Zero Difference\") ax.legend(loc='upper right', frameon=False) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.grid(True, alpha=0.3, linestyle='--') # --- Saving with Transparent Background --- # The magic argument is transparent=True + bbox_inches='tight' cuts off extra whitespace around the labels # plt.savefig('dark_bayes_plot_transparent.png', dpi=300, bbox_inches='tight', transparent=True) # to evaluate probability (idata.posterior['delta'].values \u003e 0).mean() plt.show() Figure 1. Probability of superiority: 83.2%\rüèóÔ∏è Part 2: Hierarchical Models (The ‚ÄúRobin Hood‚Äù Approach) When you run many related experiments or compare several variants in the same domain, hierarchical (multilevel) models are a practical way to borrow strength across groups. They reduce variance for small groups (shrinkage) and improve estimation stability. Below is a compact PyMC implementation that models group probabilities as draws from a shared Beta(a, b) prior.\nThis pattern is useful for dashboarding many A/B results together or for pooling information when sample sizes vary across tests.\nWhat is hierarchical models In a hierarchical (or multilevel) model, you assume that the parameters for each group are related and drawn from a common, overarching distribution. This shared distribution is governed by hyper-parameters.\nAll group-level parameters are drawn from a single, shared distribution defined by hyper-parameters a and b. For example each test‚Äôs probability is drawn from a shared Beta(a,b), where a and b are themselves parameters to be estimated.\nThe estimate for any single group is influenced both by its own data and the data from all other groups (via the shared a and b). This is called partial pooling, Say a and b are estimated to best fit all test data, and then every test‚Äôs probability is pulled slightly toward the overall average defined by a and b.\nEstimates for small groups are pulled toward the average (a process called shrinkage), leading to more stable, less extreme estimates\nWhich distribution may be used? Non-Informative Prior for Beta Hyperparameters: $$p(a,b) \\propto (a+b)^{-5/2}$$\nSimplification or approximation of the Jeffreys Prior for the hyper-parameters of the Beta distribution, which is used as a conjugate prior for binomial or Bernoulli likelihoods (common in multiple testing models, e.g., estimating the probability of a true null hypothesis)\nWhen $a$ and $b$ are the shape parameters of Beta distribution, the actual Jeffreys Prior is defined by the Fisher Information matrix:\n$$p(a,b) \\propto \\sqrt{\\det(\\mathbf{I}(a,b))}$$\nThe determinant of the Fisher Information matrix for the Beta distribution‚Äôs parameters is a complex function involving the trigamma function ($\\psi‚Äô$). Specifically, a known simplification used in some computational Bayesian contexts is related to the mean and total effective count of the Beta distribution.\nThe term $\\tau = a+b$ is often interpreted as the total effective sample size (or precision) of the Beta distribution. The exponent $-5/2$ is a specific value that results from one of the approximations designed to make the prior less influential on the posterior, often for the standard deviation or variance of the underlying distribution.\nExample # --- 1. Data Definition --- trials = np.array([842, 854, 862, 821, 839]) successes = np.array([27, 47, 69, 52, 35]) N_GROUPS = len(trials) # Constraint for Beta parameters (a and b must be \u003e 0) ALPHA_MIN = 0.01 with pm.Model() as hierarchical_model: # --- 2. Hyper-parameter Priors (a and b) --- # The Beta shape parameters a and b must be positive. # We define them with a minimally informative uniform prior. a = pm.Uniform(\"a\", lower=ALPHA_MIN, upper=100) b = pm.Uniform(\"b\", lower=ALPHA_MIN, upper=100) # --- 3. Custom Precision Prior (pm.Potential) --- # The original prior was: log p(a, b) = log((a+b)^-2.5) = -2.5 * log(a + b) PRIOR_EXPONENT = -2.5 # Use pm.Potential to add the custom log-prior term to the model's log(P) log_precision_prior = PRIOR_EXPONENT * np.log(a + b) pm.Potential(\"beta_precision_potential\", log_precision_prior) # --- 4. Group-level Prior (occurrences) --- # 'occurrences' is the probability for each group # drawn from a Beta distribution defined by the hyper-parameters a and b. occurrences = pm.Beta(\"occurrences\", alpha=a, beta=b, shape=N_GROUPS) # --- 5. Likelihood (l_obs) --- # The observed successes follow a Binomial distribution. likelihood = pm.Binomial(\"likelihood\", n=trials, p=occurrences, observed=successes) # --- 6. Sampling --- # Sampling is now done with pm.sample() # 5000 draws, 1000 tune (burn-in) idata = pm.sample(draws=1000, tune=1000, cores=-1, chains=2, random_seed=13) # To view the results: print(pm.summary(idata)) Code\rimport warnings warnings.filterwarnings(\"ignore\", category=UserWarning) # 1. Activate Dark Mode plt.style.use('dark_background') # 2. Use the figure-level function sns.displot g = sns.displot( idata.posterior.occurrences[0, :, :].values, kind='kde', # Use KDE for a smoother distribution line color='#00FFFF', # Neon Cyan for high contrast fill=True, alpha=0.6, height=5, aspect=1.5 # Set figure size/ratio ) # 3. Apply final aesthetic touches to the single axis ax = g.ax ax.set_title(\"Distribution of Occurrences\", fontsize=16, loc='left', fontweight='bold', pad=20) ax.set_xlabel(\"Occurrences Count\", fontsize=12) ax.set_ylabel(\"Density\", fontsize=12) # Ensure grid lines are subtle ax.grid(True, alpha=0.3, linestyle='--') # 4. Remove top and right spines ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) plt.show() Figure 2. Posterior probabilities in Hierarchical Model\rCode\rplt.style.use('dark_background') diff_1_vs_4 = (idata.posterior.occurrences[:, :, 1] - idata.posterior.occurrences[:, :, 4]).values.ravel() prob_v1_gt_v4 = (diff_1_vs_4 \u003e 0).mean() # Setup Figure and Axis fig, ax = plt.subplots(figsize=(10, 5)) # Define color (Neon Magenta) neon_color = '#FF00FF' # 1. Create the KDE plot sns.kdeplot( diff_1_vs_4, ax=ax, fill=True, color=neon_color, alpha=0.6, linewidth=2, # The label includes the P(V1 \u003e V4) calculation label=f\"P(V1 \u003e V4)\" ) # 2. Add Zero Line (Crucial for interpretation) # This white dashed line marks the threshold for the probability calculation ax.axvline(0, color='white', linestyle='--', linewidth=1.5, label=\"V1 = V4 (Zero Difference)\") # 3. Apply final aesthetic touches ax.set_title( f\"Posterior Distribution of Difference: V1 vs. V4\", fontsize=16, loc='left', fontweight='bold', pad=20 ) ax.set_xlabel(\"V1 - V4 (Difference)\", fontsize=12) ax.set_ylabel(\"Density\", fontsize=12) # Clean up ax.legend(loc='upper left', frameon=False, fontsize=10) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.grid(True, alpha=0.3, linestyle='--') plt.show() Figure 3. Probability of superiority: 88.7%\r‚ö° Part 3: The Need for Speed (Analytic Bayesian Solutions) MCMC is great, but sometimes you need speed. For the math nerds among us, the Beta-Binomial conjugacy is pure magic. ‚ú®\nIf you have a Beta prior and Binomial data, the posterior is ‚Ä¶ drumroll ‚Ä¶ just another Beta distribution! No complex sampling required - just simple arithmetic. Say there is $Beta(\\alpha, \\beta)$ prior and $k$ successes in $n$ trials, the posterior is $Beta(\\alpha + k, \\beta + n - k)$. This gives us closed-form solutions for the Probability of Superiority (PoS) instantly.\nSay we have 10 heads from ten coin flips, what is the probability to get get a head in the next flip?\nUsing Bayesian approach, where\n$ $ - hypothesis, $\\mathcal{D}$ - data\n$P(\\mathcal{H}) - prior$\n$P( | ) - likelihood $\n$P( | ) - posterior $\n$$ P(\\mathcal{H} | \\mathcal{D}) \\propto P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H}) $$\nIt can be shown that if\n$$ P(\\mathcal{H}) = {Beta}(p; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1} $$\n$$ P(\\mathcal{D} | \\mathcal{H}) = {Binom}(p; k, n) = C_{n}^{k} p^{k} (1-p)^{n-k} $$\nThen\nProof $$ P(\\mathcal{H} | \\mathcal{D}) = \\frac{P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H})}{P(\\mathcal{D})} $$\n$$ P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H}) = \\biggl ( C_{n}^{k} \\cdot p^{k} (1-p)^{n-k} \\biggr ) \\cdot \\biggl ( \\mathbf{\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}} \\cdot p^{\\alpha-1} (1-p)^{\\beta-1} \\biggr ) = C \\cdot p^{\\alpha+k-1}(1-p)^{\\beta+n-k-1} $$\n$$ P(\\mathcal{D}) = \\int_{0}^{1} P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H}) dp = C \\cdot \\int_{0}^{1} p^{\\alpha+k-1} (1-p)^{\\beta+n-k-1} dp = C \\cdot B(\\alpha+k, \\beta+n-k)$$\nConsequently combining those two equations, The Binomial Constant and the Prior Beta Constant are completely canceled out\n$$ P(\\mathcal{H} | \\mathcal{D}) = \\frac{1}{B(\\alpha+k, \\beta+n-k)} p^{\\alpha+k-1}(1-p)^{\\beta+n-k-1} = {Beta}(p; \\alpha + k, \\beta + n - k) $$\nEnd of Proof\nIn case of non-informative prior $Beta(p; 1, 1) $ what basically given Uniform distribution of the prior - Beta function may be presented with a short binomial coefficient formula\n$$ B(k+1, n-k+1) = \\frac{–ì(k+1)–ì(n-k+1)}{–ì(n+2)} = \\frac{k!(n-k)!}{(n+1)!} = \\frac{1}{(n+1); C_{n}^{k}} $$\nIn our case the posterior distribution is $ {Beta}(p; k + 1, n - k + 1) $\nHence we can build the predictive interval using this Beta distribution moments:\n$$ \\mu = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{k+1}{n+2} $$\nThis formula for $ $ is also used as the Laplace sequence rule, which requires adding one positive and one negative observation to estimate the posterior probability distribution for a random sample.\nAdding second moment\n$$ \\sigma^2 = {\\frac {\\alpha \\beta }{(\\alpha +\\beta )^{2}(\\alpha +\\beta +1)}} = \\frac{(k+1)(n-k+1)}{(n+2)^2(n+3)} $$\nmu = 11 / 12 sigma = (11 / 12 ** 2 / 13) ** 0.5 print(f\"Hence 2 sigma predictive interval for 10/10 successful flips is {mu:.2%} ¬± {2 * sigma :.2%}\") print(f\"Alternatively as a pair of bounds: {mu - 2 * sigma:.2%} - {min(1, mu + 2 * sigma):.2%}\") Hence 2 sigma predictive interval for 10/10 successful flips is 91.67% ¬± 15.33%\rAlternatively as a pair of bounds: 76.34% - 100.00%\rBut it‚Äôs not a Normal distribution, so it‚Äôs not 95% confidence interval, we need to take Beta distribution quantile instead or calculate it precisely. It resembles the approximation that we get using normal distribution quantiles.\nl, r = sts.beta.ppf([0.05, 1.0], a=11, b=1) print(f\"Beta predictive interval {l:.2%} - {r:.2%}\") Beta predictive interval 76.16% - 100.00%\rAnother way is get that number analytically from the integral equation that fully coincides with the value from stats package.\n$$ \\int_{p_{crit}}^{1} (n+1) \\cdot C_n^n \\cdot p^n (1-p)^0 dp = 0.95 $$\n$$ p_{crit} = \\sqrt[n + 1]{0.05}$$\nprint(f\"What makes it an easy computation: p is from {0.05 ** (1/11):.2%} - 100.00%\") What makes it an easy computation: p is from 76.16% - 100.00%\rCriterion that is used for analytical model decision making in A/B experiment\n$$ P(\\lambda_B \u003e \\lambda_A) = \\int_{p_B \u003e p_A} P(p_A, p_B | \\text{Data}) , dp_A , dp_B = \\sum_{i=0}^{\\alpha_B-1} \\frac{B(\\alpha_A+i, \\beta_A+\\beta_B)}{(\\beta_B+i)B(1+i, \\beta_B)B(\\alpha_A, \\beta_A)} $$\nThe formula is the result of applying a well-known mathematical identity that allows the cumulative probability of one Beta variable being less than another Beta variable to be expressed as a finite sum of terms involving the Beta function, rather than requiring complex numerical integration. This is why this formula is computationally efficient and preferred for exact Bayesian A/B calculations.\nRule of three: when no successes are observed üí° The rule of three is used to provide a simple way of stating an approximate 95% confidence interval in the special case that no successes have been observed - $(0, 3/n)$, alternatively by symmetry, in case of only successes $(1 - 3/n, 1) $.\nOn the other hand mathematically, if all conversions are zero, then we simply may build an equation for upper bound\n$(1-p)^n \\geq \\alpha$, where $\\alpha = .05$ and hence $n \\leq log_{.95}(.05)$\nFor example - how many trials needed to challenge the null hypothesis that the success probability is zero?\nprint(\"Approximate N:\", int(3 / 0.05)) print(\"Exact N:\", 1 + int((np.log(0.05) / np.log(0.95)))) Approximate N: 60\rExact N: 59\rQuick Tip for Zero Successes: If you launch a test and get zero conversions, don‚Äôt panic. Use the Rule of Three: your approximate 95% upper bound is simply 3/n.¬†It‚Äôs a ‚Äúpretty decent‚Äù (and fast) estimate without needing a calculator.\nüò± Part 4: The Plot Twist (Peeking) Here is the controversial bit: Bayesian A/B testing is NOT immune to peeking. The Myth: ‚ÄúI can check my Bayesian results whenever I want, and it‚Äôs always valid!‚Äù The Reality: Mathematically, the posterior is valid. BUT, if you use a fixed rule like ‚ÄúStop when Probability \u003e 95%,‚Äù you will inflate your False Positive Rate over time. You are essentially fishing for significance. If you stop early just because you crossed a line, you are falling into the same trap as frequentists, but but first things first.\nTo compare stopping rules and power, the notebook includes a Monte Carlo harness. It simulates repeated experiments, applies frequentist sequential z-tests and several Bayesian stopping rules (naive PoS threshold, expected-loss stopping (OLF), and HDI \u0026 PoS combinations), and compares false positive rates and average stopping sample sizes.\nCode\rfrom typing import Callable n_iterations = 1000 def min_sample_size(mde, mu, sigma, alpha=0.05, power=0.80) -\u003e int: \"\"\" Defines superiority one-side z-test sample size Args: mde: Relative uplift mu: Expected Value sigma: Square root of variance alpha: False Positive Rate, default = 0.05 power: Experiment power, default = 0.80 Returns: Required sample size to achieve the power \"\"\" effect_size = abs(mde) * mu / sigma return int(((sts.norm.ppf(1 - alpha) + sts.norm.ppf(power)) / effect_size) ** 2) def stops_at(is_significant: np.ndarray, sample_size: np.ndarray) -\u003e int: \"\"\" Determines the stopping sample size. This function identifies the first instance where the input condition is True and returns the corresponding sample size. Args: is_significant: A boolean array of the stop condition for each size sample_size: An array of sample sizes. Returns: The stopping sample size. Example: \u003e\u003e\u003e stops_at([False, False, True, True], [50, 100, 150, 200]) 150 \"\"\" if len(is_significant) != len(sample_size): raise ValueError(\"Input arrays must have the same length.\") w = np.where(is_significant)[0] return np.nan if len(w) == 0 else sample_size[w[0]] def monte_carlo( bayesian_stop_rule, effect_size: float=0.10, aa_test: bool=True, alpha: float=0.05, peeks: int = 1, ) -\u003e None: result = { 'Frequentist': [], 'Bayesian': [], } p = 0.20 sigma = (p * (1 - p)) ** 0.5 relative_effect = 0 if aa_test else effect_size N = min_sample_size(mde=effect_size, mu=p, sigma=sigma, alpha=alpha) n = int(N / peeks) print(f\"Running {n_iterations} simulations with total sample size {N} that is achieved in {peeks} iterations of {n} size each\") for seed in tqdm(range(n_iterations)): rng = np.random.default_rng(seed) binomial_samples = rng.binomial(n=n, p=p*(1+relative_effect), size=peeks) sizes = np.arange(n, N + 1, n) conversions = np.cumsum(binomial_samples) z_scores = [(success / trials - p) / np.sqrt(sigma ** 2 / trials) for success, trials in zip(conversions, sizes)] is_prob_high_enough = [ bayesian_stop_rule( success=success, trials=trials, alpha=alpha, p=p, effect_size=effect_size ) for success, trials in zip(conversions, sizes) ] result['Frequentist'].append(stops_at(z_scores \u003e sts.norm.ppf(1 - alpha), sizes)) result['Bayesian'].append(stops_at(is_prob_high_enough, sizes)) result = \"\\n\".join([ f\"Frequentist Rejected Rate: {np.mean(~np.isnan(result['Frequentist']))}\", f\"Frequentist Required Sample Size: {int(np.nanmean(result['Frequentist']))}\", f\"Bayesian Rejected Rate: {np.mean(~np.isnan(result['Bayesian']))}\", f\"Bayesian Required Sample Size: {int(np.nanmean(result['Bayesian']))}\", ]) print(result) return def POS(success: int, trials: int, alpha: float, p: float, **kwargs) -\u003e bool: \"\"\" Probability of Superiority decision rule \"\"\" return sts.beta.cdf(p, a = 1 + success, b = 1 + trials - success) \u003c alpha Correctness monte_carlo(bayesian_stop_rule=POS, peeks=1, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 1 iterations of 2473 size each\rFrequentist Rejected Rate: 0.06\rFrequentist Required Sample Size: 2473\rBayesian Rejected Rate: 0.06\rBayesian Required Sample Size: 2473\rmonte_carlo(bayesian_stop_rule=POS, peeks=5, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each\rFrequentist Rejected Rate: 0.126\rFrequentist Required Sample Size: 1015\rBayesian Rejected Rate: 0.126\rBayesian Required Sample Size: 1015\rA/B design power monte_carlo(bayesian_stop_rule=POS, peeks=5, aa_test=False) Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each\rFrequentist Rejected Rate: 0.855\rFrequentist Required Sample Size: 1146\rBayesian Rejected Rate: 0.855\rBayesian Required Sample Size: 1146\rThat is a crucial observation and it points to a common misunderstanding about Bayesian A/B testing:\nNo, the standard Bayesian approach does not handle peeking (optional stopping) correctly by default if your goal is to control the frequentist Type I Error Rate (False Positive Rate).\nWhile the Bayesian interpretation of results remains valid at any time, using a fixed threshold (e.g., stopping when $P(A \u003e B) \u003e 95%$) and checking repeatedly will lead to an inflated False Positive Rate over many hypothetical experiments, just like in the frequentist approach.\nWhat is NOT Affected (The Bayesian Advantage) The Posterior Distribution and the Probability of Superiority ‚Äî is always valid, regardless of when you look at the data.\nWhat IS Affected (The Peeking Problem) The problem arises when you use a fixed decision rule (like the $95%$ threshold) to stop the test prematurely, based on the outcome.\nThe Myth: The common claim that ‚ÄúBayesian testing is immune to peeking‚Äù is overstated. It is only immune in the sense that the posterior is always mathematically correct. It is not immune in the sense that it prevents the inflation of the frequentist Type I Error Rate when using a simple, fixed stopping threshold\nüõ°Ô∏è Part 5: How to Peek Safely How Bayesian Methods Truly Handle Peeking To safely peek and stop early in a Bayesian framework, you need to base your decision on a metric that incorporates the cost of a wrong decision, not just the probability of a difference.\nThe correct Bayesian decision procedure is to stop when:\nExpected Loss (EL) is Minimized: You stop the test when the Expected Loss of choosing the suboptimal variant falls below a commercially acceptable threshold $\\epsilon$. This naturally accounts for uncertainty.13 If the posterior distributions are still wide (high uncertainty), the loss will be high, and you won‚Äôt stop. $$ E[L](p_a \u003e p_b) = \\int_0^1\\int_0^1L(p_a,p_b, p_a \u003e p_b)P(p_a|a,b,n_a,k_a)P(p_b|a,b,n_b,k_b)dp_adp_b $$\nSequential Designs (Like Multi-Armed Bandits): Techniques like Thompson Sampling are inherently Bayesian and sequential. They don‚Äôt have a stopping rule based on error rates; they simply choose the best variant to show next based on the posterior, which naturally directs more traffic to the likely winner, making the experiment efficient without needing a fixed sample size. Example of Loss function applied Define the Opportunity Loss Function $L(p_A, p_B)$, which is the regret you incur by choosing a variant that is not the best.If we Choose B, the loss only happens if $p_A \u003e p_B$: $$L(\\text{Choose B}) = \\max(0, p_A - p_B)$$ If we Choose A, the loss only happens if $p_B \u003e p_A$: $$L(\\text{Choose A}) = \\max(0, p_B - p_A)$$\nUsing known properties and identities related to the Beta function, this complex double integral for Opportunity Loss function can be transformed into a closed-form summation:\n$$ EL_A = \\sum_{i=0}^{\\alpha_B-1} \\frac{\\alpha_A \\cdot B(\\alpha_A+i+1, \\beta_A+\\beta_B)}{\\beta_A \\cdot B(i+1, \\beta_B) \\cdot B(\\alpha_A, \\beta_A)} - \\sum_{i=0}^{\\alpha_B-1} \\frac{\\alpha_B \\cdot B(\\alpha_A+i, \\beta_A+\\beta_B+1)}{(\\beta_B+i) \\cdot B(i+1, \\beta_B) \\cdot B(\\alpha_A, \\beta_A)} $$\nThe Formula for one-sample test is against benchmark $\\lambda_0$:\n$$ EL_A = \\lambda_0 \\cdot I_{\\lambda_0}(\\alpha, \\beta) - \\frac{\\alpha}{\\alpha+\\beta} \\cdot I_{\\lambda_0}(\\alpha+1, \\beta) $$\nPython Implementation note that betainc $(\\alpha, \\beta, \\lambda)$ calculates $I_{\\lambda}(\\alpha, \\beta)$, that is an equivalent of sts.beta.cdf$(\\lambda, \\alpha, \\beta)$\nCode\rfrom scipy.special import betainc def calculate_opportunity_loss_one_sample( k: int, n: int, lambda_0: float, prior_alpha: int = 1, prior_beta: int = 1, ) -\u003e float: \"\"\" Calculates the Expected Loss of choosing the observed variant against a benchmark using the analytical formula. Absolute value of conversion loss. Non-informative conjugate prior is used by Default. Args: k (int): Observed successes (conversions). n (int): Observed trials (sizes). lambda_0 (float): The fixed conversion rate benchmark. prior_alpha (int): Prior alpha hyper-parameter. prior_beta (int): Prior beta hyper-parameter. Returns: float: The Expected Loss of choosing the observed variant. \"\"\" # Calculate Posterior Parameters (alpha and beta) alpha = k + prior_alpha beta = n - k + prior_beta # --- Term 1: Probability of Loss --- # Calculates I_lambda_bench(alpha, beta) = P(lambda_obs \u003c lambda_bench) # The term is: lambda_bench * P(lambda_obs \u003c lambda_bench) term1 = lambda_0 * betainc(alpha, beta, lambda_0) # --- Term 2: Weighted Expected Value --- # The fraction part: alpha / (alpha + beta) is the mean of Beta(alpha, beta) term2 = alpha / (alpha + beta) * betainc(alpha + 1, beta, lambda_0) return term1 - term2 Frequentist: We must collect $N$ samples to have an $1-\\beta$ chance of detecting a $MDE$ difference with $\\alpha$ error.\nBayesian: We will stop the test when the average potential loss incurred by choosing the sub-optimal variant is less than $\\epsilon$ percentage points. By setting a small $\\epsilon$, you ensure that the test continues until the potential future regret (loss) is extremely low, thus ensuring a high degree of confidence in the final decision while retaining the ability to peek safely\nAdjust Monte Carlo procedure with another Bayesian stopping rule\nCode\rdef OLF(success: int, trials: int, p: float, effect_size: float, **kwargs) -\u003e bool: \"\"\"Opportunity Loss Function stopping rule. Epsilon is usually set to a fraction of MDE\"\"\" fraction = 1 / 100 epsilon = fraction * effect_size * p return calculate_opportunity_loss_one_sample(success, trials, lambda_0=p) \u003c epsilon monte_carlo(bayesian_stop_rule=OLF, peeks=30, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 30 iterations of 82 size each\rFrequentist Rejected Rate: 0.249\rFrequentist Required Sample Size: 590\rBayesian Rejected Rate: 0.21\rBayesian Required Sample Size: 839\rSo, Loss Function doesn‚Äôt save from Peeking problem, it‚Äôs even more vulnerable than well-known p-value approach\nWe need another piece of the puzzle ‚Ä¶\nHere comes the sun üå§Ô∏è ‚Ä¶ and HDI Highest Density Interval (sometimes called Highest Posterior Density Interval).\nDefinition: The $X%$ HDI is the narrowest interval that contains $X%$ of the probability mass of the posterior distribution.\nPurpose: It is the Bayesian equivalent of the frequentist Confidence Interval (CI), but unlike the CI, you can state that there is an $X%$ probability that the true parameter value (e.g., the true conversion rate) lies within the HDI.\nThe width of the HDI is simply (Upper Bound - Lower Bound). It is a direct and intuitive measure of the remaining uncertainty. A wide HDI means your posterior is flat and uncertain; a narrow HDI means your posterior is sharply peaked and confident.\nCode\rimport arviz as az def calculate_beta_hdi_width(alpha: float, beta: float, hdi_prob=0.95, num_samples=10_000) -\u003e float: \"\"\" Calculates the Highest Density Interval (HDI) for a Beta distribution using Monte Carlo sampling and the arviz library. The calculation of the HDI is an iterative process that must find the interval boundary points where the probability density is equal, while the area between the points equals the target probability. Since Beta distribution is generally not symmetrical, the HDI bounds are not the same as the quantiles, which is why a specialized function is needed. Args: alpha (float): The posterior alpha parameter (k_obs + prior_alpha). beta (float): The posterior beta parameter (n_obs - k_obs + prior_beta). hdi_prob (float): The target probability mass (e.g., 0.95 for 95% HDI). num_samples (int): Number of samples to draw for Monte Carlo calculation. Returns: tuple: (lower_bound, upper_bound, width) \"\"\" posterior_samples = sts.beta.rvs(a=alpha, b=beta, size=num_samples) # designed to work on posterior samples hdi_interval = az.hdi(posterior_samples, hdi_prob=hdi_prob) lower_bound = hdi_interval[0] upper_bound = hdi_interval[1] return upper_bound - lower_bound Let‚Äôs update Monte Carlo once again with a combination of PoS and HDI stopping what represent business and statistical robustness respectively, shall we? - Note that HDI density affects the inference vastly and you‚Äôd better experiment to pick up a good for for the data.\nCode\rprint(f\"Width is multiplied by {round(calculate_beta_hdi_width(100, 100, .99) / calculate_beta_hdi_width(100, 100, 0.8), 2)} when increase required density from 0.8 to 0.99\") Width is multiplied by 1.98 when increase required density from 0.8 to 0.99\rCode\rdef HDI(success: int, trials: int, alpha: float, p: float, effect_size: float) -\u003e bool: \"\"\" PoS combined with 95% HDI stopping rule \"\"\" return ( # checks if 95% of posterior distribution is narrow enough and lays in ¬± MDE (calculate_beta_hdi_width(1 + success, 1 + trials - success) \u003c 2 * effect_size * p) # checks if posterior distribution by 95% chance better and POS(success, trials, alpha, p) ) Correctness\nmonte_carlo(bayesian_stop_rule=OLF, peeks=1, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 1 iterations of 2473 size each\rFrequentist Rejected Rate: 0.06\rFrequentist Required Sample Size: 2473\rBayesian Rejected Rate: 0.069\rBayesian Required Sample Size: 2473\rmonte_carlo(bayesian_stop_rule=HDI, peeks=10, aa_test=True) Running 1000 simulations with total sample size 2473 that is achieved in 10 iterations of 247 size each\rFrequentist Rejected Rate: 0.193\rFrequentist Required Sample Size: 876\rBayesian Rejected Rate: 0.1\rBayesian Required Sample Size: 1924\rPower\nmonte_carlo(bayesian_stop_rule=HDI, peeks=5, aa_test=False) Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each\rFrequentist Rejected Rate: 0.855\rFrequentist Required Sample Size: 1146\rBayesian Rejected Rate: 0.818\rBayesian Required Sample Size: 2035\rHDI accompanied by Probability of Superiority is a good criterion, although very strict if you increase the required density for HDI from 80% above 95% and it hence requires bigger sample size than Frequentist approach.\nCombination of HDI and PoS checks makes the criterion less sensitive to peeking, however it‚Äôs yet not fully immune.\nStopping Rules Overview This table breaks down three common criteria used in Bayesian A/B testing, highlighting their function and their robustness against peeking (stopping a test too early based on transient results).\nCriterion Function / Definition Robustness Against Peeking Probability of Superiority (PoS) Measures how often the posterior probability of $\\lambda_B$ is greater than $\\lambda_A$ (i.e., $P(\\lambda_B \u003e \\lambda_A)$). Low Robustness. The threshold can be quickly and spuriously crossed by early noise or transient fluctuations. Expected Loss (EL) Measures the average Cost or Regret of selecting the inferior variant. High Robustness. Requires the posterior distribution to be tight enough that the potential loss (regret) is small, preventing premature stopping. HDI Width Measures the Precision or Uncertainty of the posterior distribution (e.g., the width of the 95% credible interval). High Robustness. Forces the test to continue until the uncertainty is low (the HDI is narrow), regardless of the posterior mean, ensuring adequate data collection. üìù Conclusions \u0026 Practical Recommendations Use the posterior (and PoS) to interpret results, but prefer decision-theoretic stopping when making business choices: stop when the expected loss of choosing the sub-optimal variant is below a tolerated threshold. . Combine HDI (precision) with PoS (direction) for a conservative, safe stopping rule - but higher density HDI thresholds require larger samples. When many variants or small groups are present, hierarchical models provide safer estimates via partial pooling. If your goal is to guarantee frequentist properties (e.g., Type I control under peeking), design the sequential procedure explicitly: group sequential testing with alpha-spending function or always valid inference approach - Sequential Testing Guide will let you know all you need Further experiments to try:\nReplace uniform priors with domain-informed priors when available. Explore Thompson Sampling for continuous allocation instead of fixed-sample stopping. Visualize posterior trajectories and stopping-rule trade-offs across simulated peeks. Thanks for reading, feel free to fork this notebook and go forth, experiment with your own traffic and loss thresholds as well as react to the post below, and may your posteriors always be narrow!\n",
  "wordCount" : "4870",
  "inLanguage": "en",
  "datePublished": "2025-11-30T00:00:00Z",
  "dateModified": "2025-11-30T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Nikita Podlozhniy"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://npodlozhniy.github.io/posts/bayesian-test/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nikita Podlozhniy",
    "logo": {
      "@type": "ImageObject",
      "url": "https://npodlozhniy.github.io/favicons/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://npodlozhniy.github.io/" accesskey="h" title="Nikita Podlozhniy (Alt + H)">Nikita Podlozhniy</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://npodlozhniy.github.io/ru/" title="Russian"
                            aria-label=":ru:">Ru</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://npodlozhniy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://npodlozhniy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://npodlozhniy.github.io/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://npodlozhniy.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://npodlozhniy.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://npodlozhniy.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Bayesian A/B Test is NOT immune to peeking
    </h1>
    <div class="post-meta"><span title='2025-11-30 00:00:00 +0000 UTC'>November 30, 2025</span>&nbsp;¬∑&nbsp;23 min&nbsp;¬∑&nbsp;Nikita Podlozhniy

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#-part-1-a-simple-slice-of-pymc" aria-label="ü•ß Part 1: A Simple Slice of PyMC">ü•ß Part 1: A Simple Slice of PyMC</a><ul>
                        
                <li>
                    <a href="#set-up-imports" aria-label="Set up imports">Set up imports</a></li>
                <li>
                    <a href="#simulate-observed-data" aria-label="Simulate observed data">Simulate observed data</a></li>
                <li>
                    <a href="#define-the-bayesian-model" aria-label="Define the Bayesian model">Define the Bayesian model</a></li>
                <li>
                    <a href="#sample-from-the-posterior" aria-label="Sample from the posterior">Sample from the posterior</a></li>
                <li>
                    <a href="#visualize-the-posteriors" aria-label="Visualize the posteriors">Visualize the posteriors</a></li></ul>
                </li>
                <li>
                    <a href="#-part-2-hierarchical-models-the-robin-hood-approach" aria-label="üèóÔ∏è Part 2: Hierarchical Models (The &amp;ldquo;Robin Hood&amp;rdquo; Approach)">üèóÔ∏è Part 2: Hierarchical Models (The &ldquo;Robin Hood&rdquo; Approach)</a><ul>
                        
                <li>
                    <a href="#what-is-hierarchical-models" aria-label="What is hierarchical models">What is hierarchical models</a></li>
                <li>
                    <a href="#which-distribution-may-be-used" aria-label="Which distribution may be used?">Which distribution may be used?</a></li>
                <li>
                    <a href="#example" aria-label="Example">Example</a></li></ul>
                </li>
                <li>
                    <a href="#-part-3-the-need-for-speed-analytic-bayesian-solutions" aria-label="‚ö° Part 3: The Need for Speed (Analytic Bayesian Solutions)">‚ö° Part 3: The Need for Speed (Analytic Bayesian Solutions)</a><ul>
                        
                <li>
                    <a href="#rule-of-three-when-no-successes-are-observed-" aria-label="Rule of three: when no successes are observed üí°">Rule of three: when no successes are observed üí°</a></li></ul>
                </li>
                <li>
                    <a href="#-part-4-the-plot-twist-peeking" aria-label="üò± Part 4: The Plot Twist (Peeking)">üò± Part 4: The Plot Twist (Peeking)</a><ul>
                        
                <li>
                    <a href="#correctness" aria-label="Correctness">Correctness</a></li>
                <li>
                    <a href="#ab-design-power" aria-label="A/B design power">A/B design power</a></li></ul>
                </li>
                <li>
                    <a href="#-part-5-how-to-peek-safely" aria-label="üõ°Ô∏è Part 5: How to Peek Safely">üõ°Ô∏è Part 5: How to Peek Safely</a><ul>
                        
                <li>
                    <a href="#how-bayesian-methods-truly-handle-peeking" aria-label="How Bayesian Methods Truly Handle Peeking">How Bayesian Methods Truly Handle Peeking</a></li>
                <li>
                    <a href="#example-of-loss-function-applied" aria-label="Example of Loss function applied">Example of Loss function applied</a></li>
                <li>
                    <a href="#here-comes-the-sun---and-hdi" aria-label="Here comes the sun üå§Ô∏è &amp;hellip; and HDI">Here comes the sun üå§Ô∏è &hellip; and HDI</a></li>
                <li>
                    <a href="#stopping-rules-overview" aria-label="Stopping Rules Overview">Stopping Rules Overview</a></li></ul>
                </li>
                <li>
                    <a href="#-conclusions--practical-recommendations" aria-label="üìù Conclusions &amp;amp; Practical Recommendations">üìù Conclusions &amp; Practical Recommendations</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><!--
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<p>This interactive notebook demonstrates a concise, pragmatic approach to Bayesian A/B testing using PyMC and analytic Beta-Binomial formulas.</p>
<p>What you&rsquo;ll find here:</p>
<ul>
<li>A minimal, runnable PyMC example to obtain posterior samples.</li>
<li>A hierarchical model example for multiple related tests (shrinkage).</li>
<li>Analytic Beta-Binomial updates and closed-form PoS / Expected Loss expressions.</li>
<li>A Monte Carlo harness comparing frequentist sequential z-tests and several Bayesian stopping rules.</li>
</ul>
<p>TL;DR: Posterior summaries like the Probability of Superiority (PoS) are fantastic for interpretation, but if you stare at them until they cross a threshold (peeking), you will break your error guarantees. üõë If you care about long-run false positives, use decision-theoretic rules (Expected Loss) or precision-aware metrics (HDI).</p>
<h2 id="-part-1-a-simple-slice-of-pymc">ü•ß Part 1: A Simple Slice of PyMC<a hidden class="anchor" aria-hidden="true" href="#-part-1-a-simple-slice-of-pymc">#</a></h2>
<p>Let&rsquo;s kick things off with the basics: two variants, A and B, and a binary outcome (conversion: yes/no). Our goal? Use Markov Chains to sample the success probabilities and figure out if A is actually beating B.</p>
<h3 id="set-up-imports">Set up imports<a hidden class="anchor" aria-hidden="true" href="#set-up-imports">#</a></h3>
<p>Grab PyMC along with must have DS and BI libraries.</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pymc <span style="color:#66d9ef">as</span> pm
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pymc <span style="color:#f92672">import</span> Uniform, Bernoulli
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy <span style="color:#f92672">import</span> stats <span style="color:#66d9ef">as</span> sts
</span></span></code></pre></div></details>
<h3 id="simulate-observed-data">Simulate observed data<a hidden class="anchor" aria-hidden="true" href="#simulate-observed-data">#</a></h3>
<p>Generate sample data: two variants with known conversion rates. In practice, these would be your real observed counts from the experiment.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a_default, b_default <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.06</span>, <span style="color:#ae81ff">0.04</span>
</span></span><span style="display:flex;"><span>a_count, b_count <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">150</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rng <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>default_rng(seed<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a_bernoulli_samples <span style="color:#f92672">=</span> rng<span style="color:#f92672">.</span>binomial(n<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, p<span style="color:#f92672">=</span>a_default, size<span style="color:#f92672">=</span>a_count)
</span></span><span style="display:flex;"><span>b_bernoulli_samples <span style="color:#f92672">=</span> rng<span style="color:#f92672">.</span>binomial(n<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, p<span style="color:#f92672">=</span>b_default, size<span style="color:#f92672">=</span>b_count)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Point Estimate&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">- A: </span><span style="color:#e6db74">{</span>a_bernoulli_samples<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> a_count <span style="color:#e6db74">:</span><span style="color:#e6db74">.3f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">- B: </span><span style="color:#e6db74">{</span>b_bernoulli_samples<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> b_count <span style="color:#e6db74">:</span><span style="color:#e6db74">.3f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><pre><code>Point Estimate
- A: 0.040
- B: 0.020
</code></pre>
<p>Quick sanity check: plug-in estimates (observed proportions). These will be compared with posterior estimates below.</p>
<h3 id="define-the-bayesian-model">Define the Bayesian model<a hidden class="anchor" aria-hidden="true" href="#define-the-bayesian-model">#</a></h3>
<p>We treat success probabilities as independent random variables. Since we don&rsquo;t know much yet, we use a uniform prior (weakly informative - go to for proportions). The observed data (Bernoulli trials) are the likelihood. We also track <em>deterministic</em> difference $= A - B $ , - because that&rsquo;s what we actually care about!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">with</span> pm<span style="color:#f92672">.</span>Model() <span style="color:#66d9ef">as</span> my_model:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    A_prior <span style="color:#f92672">=</span> Uniform(<span style="color:#e6db74">&#39;A_prior&#39;</span>, lower<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, upper<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    B_prior <span style="color:#f92672">=</span> Uniform(<span style="color:#e6db74">&#39;B_prior&#39;</span>, lower<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, upper<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    A_observed <span style="color:#f92672">=</span> Bernoulli(<span style="color:#e6db74">&#39;A_observed&#39;</span>, p<span style="color:#f92672">=</span>A_prior, observed<span style="color:#f92672">=</span>a_bernoulli_samples)
</span></span><span style="display:flex;"><span>    B_observed <span style="color:#f92672">=</span> Bernoulli(<span style="color:#e6db74">&#39;B_observed&#39;</span>, p<span style="color:#f92672">=</span>B_prior, observed<span style="color:#f92672">=</span>b_bernoulli_samples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    delta <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Deterministic(<span style="color:#e6db74">&#34;delta&#34;</span>, A_prior <span style="color:#f92672">-</span> B_prior)
</span></span></code></pre></div><h3 id="sample-from-the-posterior">Sample from the posterior<a hidden class="anchor" aria-hidden="true" href="#sample-from-the-posterior">#</a></h3>
<p>PyMC unleashes the &ldquo;No-U-Turn Sampler&rdquo; by default to draw samples from the joint posterior. The <code>tune</code> parameter controls burn-in iterations (discarded); <code>draws</code> are the kept samples used for inference.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">with</span> my_model:
</span></span><span style="display:flex;"><span>    idata <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>sample(draws<span style="color:#f92672">=</span><span style="color:#ae81ff">5000</span>, tune<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, cores<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><h3 id="visualize-the-posteriors">Visualize the posteriors<a hidden class="anchor" aria-hidden="true" href="#visualize-the-posteriors">#</a></h3>
<p>Plot those posterior distributions of each variant and their difference. The vertical black line shows the true (generating) difference; the red line marks zero (no difference). If the posterior difference doesn&rsquo;t touch zero, you&rsquo;re onto something.</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># --- Setup Dark Mode ---</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;dark_background&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Ensure texts are white (sometimes needed depending on Jupyter setup)</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams<span style="color:#f92672">.</span>update({
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;text.color&#34;</span>: <span style="color:#e6db74">&#34;white&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;axes.labelcolor&#34;</span>: <span style="color:#e6db74">&#34;white&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;xtick.color&#34;</span>: <span style="color:#e6db74">&#34;white&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;ytick.color&#34;</span>: <span style="color:#e6db74">&#34;white&#34;</span>
</span></span><span style="display:flex;"><span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>), constrained_layout<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Neon colors for pop against dark background</span>
</span></span><span style="display:flex;"><span>colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;#00FFFF&#39;</span>, <span style="color:#e6db74">&#39;#FF00FF&#39;</span>, <span style="color:#e6db74">&#39;#32CD32&#39;</span>] <span style="color:#75715e"># Cyan, Magenta, Lime</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- Plot 1: Posterior P(A) ---</span>
</span></span><span style="display:flex;"><span>ax <span style="color:#f92672">=</span> axes[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>data_a <span style="color:#f92672">=</span> idata<span style="color:#f92672">.</span>posterior[<span style="color:#e6db74">&#39;A_prior&#39;</span>]<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>ravel()
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>hist(data_a, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, density<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">0</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>, 
</span></span><span style="display:flex;"><span>        edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1.2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Posterior $P(A)$&#34;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Posterior Probability of A&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>, loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;left&#39;</span>, fontweight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bold&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>, frameon<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Density&#34;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- Plot 2: Posterior P(B) ---</span>
</span></span><span style="display:flex;"><span>ax <span style="color:#f92672">=</span> axes[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>data_b <span style="color:#f92672">=</span> idata<span style="color:#f92672">.</span>posterior[<span style="color:#e6db74">&#39;B_prior&#39;</span>]<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>ravel()
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>hist(data_b, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, density<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>, 
</span></span><span style="display:flex;"><span>        edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1.2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Posterior $P(B)$&#34;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Posterior Probability of B&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>, loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;left&#39;</span>, fontweight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bold&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>, frameon<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Density&#34;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- Plot 3: Difference ---</span>
</span></span><span style="display:flex;"><span>ax <span style="color:#f92672">=</span> axes[<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>data_delta <span style="color:#f92672">=</span> idata<span style="color:#f92672">.</span>posterior[<span style="color:#e6db74">&#39;delta&#39;</span>]<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>ravel()
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>hist(data_delta, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, density<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">2</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>, 
</span></span><span style="display:flex;"><span>        edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1.2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Difference ($A - B$)&#34;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Difference in Probabilities&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>, loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;left&#39;</span>, fontweight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bold&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Density&#34;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;Delta Value&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Vertical Lines</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>axvline(a_default <span style="color:#f92672">-</span> b_default, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;:&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Expected Diff&#34;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>axvline(<span style="color:#ae81ff">0</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#FF4444&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;-&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Zero Difference&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>, frameon<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># --- Saving with Transparent Background ---</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The magic argument is transparent=True + bbox_inches=&#39;tight&#39; cuts off extra whitespace around the labels</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># plt.savefig(&#39;dark_bayes_plot_transparent.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;, transparent=True)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># to evaluate probability (idata.posterior[&#39;delta&#39;].values &gt; 0).mean()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div></details>
<img src="BayesianAB_files/figure-markdown_strict/fig-bayesian-plot-1-output-1.png" id="fig-bayesian-plot-1" alt="Figure¬†1: Posterior distributions for the conversion rates of variants A and B" />
<p style="text-align: center;">
Figure 1. Probability of superiority: 83.2%
</p>
<h2 id="-part-2-hierarchical-models-the-robin-hood-approach">üèóÔ∏è Part 2: Hierarchical Models (The &ldquo;Robin Hood&rdquo; Approach)<a hidden class="anchor" aria-hidden="true" href="#-part-2-hierarchical-models-the-robin-hood-approach">#</a></h2>
<p>When you run many related experiments or compare several variants in the same domain, hierarchical (multilevel) models are a practical way to borrow strength across groups. They reduce variance for small groups (shrinkage) and improve estimation stability. Below is a compact PyMC implementation that models group probabilities as draws from a shared Beta(a, b) prior.</p>
<p>This pattern is useful for dashboarding many A/B results together or for pooling information when sample sizes vary across tests.</p>
<h3 id="what-is-hierarchical-models">What is hierarchical models<a hidden class="anchor" aria-hidden="true" href="#what-is-hierarchical-models">#</a></h3>
<p>In a hierarchical (or multilevel) model, you assume that the parameters for each group are related and drawn from a common, overarching distribution. This shared distribution is governed by hyper-parameters.</p>
<p>All group-level parameters are drawn from a single, shared distribution defined by hyper-parameters a and b. For example each test&rsquo;s probability is drawn from a shared Beta(a,b), where a and b are themselves parameters to be estimated.</p>
<p>The estimate for any single group is influenced both by its own data and the data from all other groups (via the shared a and b). This is called partial pooling, Say a and b are estimated to best fit all test data, and then every test&rsquo;s probability is pulled slightly toward the overall average defined by a and b.</p>
<p>Estimates for small groups are pulled toward the average (a process called shrinkage), leading to more stable, less extreme estimates</p>
<h3 id="which-distribution-may-be-used">Which distribution may be used?<a hidden class="anchor" aria-hidden="true" href="#which-distribution-may-be-used">#</a></h3>
<p>Non-Informative Prior for Beta Hyperparameters:
$$p(a,b) \propto (a+b)^{-5/2}$$</p>
<p>Simplification or approximation of the Jeffreys Prior for the hyper-parameters of the Beta distribution, which is used as a conjugate prior for binomial or Bernoulli likelihoods (common in multiple testing models, e.g., estimating the probability of a true null hypothesis)</p>
<p>When $a$ and $b$ are the shape parameters of Beta distribution, the actual Jeffreys Prior is defined by the Fisher Information matrix:</p>
<p>$$p(a,b) \propto \sqrt{\det(\mathbf{I}(a,b))}$$</p>
<p>The determinant of the Fisher Information matrix for the Beta distribution&rsquo;s parameters is a complex function involving the trigamma function ($\psi&rsquo;$). Specifically, a known simplification used in some computational Bayesian contexts is related to the mean and total effective count of the Beta distribution.</p>
<p>The term $\tau = a+b$ is often interpreted as the total effective sample size (or precision) of the Beta distribution. The exponent $-5/2$ is a specific value that results from one of the approximations designed to make the prior less influential on the posterior, often for the standard deviation or variance of the underlying distribution.</p>
<h3 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># --- 1. Data Definition ---</span>
</span></span><span style="display:flex;"><span>trials <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">842</span>, <span style="color:#ae81ff">854</span>, <span style="color:#ae81ff">862</span>, <span style="color:#ae81ff">821</span>, <span style="color:#ae81ff">839</span>])
</span></span><span style="display:flex;"><span>successes <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">27</span>, <span style="color:#ae81ff">47</span>, <span style="color:#ae81ff">69</span>, <span style="color:#ae81ff">52</span>, <span style="color:#ae81ff">35</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N_GROUPS <span style="color:#f92672">=</span> len(trials)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Constraint for Beta parameters (a and b must be &gt; 0)</span>
</span></span><span style="display:flex;"><span>ALPHA_MIN <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> pm<span style="color:#f92672">.</span>Model() <span style="color:#66d9ef">as</span> hierarchical_model:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --- 2. Hyper-parameter Priors (a and b) ---</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The Beta shape parameters a and b must be positive.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># We define them with a minimally informative uniform prior.</span>
</span></span><span style="display:flex;"><span>    a <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Uniform(<span style="color:#e6db74">&#34;a&#34;</span>, lower<span style="color:#f92672">=</span>ALPHA_MIN, upper<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Uniform(<span style="color:#e6db74">&#34;b&#34;</span>, lower<span style="color:#f92672">=</span>ALPHA_MIN, upper<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --- 3. Custom Precision Prior (pm.Potential) ---</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The original prior was: log p(a, b) = log((a+b)^-2.5) = -2.5 * log(a + b)</span>
</span></span><span style="display:flex;"><span>    PRIOR_EXPONENT <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2.5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Use pm.Potential to add the custom log-prior term to the model&#39;s log(P)</span>
</span></span><span style="display:flex;"><span>    log_precision_prior <span style="color:#f92672">=</span> PRIOR_EXPONENT <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(a <span style="color:#f92672">+</span> b)
</span></span><span style="display:flex;"><span>    pm<span style="color:#f92672">.</span>Potential(<span style="color:#e6db74">&#34;beta_precision_potential&#34;</span>, log_precision_prior)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --- 4. Group-level Prior (occurrences) ---</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># &#39;occurrences&#39; is the probability for each group</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># drawn from a Beta distribution defined by the hyper-parameters a and b.</span>
</span></span><span style="display:flex;"><span>    occurrences <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Beta(<span style="color:#e6db74">&#34;occurrences&#34;</span>, alpha<span style="color:#f92672">=</span>a, beta<span style="color:#f92672">=</span>b, shape<span style="color:#f92672">=</span>N_GROUPS)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --- 5. Likelihood (l_obs) ---</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The observed successes follow a Binomial distribution.</span>
</span></span><span style="display:flex;"><span>    likelihood <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Binomial(<span style="color:#e6db74">&#34;likelihood&#34;</span>, n<span style="color:#f92672">=</span>trials, p<span style="color:#f92672">=</span>occurrences, observed<span style="color:#f92672">=</span>successes)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --- 6. Sampling ---</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Sampling is now done with pm.sample()</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5000 draws, 1000 tune (burn-in)</span>
</span></span><span style="display:flex;"><span>    idata <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>sample(draws<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, tune<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, cores<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, chains<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, random_seed<span style="color:#f92672">=</span><span style="color:#ae81ff">13</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># To view the results:</span>
</span></span><span style="display:flex;"><span>print(pm<span style="color:#f92672">.</span>summary(idata))
</span></span></code></pre></div><details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> warnings
</span></span><span style="display:flex;"><span>warnings<span style="color:#f92672">.</span>filterwarnings(<span style="color:#e6db74">&#34;ignore&#34;</span>, category<span style="color:#f92672">=</span><span style="color:#a6e22e">UserWarning</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Activate Dark Mode</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;dark_background&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Use the figure-level function sns.displot</span>
</span></span><span style="display:flex;"><span>g <span style="color:#f92672">=</span> sns<span style="color:#f92672">.</span>displot(
</span></span><span style="display:flex;"><span>    idata<span style="color:#f92672">.</span>posterior<span style="color:#f92672">.</span>occurrences[<span style="color:#ae81ff">0</span>, :, :]<span style="color:#f92672">.</span>values, 
</span></span><span style="display:flex;"><span>    kind<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;kde&#39;</span>, <span style="color:#75715e"># Use KDE for a smoother distribution line</span>
</span></span><span style="display:flex;"><span>    color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#00FFFF&#39;</span>, <span style="color:#75715e"># Neon Cyan for high contrast</span>
</span></span><span style="display:flex;"><span>    fill<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, 
</span></span><span style="display:flex;"><span>    alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>,
</span></span><span style="display:flex;"><span>    height<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, 
</span></span><span style="display:flex;"><span>    aspect<span style="color:#f92672">=</span><span style="color:#ae81ff">1.5</span> <span style="color:#75715e"># Set figure size/ratio</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Apply final aesthetic touches to the single axis</span>
</span></span><span style="display:flex;"><span>ax <span style="color:#f92672">=</span> g<span style="color:#f92672">.</span>ax 
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Distribution of Occurrences&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;left&#39;</span>, fontweight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bold&#39;</span>, pad<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;Occurrences Count&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Density&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Ensure grid lines are subtle</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Remove top and right spines</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div></details>
<img src="BayesianAB_files/figure-markdown_strict/fig-bayesian-plot-2-output-1.png" id="fig-bayesian-plot-2" alt="Figure¬†2: Distribution of occurrences for multiple hypotheses" />
<p style="text-align: center;">
Figure 2. Posterior probabilities in Hierarchical Model
</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;dark_background&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>diff_1_vs_4 <span style="color:#f92672">=</span> (idata<span style="color:#f92672">.</span>posterior<span style="color:#f92672">.</span>occurrences[:, :, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> idata<span style="color:#f92672">.</span>posterior<span style="color:#f92672">.</span>occurrences[:, :, <span style="color:#ae81ff">4</span>])<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>ravel()
</span></span><span style="display:flex;"><span>prob_v1_gt_v4 <span style="color:#f92672">=</span> (diff_1_vs_4 <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Setup Figure and Axis</span>
</span></span><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define color (Neon Magenta)</span>
</span></span><span style="display:flex;"><span>neon_color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#FF00FF&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Create the KDE plot</span>
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(
</span></span><span style="display:flex;"><span>    diff_1_vs_4, 
</span></span><span style="display:flex;"><span>    ax<span style="color:#f92672">=</span>ax,
</span></span><span style="display:flex;"><span>    fill<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, 
</span></span><span style="display:flex;"><span>    color<span style="color:#f92672">=</span>neon_color, 
</span></span><span style="display:flex;"><span>    alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>,
</span></span><span style="display:flex;"><span>    linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The label includes the P(V1 &gt; V4) calculation</span>
</span></span><span style="display:flex;"><span>    label<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;P(V1 &gt; V4)&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Add Zero Line (Crucial for interpretation)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This white dashed line marks the threshold for the probability calculation</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>axvline(<span style="color:#ae81ff">0</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1.5</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;V1 = V4 (Zero Difference)&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Apply final aesthetic touches</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_title(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Posterior Distribution of Difference: V1 vs. V4&#34;</span>, 
</span></span><span style="display:flex;"><span>    fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, 
</span></span><span style="display:flex;"><span>    loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;left&#39;</span>, 
</span></span><span style="display:flex;"><span>    fontweight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bold&#39;</span>, 
</span></span><span style="display:flex;"><span>    pad<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;V1 - V4 (Difference)&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Density&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Clean up</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper left&#39;</span>, frameon<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_visible(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div></details>
<img src="BayesianAB_files/figure-markdown_strict/fig-bayesian-plot-3-output-1.png" id="fig-bayesian-plot-3" alt="Figure¬†3: Posterior distribution of difference" />
<p style="text-align: center;">
Figure 3. Probability of superiority: 88.7%
</p>
<h2 id="-part-3-the-need-for-speed-analytic-bayesian-solutions">‚ö° Part 3: The Need for Speed (Analytic Bayesian Solutions)<a hidden class="anchor" aria-hidden="true" href="#-part-3-the-need-for-speed-analytic-bayesian-solutions">#</a></h2>
<p>MCMC is great, but sometimes you need speed. For the math nerds among us, the Beta-Binomial conjugacy is pure magic. ‚ú®</p>
<p>If you have a Beta prior and Binomial data, the posterior is &hellip; drumroll &hellip; just another Beta distribution! No complex sampling required - just simple arithmetic. Say there is $Beta(\alpha, \beta)$ prior and $k$ successes in $n$ trials, the posterior is $Beta(\alpha + k, \beta + n - k)$. This gives us closed-form solutions for the Probability of Superiority (PoS) instantly.</p>
<p>Say we have 10 heads from ten coin flips, what is the probability to get get a head in the next flip?</p>
<p>Using Bayesian approach, where</p>
<p>$ $ - hypothesis, $\mathcal{D}$ - data</p>
<p>$P(\mathcal{H}) - prior$</p>
<p>$P( | ) - likelihood $</p>
<p>$P( | ) - posterior $</p>
<p>$$ P(\mathcal{H} | \mathcal{D}) \propto P(\mathcal{D} | \mathcal{H}) P(\mathcal{H}) $$</p>
<p>It can be shown that if</p>
<p>$$ P(\mathcal{H}) = {Beta}(p; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha-1}(1-p)^{\beta-1} $$</p>
<p>$$ P(\mathcal{D} | \mathcal{H}) = {Binom}(p; k, n) = C_{n}^{k} p^{k} (1-p)^{n-k} $$</p>
<p>Then</p>
<p>Proof
$$ P(\mathcal{H} | \mathcal{D}) = \frac{P(\mathcal{D} | \mathcal{H}) P(\mathcal{H})}{P(\mathcal{D})} $$</p>
<p>$$ P(\mathcal{D} | \mathcal{H}) P(\mathcal{H}) = \biggl ( C_{n}^{k} \cdot p^{k} (1-p)^{n-k} \biggr ) \cdot \biggl ( \mathbf{\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}} \cdot p^{\alpha-1} (1-p)^{\beta-1} \biggr ) = C \cdot p^{\alpha+k-1}(1-p)^{\beta+n-k-1} $$</p>
<p>$$ P(\mathcal{D}) = \int_{0}^{1} P(\mathcal{D} | \mathcal{H}) P(\mathcal{H}) dp = C \cdot \int_{0}^{1} p^{\alpha+k-1} (1-p)^{\beta+n-k-1} dp = C \cdot B(\alpha+k, \beta+n-k)$$</p>
<p>Consequently combining those two equations, The Binomial Constant and the Prior Beta Constant are completely canceled out</p>
<p>$$ P(\mathcal{H} | \mathcal{D}) = \frac{1}{B(\alpha+k, \beta+n-k)} p^{\alpha+k-1}(1-p)^{\beta+n-k-1} = {Beta}(p; \alpha + k, \beta + n - k) $$</p>
<p>End of Proof</p>
<p>In case of non-informative prior $Beta(p; 1, 1) $ what basically given Uniform distribution of the prior - Beta function may be presented with a short binomial coefficient formula</p>
<p>$$ B(k+1, n-k+1) = \frac{–ì(k+1)–ì(n-k+1)}{–ì(n+2)} = \frac{k!(n-k)!}{(n+1)!} = \frac{1}{(n+1); C_{n}^{k}} $$</p>
<p>In our case the posterior distribution is $ {Beta}(p; k + 1, n - k + 1) $</p>
<p>Hence we can build the predictive interval using this Beta distribution moments:</p>
<p>$$ \mu = \frac{\alpha}{\alpha + \beta} = \frac{k+1}{n+2} $$</p>
<p>This formula for $ $ is also used as the Laplace sequence rule, which requires adding one positive and one negative observation to estimate the posterior probability distribution for a random sample.</p>
<p>Adding second moment</p>
<p>$$ \sigma^2 = {\frac {\alpha \beta }{(\alpha +\beta )^{2}(\alpha +\beta +1)}} = \frac{(k+1)(n-k+1)}{(n+2)^2(n+3)} $$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mu <span style="color:#f92672">=</span> <span style="color:#ae81ff">11</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>sigma <span style="color:#f92672">=</span> (<span style="color:#ae81ff">11</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">12</span> <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">13</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Hence 2 sigma predictive interval for 10/10 successful flips is </span><span style="color:#e6db74">{</span>mu<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ¬± </span><span style="color:#e6db74">{</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> sigma <span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Alternatively as a pair of bounds: </span><span style="color:#e6db74">{</span>mu <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> sigma<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> - </span><span style="color:#e6db74">{</span>min(<span style="color:#ae81ff">1</span>, mu <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> sigma)<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>Hence 2 sigma predictive interval for 10/10 successful flips is 91.67% ¬± 15.33%
Alternatively as a pair of bounds: 76.34% - 100.00%
</code></pre>
<p>But it&rsquo;s not a Normal distribution, so it&rsquo;s not 95% confidence interval, we need to take Beta distribution quantile instead or calculate it precisely. It resembles the approximation that we get using normal distribution quantiles.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>l, r <span style="color:#f92672">=</span> sts<span style="color:#f92672">.</span>beta<span style="color:#f92672">.</span>ppf([<span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">1.0</span>], a<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>, b<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Beta predictive interval </span><span style="color:#e6db74">{</span>l<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> - </span><span style="color:#e6db74">{</span>r<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>Beta predictive interval 76.16% - 100.00%
</code></pre>
<p>Another way is get that number analytically from the integral equation that fully coincides with the value from stats package.</p>
<p>$$ \int_{p_{crit}}^{1} (n+1) \cdot C_n^n \cdot p^n (1-p)^0 dp = 0.95 $$</p>
<p>$$ p_{crit} = \sqrt[n + 1]{0.05}$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;What makes it an easy computation: p is from </span><span style="color:#e6db74">{</span><span style="color:#ae81ff">0.05</span> <span style="color:#f92672">**</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">11</span>)<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> - 100.00%&#34;</span>)
</span></span></code></pre></div><pre><code>What makes it an easy computation: p is from 76.16% - 100.00%
</code></pre>
<p>Criterion that is used for analytical model decision making in A/B experiment</p>
<p>$$ P(\lambda_B &gt; \lambda_A) = \int_{p_B &gt; p_A} P(p_A, p_B | \text{Data}) , dp_A , dp_B = \sum_{i=0}^{\alpha_B-1} \frac{B(\alpha_A+i, \beta_A+\beta_B)}{(\beta_B+i)B(1+i, \beta_B)B(\alpha_A, \beta_A)} $$</p>
<p>The formula is the result of applying a well-known mathematical identity that allows the cumulative probability of one Beta variable being less than another Beta variable to be expressed as a finite sum of terms involving the Beta function, rather than requiring complex numerical integration. This is why this formula is computationally efficient and preferred for exact Bayesian A/B calculations.</p>
<h3 id="rule-of-three-when-no-successes-are-observed-">Rule of three: when no successes are observed üí°<a hidden class="anchor" aria-hidden="true" href="#rule-of-three-when-no-successes-are-observed-">#</a></h3>
<p>The rule of three is used to provide a simple way of stating an approximate 95% confidence interval in the special case that no successes have been observed - $(0, 3/n)$, alternatively by symmetry, in case of only successes $(1 - 3/n, 1) $.</p>
<p>On the other hand mathematically, if all conversions are zero, then we simply may build an equation for upper bound</p>
<p>$(1-p)^n \geq \alpha$, where $\alpha = .05$ and hence $n \leq log_{.95}(.05)$</p>
<p>For example - how many trials needed to challenge the null hypothesis that the success probability is zero?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Approximate N:&#34;</span>, int(<span style="color:#ae81ff">3</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">0.05</span>))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Exact N:&#34;</span>, <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> int((np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">0.05</span>) <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">0.95</span>))))
</span></span></code></pre></div><pre><code>Approximate N: 60
Exact N: 59
</code></pre>
<p><strong>Quick Tip for Zero Successes</strong>: If you launch a test and get zero conversions, don&rsquo;t panic. Use the Rule of Three: your approximate 95% upper bound is simply 3/n.¬†It&rsquo;s a &ldquo;pretty decent&rdquo; (and fast) estimate without needing a calculator.</p>
<h2 id="-part-4-the-plot-twist-peeking">üò± Part 4: The Plot Twist (Peeking)<a hidden class="anchor" aria-hidden="true" href="#-part-4-the-plot-twist-peeking">#</a></h2>
<p>Here is the controversial bit: Bayesian A/B testing is NOT immune to peeking. The Myth: &ldquo;I can check my Bayesian results whenever I want, and it&rsquo;s always valid!&rdquo; The Reality: Mathematically, the posterior is valid. BUT, if you use a fixed rule like &ldquo;Stop when Probability &gt; 95%,&rdquo; you will inflate your False Positive Rate over time. You are essentially fishing for significance. If you stop early just because you crossed a line, you are falling into the same trap as frequentists, but but first things first.</p>
<p>To compare stopping rules and power, the notebook includes a Monte Carlo harness. It simulates repeated experiments, applies frequentist sequential z-tests and several Bayesian stopping rules (naive PoS threshold, expected-loss stopping (OLF), and HDI &amp; PoS combinations), and compares false positive rates and average stopping sample sizes.</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Callable
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">min_sample_size</span>(mde, mu, sigma, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>, power<span style="color:#f92672">=</span><span style="color:#ae81ff">0.80</span>) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Defines superiority one-side z-test sample size
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        mde: Relative uplift
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        mu: Expected Value
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        sigma: Square root of variance
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        alpha: False Positive Rate, default = 0.05
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        power: Experiment power, default = 0.80
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Required sample size to achieve the power
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    effect_size <span style="color:#f92672">=</span> abs(mde) <span style="color:#f92672">*</span> mu <span style="color:#f92672">/</span> sigma
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> int(((sts<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>ppf(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> alpha) <span style="color:#f92672">+</span> sts<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>ppf(power)) <span style="color:#f92672">/</span> effect_size) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stops_at</span>(is_significant: np<span style="color:#f92672">.</span>ndarray, sample_size: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Determines the stopping sample size.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    This function identifies the first instance where the input
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    condition is True and returns the corresponding sample size.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        is_significant: A boolean array of the stop condition for each size
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        sample_size: An array of sample sizes.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        The stopping sample size.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Example:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &gt;&gt;&gt; stops_at([False, False, True, True], [50, 100, 150, 200])
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        150
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> len(is_significant) <span style="color:#f92672">!=</span> len(sample_size):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>(<span style="color:#e6db74">&#34;Input arrays must have the same length.&#34;</span>)
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(is_significant)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>nan <span style="color:#66d9ef">if</span> len(w) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> sample_size[w[<span style="color:#ae81ff">0</span>]]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">monte_carlo</span>(
</span></span><span style="display:flex;"><span>    bayesian_stop_rule,
</span></span><span style="display:flex;"><span>    effect_size: float<span style="color:#f92672">=</span><span style="color:#ae81ff">0.10</span>,
</span></span><span style="display:flex;"><span>    aa_test: bool<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    alpha: float<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>,
</span></span><span style="display:flex;"><span>    peeks: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;Frequentist&#39;</span>: [],
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;Bayesian&#39;</span>: [],
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    p <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.20</span>
</span></span><span style="display:flex;"><span>    sigma <span style="color:#f92672">=</span> (p <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> p)) <span style="color:#f92672">**</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>    relative_effect <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">if</span> aa_test <span style="color:#66d9ef">else</span> effect_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    N <span style="color:#f92672">=</span> min_sample_size(mde<span style="color:#f92672">=</span>effect_size, mu<span style="color:#f92672">=</span>p, sigma<span style="color:#f92672">=</span>sigma, alpha<span style="color:#f92672">=</span>alpha)
</span></span><span style="display:flex;"><span>    n <span style="color:#f92672">=</span> int(N <span style="color:#f92672">/</span> peeks)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Running </span><span style="color:#e6db74">{</span>n_iterations<span style="color:#e6db74">}</span><span style="color:#e6db74"> simulations with total sample size </span><span style="color:#e6db74">{</span>N<span style="color:#e6db74">}</span><span style="color:#e6db74"> that is achieved in </span><span style="color:#e6db74">{</span>peeks<span style="color:#e6db74">}</span><span style="color:#e6db74"> iterations of </span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74"> size each&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> seed <span style="color:#f92672">in</span> tqdm(range(n_iterations)):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        rng <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>default_rng(seed)
</span></span><span style="display:flex;"><span>        binomial_samples <span style="color:#f92672">=</span> rng<span style="color:#f92672">.</span>binomial(n<span style="color:#f92672">=</span>n, p<span style="color:#f92672">=</span>p<span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>relative_effect), size<span style="color:#f92672">=</span>peeks)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sizes <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(n, N <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, n)
</span></span><span style="display:flex;"><span>        conversions <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cumsum(binomial_samples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        z_scores <span style="color:#f92672">=</span> [(success <span style="color:#f92672">/</span> trials <span style="color:#f92672">-</span> p) <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(sigma <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> trials) <span style="color:#66d9ef">for</span> success, trials <span style="color:#f92672">in</span> zip(conversions, sizes)]
</span></span><span style="display:flex;"><span>        is_prob_high_enough <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>            bayesian_stop_rule(
</span></span><span style="display:flex;"><span>                success<span style="color:#f92672">=</span>success,
</span></span><span style="display:flex;"><span>                trials<span style="color:#f92672">=</span>trials,
</span></span><span style="display:flex;"><span>                alpha<span style="color:#f92672">=</span>alpha,
</span></span><span style="display:flex;"><span>                p<span style="color:#f92672">=</span>p,
</span></span><span style="display:flex;"><span>                effect_size<span style="color:#f92672">=</span>effect_size
</span></span><span style="display:flex;"><span>            ) <span style="color:#66d9ef">for</span> success, trials <span style="color:#f92672">in</span> zip(conversions, sizes)
</span></span><span style="display:flex;"><span>        ]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        result[<span style="color:#e6db74">&#39;Frequentist&#39;</span>]<span style="color:#f92672">.</span>append(stops_at(z_scores <span style="color:#f92672">&gt;</span> sts<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>ppf(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> alpha), sizes))
</span></span><span style="display:flex;"><span>        result[<span style="color:#e6db74">&#39;Bayesian&#39;</span>]<span style="color:#f92672">.</span>append(stops_at(is_prob_high_enough, sizes))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>join([
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Frequentist Rejected Rate: </span><span style="color:#e6db74">{</span>np<span style="color:#f92672">.</span>mean(<span style="color:#f92672">~</span>np<span style="color:#f92672">.</span>isnan(result[<span style="color:#e6db74">&#39;Frequentist&#39;</span>]))<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Frequentist Required Sample Size: </span><span style="color:#e6db74">{</span>int(np<span style="color:#f92672">.</span>nanmean(result[<span style="color:#e6db74">&#39;Frequentist&#39;</span>]))<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Bayesian Rejected Rate: </span><span style="color:#e6db74">{</span>np<span style="color:#f92672">.</span>mean(<span style="color:#f92672">~</span>np<span style="color:#f92672">.</span>isnan(result[<span style="color:#e6db74">&#39;Bayesian&#39;</span>]))<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Bayesian Required Sample Size: </span><span style="color:#e6db74">{</span>int(np<span style="color:#f92672">.</span>nanmean(result[<span style="color:#e6db74">&#39;Bayesian&#39;</span>]))<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>    ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">POS</span>(success: int, trials: int, alpha: float, p: float, <span style="color:#f92672">**</span>kwargs) <span style="color:#f92672">-&gt;</span> bool:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34; Probability of Superiority decision rule &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> sts<span style="color:#f92672">.</span>beta<span style="color:#f92672">.</span>cdf(p, a <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> success, b <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> trials <span style="color:#f92672">-</span> success) <span style="color:#f92672">&lt;</span> alpha
</span></span></code></pre></div></details>
<h3 id="correctness">Correctness<a hidden class="anchor" aria-hidden="true" href="#correctness">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>monte_carlo(bayesian_stop_rule<span style="color:#f92672">=</span>POS, peeks<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, aa_test<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>Running 1000 simulations with total sample size 2473 that is achieved in 1 iterations of 2473 size each
Frequentist Rejected Rate: 0.06
Frequentist Required Sample Size: 2473
Bayesian Rejected Rate: 0.06
Bayesian Required Sample Size: 2473
</code></pre>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"44b0480eba1e40c9b338530f3f80ec4a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>monte_carlo(bayesian_stop_rule<span style="color:#f92672">=</span>POS, peeks<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, aa_test<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each
Frequentist Rejected Rate: 0.126
Frequentist Required Sample Size: 1015
Bayesian Rejected Rate: 0.126
Bayesian Required Sample Size: 1015
</code></pre>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b64b5f4cc2af46028d5703ada98c4528","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
<h3 id="ab-design-power">A/B design power<a hidden class="anchor" aria-hidden="true" href="#ab-design-power">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>monte_carlo(bayesian_stop_rule<span style="color:#f92672">=</span>POS, peeks<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, aa_test<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><pre><code>Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each
Frequentist Rejected Rate: 0.855
Frequentist Required Sample Size: 1146
Bayesian Rejected Rate: 0.855
Bayesian Required Sample Size: 1146
</code></pre>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4648418bceed45a5b8d3434c04e36104","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
<p>That is a crucial observation and it points to a common misunderstanding about Bayesian A/B testing:</p>
<p>No, the standard Bayesian approach does not handle peeking (optional stopping) correctly by default if your goal is to control the frequentist Type I Error Rate (False Positive Rate).</p>
<p>While the Bayesian interpretation of results remains valid at any time, using a fixed threshold (e.g., stopping when $P(A &gt; B) &gt; 95%$) and checking repeatedly will lead to an inflated False Positive Rate over many hypothetical experiments, just like in the frequentist approach.</p>
<p>What is NOT Affected (The Bayesian Advantage) The Posterior Distribution and the Probability of Superiority &mdash; is always valid, regardless of when you look at the data.</p>
<p>What IS Affected (The Peeking Problem) The problem arises when you use a fixed decision rule (like the $95%$ threshold) to stop the test prematurely, based on the outcome.</p>
<p>The Myth: The common claim that &ldquo;Bayesian testing is immune to peeking&rdquo; is overstated. It is only immune in the sense that the posterior is always mathematically correct. It is not immune in the sense that it prevents the inflation of the frequentist Type I Error Rate when using a simple, fixed stopping threshold</p>
<h2 id="-part-5-how-to-peek-safely">üõ°Ô∏è Part 5: How to Peek Safely<a hidden class="anchor" aria-hidden="true" href="#-part-5-how-to-peek-safely">#</a></h2>
<h3 id="how-bayesian-methods-truly-handle-peeking">How Bayesian Methods Truly Handle Peeking<a hidden class="anchor" aria-hidden="true" href="#how-bayesian-methods-truly-handle-peeking">#</a></h3>
<p>To safely peek and stop early in a Bayesian framework, you need to base your decision on a metric that incorporates the cost of a wrong decision, not just the probability of a difference.</p>
<p>The correct Bayesian decision procedure is to stop when:</p>
<ul>
<li>Expected Loss (EL) is Minimized: You stop the test when the Expected Loss of choosing the suboptimal variant falls below a commercially acceptable threshold $\epsilon$. This naturally accounts for uncertainty.13 If the posterior distributions are still wide (high uncertainty), the loss will be high, and you won&rsquo;t stop.</li>
</ul>
<p>$$ E[L](p_a &gt; p_b) = \int_0^1\int_0^1L(p_a,p_b, p_a &gt; p_b)P(p_a|a,b,n_a,k_a)P(p_b|a,b,n_b,k_b)dp_adp_b $$</p>
<ul>
<li>Sequential Designs (Like Multi-Armed Bandits): Techniques like Thompson Sampling are inherently Bayesian and sequential. They don&rsquo;t have a stopping rule based on error rates; they simply choose the best variant to show next based on the posterior, which naturally directs more traffic to the likely winner, making the experiment efficient without needing a fixed sample size.</li>
</ul>
<h3 id="example-of-loss-function-applied">Example of Loss function applied<a hidden class="anchor" aria-hidden="true" href="#example-of-loss-function-applied">#</a></h3>
<p>Define the Opportunity Loss Function $L(p_A, p_B)$, which is the regret you incur by choosing a variant that is not the best.If we Choose B, the loss only happens if $p_A &gt; p_B$: $$L(\text{Choose B}) = \max(0, p_A - p_B)$$ If we Choose A, the loss only happens if $p_B &gt; p_A$: $$L(\text{Choose A}) = \max(0, p_B - p_A)$$</p>
<p>Using known properties and identities related to the Beta function, this complex double integral for Opportunity Loss function can be transformed into a closed-form summation:</p>
<p>$$ EL_A = \sum_{i=0}^{\alpha_B-1} \frac{\alpha_A \cdot B(\alpha_A+i+1, \beta_A+\beta_B)}{\beta_A \cdot B(i+1, \beta_B) \cdot B(\alpha_A, \beta_A)} - \sum_{i=0}^{\alpha_B-1} \frac{\alpha_B \cdot B(\alpha_A+i, \beta_A+\beta_B+1)}{(\beta_B+i) \cdot B(i+1, \beta_B) \cdot B(\alpha_A, \beta_A)} $$</p>
<p>The Formula for one-sample test is against benchmark $\lambda_0$:</p>
<p>$$ EL_A = \lambda_0 \cdot I_{\lambda_0}(\alpha, \beta) - \frac{\alpha}{\alpha+\beta} \cdot I_{\lambda_0}(\alpha+1, \beta) $$</p>
<p>Python Implementation note that <code>betainc</code> $(\alpha, \beta, \lambda)$ calculates $I_{\lambda}(\alpha, \beta)$, that is an equivalent of <code>sts.beta.cdf</code>$(\lambda, \alpha, \beta)$</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.special <span style="color:#f92672">import</span> betainc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_opportunity_loss_one_sample</span>(
</span></span><span style="display:flex;"><span>    k: int,
</span></span><span style="display:flex;"><span>    n: int,
</span></span><span style="display:flex;"><span>    lambda_0: float,
</span></span><span style="display:flex;"><span>    prior_alpha: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    prior_beta: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>) <span style="color:#f92672">-&gt;</span> float:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Calculates the Expected Loss of choosing the observed variant against
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    a benchmark using the analytical formula. Absolute value of conversion loss.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Non-informative conjugate prior is used by Default.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        k (int): Observed successes (conversions).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        n (int): Observed trials (sizes).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        lambda_0 (float): The fixed conversion rate benchmark.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        prior_alpha (int): Prior alpha hyper-parameter.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        prior_beta (int): Prior beta hyper-parameter.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        float: The Expected Loss of choosing the observed variant.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculate Posterior Parameters (alpha and beta)</span>
</span></span><span style="display:flex;"><span>    alpha <span style="color:#f92672">=</span> k <span style="color:#f92672">+</span> prior_alpha
</span></span><span style="display:flex;"><span>    beta <span style="color:#f92672">=</span> n <span style="color:#f92672">-</span> k <span style="color:#f92672">+</span> prior_beta
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --- Term 1: Probability of Loss ---</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculates I_lambda_bench(alpha, beta) = P(lambda_obs &lt; lambda_bench)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The term is: lambda_bench * P(lambda_obs &lt; lambda_bench)</span>
</span></span><span style="display:flex;"><span>    term1 <span style="color:#f92672">=</span> lambda_0 <span style="color:#f92672">*</span> betainc(alpha, beta, lambda_0)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># --- Term 2: Weighted Expected Value ---</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The fraction part: alpha / (alpha + beta) is the mean of Beta(alpha, beta)</span>
</span></span><span style="display:flex;"><span>    term2 <span style="color:#f92672">=</span> alpha <span style="color:#f92672">/</span> (alpha <span style="color:#f92672">+</span> beta) <span style="color:#f92672">*</span> betainc(alpha <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, beta, lambda_0)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> term1 <span style="color:#f92672">-</span> term2
</span></span></code></pre></div></details>
<p>Frequentist: We must collect $N$ samples to have an $1-\beta$ chance of detecting a $MDE$ difference with $\alpha$ error.</p>
<p>Bayesian: We will stop the test when the average potential loss incurred by choosing the sub-optimal variant is less than $\epsilon$ percentage points. By setting a small $\epsilon$, you ensure that the test continues until the potential future regret (loss) is extremely low, thus ensuring a high degree of confidence in the final decision while retaining the ability to peek safely</p>
<p>Adjust Monte Carlo procedure with another Bayesian stopping rule</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">OLF</span>(success: int, trials: int, p: float, effect_size: float, <span style="color:#f92672">**</span>kwargs) <span style="color:#f92672">-&gt;</span> bool:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Opportunity Loss Function stopping rule. Epsilon is usually set to a fraction of MDE&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    fraction <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>    epsilon <span style="color:#f92672">=</span> fraction <span style="color:#f92672">*</span> effect_size <span style="color:#f92672">*</span> p
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> calculate_opportunity_loss_one_sample(success, trials, lambda_0<span style="color:#f92672">=</span>p) <span style="color:#f92672">&lt;</span> epsilon
</span></span></code></pre></div></details>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>monte_carlo(bayesian_stop_rule<span style="color:#f92672">=</span>OLF, peeks<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, aa_test<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>Running 1000 simulations with total sample size 2473 that is achieved in 30 iterations of 82 size each
Frequentist Rejected Rate: 0.249
Frequentist Required Sample Size: 590
Bayesian Rejected Rate: 0.21
Bayesian Required Sample Size: 839
</code></pre>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c8ce647a050042cb9f76ffdb377c8e33","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
<p>So, Loss Function doesn&rsquo;t save from Peeking problem, it&rsquo;s even more vulnerable than well-known p-value approach</p>
<p>We need another piece of the puzzle &hellip;</p>
<h3 id="here-comes-the-sun---and-hdi">Here comes the sun üå§Ô∏è &hellip; and HDI<a hidden class="anchor" aria-hidden="true" href="#here-comes-the-sun---and-hdi">#</a></h3>
<p>Highest Density Interval (sometimes called Highest Posterior Density Interval).</p>
<p>Definition: The $X%$ HDI is the narrowest interval that contains $X%$ of the probability mass of the posterior distribution.</p>
<p>Purpose: It is the Bayesian equivalent of the frequentist Confidence Interval (CI), but unlike the CI, you can state that there is an $X%$ probability that the true parameter value (e.g., the true conversion rate) lies within the HDI.</p>
<p>The width of the HDI is simply (Upper Bound - Lower Bound). It is a direct and intuitive measure of the remaining uncertainty. A wide HDI means your posterior is flat and uncertain; a narrow HDI means your posterior is sharply peaked and confident.</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> arviz <span style="color:#66d9ef">as</span> az
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_beta_hdi_width</span>(alpha: float, beta: float, hdi_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">10_000</span>) <span style="color:#f92672">-&gt;</span> float:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Calculates the Highest Density Interval (HDI) for a Beta distribution
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    using Monte Carlo sampling and the arviz library.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The calculation of the HDI is an iterative process that must find
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    the interval boundary points where the probability density is equal,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    while the area between the points equals the target probability.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Since Beta distribution is generally not symmetrical, the HDI bounds are not
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    the same as the quantiles, which is why a specialized function is needed.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        alpha (float): The posterior alpha parameter (k_obs + prior_alpha).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        beta (float): The posterior beta parameter (n_obs - k_obs + prior_beta).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        hdi_prob (float): The target probability mass (e.g., 0.95 for 95% HDI).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_samples (int): Number of samples to draw for Monte Carlo calculation.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        tuple: (lower_bound, upper_bound, width)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    posterior_samples <span style="color:#f92672">=</span> sts<span style="color:#f92672">.</span>beta<span style="color:#f92672">.</span>rvs(a<span style="color:#f92672">=</span>alpha, b<span style="color:#f92672">=</span>beta, size<span style="color:#f92672">=</span>num_samples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># designed to work on posterior samples</span>
</span></span><span style="display:flex;"><span>    hdi_interval <span style="color:#f92672">=</span> az<span style="color:#f92672">.</span>hdi(posterior_samples, hdi_prob<span style="color:#f92672">=</span>hdi_prob)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    lower_bound <span style="color:#f92672">=</span> hdi_interval[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    upper_bound <span style="color:#f92672">=</span> hdi_interval[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> upper_bound <span style="color:#f92672">-</span> lower_bound
</span></span></code></pre></div></details>
<p>Let&rsquo;s update Monte Carlo once again with a combination of PoS and HDI stopping what represent business and statistical robustness respectively, shall we? - Note that HDI density affects the inference vastly and you&rsquo;d better experiment to pick up a good for for the data.</p>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Width is multiplied by </span><span style="color:#e6db74">{</span>round(calculate_beta_hdi_width(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">.99</span>) <span style="color:#f92672">/</span> calculate_beta_hdi_width(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">0.8</span>), <span style="color:#ae81ff">2</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74"> when increase required density from 0.8 to 0.99&#34;</span>)
</span></span></code></pre></div></details>
<pre><code>Width is multiplied by 1.98 when increase required density from 0.8 to 0.99
</code></pre>
<details>
<summary>Code</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">HDI</span>(success: int, trials: int, alpha: float, p: float, effect_size: float) <span style="color:#f92672">-&gt;</span> bool:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34; PoS combined with 95% HDI stopping rule &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># checks if 95% of posterior distribution is narrow enough and lays in ¬± MDE</span>
</span></span><span style="display:flex;"><span>        (calculate_beta_hdi_width(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> success, <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> trials <span style="color:#f92672">-</span> success) <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> effect_size <span style="color:#f92672">*</span> p)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># checks if posterior distribution by 95% chance better</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">and</span> POS(success, trials, alpha, p)
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div></details>
<p>Correctness</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>monte_carlo(bayesian_stop_rule<span style="color:#f92672">=</span>OLF, peeks<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, aa_test<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>Running 1000 simulations with total sample size 2473 that is achieved in 1 iterations of 2473 size each
Frequentist Rejected Rate: 0.06
Frequentist Required Sample Size: 2473
Bayesian Rejected Rate: 0.069
Bayesian Required Sample Size: 2473
</code></pre>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bb0d04680b55411c97e9aa6e2e52ca2d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>monte_carlo(bayesian_stop_rule<span style="color:#f92672">=</span>HDI, peeks<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, aa_test<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>Running 1000 simulations with total sample size 2473 that is achieved in 10 iterations of 247 size each
Frequentist Rejected Rate: 0.193
Frequentist Required Sample Size: 876
Bayesian Rejected Rate: 0.1
Bayesian Required Sample Size: 1924
</code></pre>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9f64a18492b04c9a89a896b63ecf0b6a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
<p>Power</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>monte_carlo(bayesian_stop_rule<span style="color:#f92672">=</span>HDI, peeks<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, aa_test<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><pre><code>Running 1000 simulations with total sample size 2473 that is achieved in 5 iterations of 494 size each
Frequentist Rejected Rate: 0.855
Frequentist Required Sample Size: 1146
Bayesian Rejected Rate: 0.818
Bayesian Required Sample Size: 2035
</code></pre>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5dbeef9db69f4f3696d4f7f85db7b7cb","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
<p>HDI accompanied by Probability of Superiority is a good criterion, although very strict if you increase the required density for HDI from 80% above 95% and it hence requires bigger sample size than Frequentist approach.</p>
<p>Combination of HDI and PoS checks makes the criterion less sensitive to peeking, however it&rsquo;s yet not fully immune.</p>
<h3 id="stopping-rules-overview">Stopping Rules Overview<a hidden class="anchor" aria-hidden="true" href="#stopping-rules-overview">#</a></h3>
<p>This table breaks down three common criteria used in Bayesian A/B testing, highlighting their function and their <strong>robustness against peeking</strong> (stopping a test too early based on transient results).</p>
<table>
<thead>
<tr>
<th style="text-align:left">Criterion</th>
<th style="text-align:left">Function / Definition</th>
<th style="text-align:left">Robustness Against Peeking</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Probability of Superiority (PoS)</strong></td>
<td style="text-align:left">Measures how often the posterior probability of $\lambda_B$ is greater than $\lambda_A$ (i.e., $P(\lambda_B &gt; \lambda_A)$).</td>
<td style="text-align:left"><strong>Low Robustness.</strong> The threshold can be quickly and spuriously crossed by early noise or transient fluctuations.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Expected Loss (EL)</strong></td>
<td style="text-align:left">Measures the average <strong>Cost or Regret</strong> of selecting the inferior variant.</td>
<td style="text-align:left"><strong>High Robustness.</strong> Requires the posterior distribution to be <strong>tight enough</strong> that the potential loss (regret) is small, preventing premature stopping.</td>
</tr>
<tr>
<td style="text-align:left"><strong>HDI Width</strong></td>
<td style="text-align:left">Measures the <strong>Precision</strong> or <strong>Uncertainty</strong> of the posterior distribution (e.g., the width of the 95% credible interval).</td>
<td style="text-align:left"><strong>High Robustness.</strong> Forces the test to continue until the uncertainty is <strong>low</strong> (the HDI is narrow), regardless of the posterior mean, ensuring adequate data collection.</td>
</tr>
</tbody>
</table>
<h2 id="-conclusions--practical-recommendations">üìù Conclusions &amp; Practical Recommendations<a hidden class="anchor" aria-hidden="true" href="#-conclusions--practical-recommendations">#</a></h2>
<ul>
<li>Use the posterior (and PoS) to <em>interpret</em> results, but prefer decision-theoretic stopping when making business choices: stop when the expected loss of choosing the sub-optimal variant is below a tolerated threshold.</li>
<li>. Combine HDI (precision) with PoS (direction) for a conservative, safe stopping rule - but higher density HDI thresholds require larger samples.</li>
<li>When many variants or small groups are present, hierarchical models provide safer estimates via partial pooling.</li>
<li>If your goal is to guarantee frequentist properties (e.g., Type I control under peeking), design the sequential procedure explicitly: group sequential testing with alpha-spending function or always valid inference approach - <a href="https://npodlozhniy.github.io/posts/sequential-testing/">Sequential Testing Guide</a> will let you know all you need</li>
</ul>
<p>Further experiments to try:</p>
<ul>
<li>Replace uniform priors with domain-informed priors when available.</li>
<li>Explore Thompson Sampling for continuous allocation instead of fixed-sample stopping.</li>
<li>Visualize posterior trajectories and stopping-rule trade-offs across simulated peeks.</li>
</ul>
<p>Thanks for reading, feel free to fork this notebook and go forth, experiment with your own traffic and loss thresholds as well as react to the post below, and may your posteriors always be narrow!</p>
<script type=application/vnd.jupyter.widget-state+json>
{"state":{"0181731c3e7e4f5886dd418d811ebd9e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0554d93f7dfc4c33b270fed6a8c731b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"05924aeff1f54bf58cafc79de6f4cd1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"07aefbc23958493cadf61a3f7d2ff449":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0962f908e5ac479d9ac335cb7fb1d3bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fbdcd143a2d8422b854acebbb0836f4d","placeholder":"‚Äã","style":"IPY_MODEL_0bacea67b70b47678da4114c670622eb","tabbable":null,"tooltip":null,"value":" 1000/1000 [00:00&lt;00:00, 1571.80it/s]"}},"097ea0be9e6540fea1f6e37723965b49":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b0780cbcf67543e4abec37637e0b9e29","placeholder":"‚Äã","style":"IPY_MODEL_e9a4b7f06616455784758576ea78ca50","tabbable":null,"tooltip":null,"value":"100%"}},"0bacea67b70b47678da4114c670622eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0e4900c9d34448cc9300f7b4916479a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_188043e6369142cfaa1b38fcd1fb521e","placeholder":"‚Äã","style":"IPY_MODEL_a98e46b8237c463e9fe8a9eacbe5fe64","tabbable":null,"tooltip":null,"value":" 1000/1000 [00:16&lt;00:00, 59.57it/s]"}},"12b7239b009d469a86a2682da9e62f10":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1661914f8d204fc0b4cef9f0f5cd39a3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1747c39aeb1c4b8385942369bbec3ad7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"188043e6369142cfaa1b38fcd1fb521e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"210c2eefb8d74134a10a25821824da79":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35e2327e3b684962a1d3fc681b715f9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_210c2eefb8d74134a10a25821824da79","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83fbbfa362a948359151ec0e17a0399d","tabbable":null,"tooltip":null,"value":1000}},"3a83457f00be47ef8fc554c7f7b8517b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c21633b81d44236a7df49cde47eca47":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3c42a4d930324bd88406ae5388ddeb0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_3a83457f00be47ef8fc554c7f7b8517b","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ab523655e834570af9b70fa6d1183dd","tabbable":null,"tooltip":null,"value":1000}},"3f1594d73f1f49e4a38ec80e9622e820":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3f306533888f4810a2576ca380f5cd51":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44b0480eba1e40c9b338530f3f80ec4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f61f94b740a54a48b61f483eb9300723","IPY_MODEL_ab9afaf12fee40c1a21c02b59f40bd41","IPY_MODEL_d1c458e4ae4644608d5096c26d093061"],"layout":"IPY_MODEL_4a6d6934fb8d4396b0acaa493989c358","tabbable":null,"tooltip":null}},"4648418bceed45a5b8d3434c04e36104":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bcc95361668d4390bb257a7b843e4ed9","IPY_MODEL_35e2327e3b684962a1d3fc681b715f9c","IPY_MODEL_da97c3b36cd74cafb0a46f1ba8966fff"],"layout":"IPY_MODEL_4d2bd273d5c340d79d6853ac4e39b63c","tabbable":null,"tooltip":null}},"49d1a84abc9b4807967d5109322735db":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a6d6934fb8d4396b0acaa493989c358":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d2bd273d5c340d79d6853ac4e39b63c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f16c40cea8b4f5890d36a17cc525cac":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5ef167be804746fdbbf7b62c01253249","placeholder":"‚Äã","style":"IPY_MODEL_a8d4a21db8c14e139850f9375e826814","tabbable":null,"tooltip":null,"value":"100%"}},"54ab74155fff46768e46e7d042c82bb1":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54deea47056043d3a45590c3857fffd4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d2b15f2353f4ab7884060d588a68800":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_12b7239b009d469a86a2682da9e62f10","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc5772f963a6403cad16a9acb93e1dce","tabbable":null,"tooltip":null,"value":1000}},"5dbeef9db69f4f3696d4f7f85db7b7cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_097ea0be9e6540fea1f6e37723965b49","IPY_MODEL_9ff46d370eef4689a6efe1e63fcccb96","IPY_MODEL_9f40db484b314e66a32b9ba0efce260b"],"layout":"IPY_MODEL_0181731c3e7e4f5886dd418d811ebd9e","tabbable":null,"tooltip":null}},"5ef167be804746fdbbf7b62c01253249":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6574bc8f963740b8929eb2a0e770ab7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_a5db07c447e24909b4b0dce01e17ab40","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f306533888f4810a2576ca380f5cd51","tabbable":null,"tooltip":null,"value":1000}},"6825e9a3aa7d44fda10d8ec64acf056d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c9ee3fe4ac904e4f838321673281b3d4","placeholder":"‚Äã","style":"IPY_MODEL_f2a2496263bb4b9dacbfabaf91f5d9a2","tabbable":null,"tooltip":null,"value":"100%"}},"6a970a6dd66d45c0927d614586daf792":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_49d1a84abc9b4807967d5109322735db","placeholder":"‚Äã","style":"IPY_MODEL_3f1594d73f1f49e4a38ec80e9622e820","tabbable":null,"tooltip":null,"value":" 1000/1000 [00:00&lt;00:00, 1555.70it/s]"}},"83fbbfa362a948359151ec0e17a0399d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ab523655e834570af9b70fa6d1183dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c4649fe4645490795bf4709c74bae66":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f899e84351a147edbd2be712c214197b","placeholder":"‚Äã","style":"IPY_MODEL_a7e642821b9546fab95641336bb47695","tabbable":null,"tooltip":null,"value":" 1000/1000 [00:00&lt;00:00, 1175.91it/s]"}},"9d98642b7d27440788e99d1915949fc8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd589fcc9ea45ca8cf36a4716ea632d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed474f75820407f9bfa7468ce7bbb04":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f40db484b314e66a32b9ba0efce260b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_baeeae25db614d0a8d8b79a07f3c3c46","placeholder":"‚Äã","style":"IPY_MODEL_07aefbc23958493cadf61a3f7d2ff449","tabbable":null,"tooltip":null,"value":" 1000/1000 [00:08&lt;00:00, 116.80it/s]"}},"9f64a18492b04c9a89a896b63ecf0b6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa391050a9914997b77bb38b97d756e5","IPY_MODEL_6574bc8f963740b8929eb2a0e770ab7a","IPY_MODEL_0e4900c9d34448cc9300f7b4916479a7"],"layout":"IPY_MODEL_9ed474f75820407f9bfa7468ce7bbb04","tabbable":null,"tooltip":null}},"9ff46d370eef4689a6efe1e63fcccb96":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_54ab74155fff46768e46e7d042c82bb1","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4a1ec09f9404fafbe02066da169514d","tabbable":null,"tooltip":null,"value":1000}},"a5db07c447e24909b4b0dce01e17ab40":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6451f818dc048aa842b021e8e94a5ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a7e642821b9546fab95641336bb47695":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a8d4a21db8c14e139850f9375e826814":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a98e46b8237c463e9fe8a9eacbe5fe64":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ab9afaf12fee40c1a21c02b59f40bd41":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_d2a993f8a6964c4a81c829cedf79b5cd","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6742189ef95428ba32b4f3c017802c7","tabbable":null,"tooltip":null,"value":1000}},"aeb31422aa1046728e09396a8bd29f91":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0780cbcf67543e4abec37637e0b9e29":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b64b5f4cc2af46028d5703ada98c4528":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6825e9a3aa7d44fda10d8ec64acf056d","IPY_MODEL_5d2b15f2353f4ab7884060d588a68800","IPY_MODEL_9c4649fe4645490795bf4709c74bae66"],"layout":"IPY_MODEL_f0adcdd3da4847b18e4bcdc20a845078","tabbable":null,"tooltip":null}},"b913916eee934b66940d06a8e48457b6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba80dad4887445dca7aa63ebca2e3974":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"baeeae25db614d0a8d8b79a07f3c3c46":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb0d04680b55411c97e9aa6e2e52ca2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e66faca9c6a34064801a112eb58cf80f","IPY_MODEL_e4fd4829079644148b0a2dd17b5efba6","IPY_MODEL_6a970a6dd66d45c0927d614586daf792"],"layout":"IPY_MODEL_54deea47056043d3a45590c3857fffd4","tabbable":null,"tooltip":null}},"bc5772f963a6403cad16a9acb93e1dce":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcc95361668d4390bb257a7b843e4ed9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1747c39aeb1c4b8385942369bbec3ad7","placeholder":"‚Äã","style":"IPY_MODEL_3c21633b81d44236a7df49cde47eca47","tabbable":null,"tooltip":null,"value":"100%"}},"bda4a8aa7b894945be5c9e33fb15a431":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4a1ec09f9404fafbe02066da169514d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8ce647a050042cb9f76ffdb377c8e33":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f16c40cea8b4f5890d36a17cc525cac","IPY_MODEL_3c42a4d930324bd88406ae5388ddeb0a","IPY_MODEL_0962f908e5ac479d9ac335cb7fb1d3bc"],"layout":"IPY_MODEL_bda4a8aa7b894945be5c9e33fb15a431","tabbable":null,"tooltip":null}},"c9ee3fe4ac904e4f838321673281b3d4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1c458e4ae4644608d5096c26d093061":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9d98642b7d27440788e99d1915949fc8","placeholder":"‚Äã","style":"IPY_MODEL_ee863cd54d4d4fabb5f616ec3e63f748","tabbable":null,"tooltip":null,"value":" 1000/1000 [00:00&lt;00:00, 1697.57it/s]"}},"d2a993f8a6964c4a81c829cedf79b5cd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d56634c72c7445c58808a1c8bd83387c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da97c3b36cd74cafb0a46f1ba8966fff":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1661914f8d204fc0b4cef9f0f5cd39a3","placeholder":"‚Äã","style":"IPY_MODEL_ba80dad4887445dca7aa63ebca2e3974","tabbable":null,"tooltip":null,"value":" 1000/1000 [00:00&lt;00:00, 984.02it/s]"}},"e4fd4829079644148b0a2dd17b5efba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_9dd589fcc9ea45ca8cf36a4716ea632d","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6a437b6eea94b6ca0205463a1447f86","tabbable":null,"tooltip":null,"value":1000}},"e66faca9c6a34064801a112eb58cf80f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b913916eee934b66940d06a8e48457b6","placeholder":"‚Äã","style":"IPY_MODEL_a6451f818dc048aa842b021e8e94a5ad","tabbable":null,"tooltip":null,"value":"100%"}},"e6742189ef95428ba32b4f3c017802c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a437b6eea94b6ca0205463a1447f86":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9a4b7f06616455784758576ea78ca50":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ee863cd54d4d4fabb5f616ec3e63f748":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f0adcdd3da4847b18e4bcdc20a845078":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2a2496263bb4b9dacbfabaf91f5d9a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f61f94b740a54a48b61f483eb9300723":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_aeb31422aa1046728e09396a8bd29f91","placeholder":"‚Äã","style":"IPY_MODEL_05924aeff1f54bf58cafc79de6f4cd1c","tabbable":null,"tooltip":null,"value":"100%"}},"f899e84351a147edbd2be712c214197b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa391050a9914997b77bb38b97d756e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d56634c72c7445c58808a1c8bd83387c","placeholder":"‚Äã","style":"IPY_MODEL_0554d93f7dfc4c33b270fed6a8c731b1","tabbable":null,"tooltip":null,"value":"100%"}},"fbdcd143a2d8422b854acebbb0836f4d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}
</script>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://npodlozhniy.github.io/tags/ab-testing/">ab-testing</a></li>
      <li><a href="https://npodlozhniy.github.io/tags/bayesian-inference/">bayesian-inference</a></li>
      <li><a href="https://npodlozhniy.github.io/tags/monte-carlo/">monte-carlo</a></li>
      <li><a href="https://npodlozhniy.github.io/tags/pymc/">pymc</a></li>
      <li><a href="https://npodlozhniy.github.io/tags/sequential-testing/">sequential-testing</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian A/B Test is NOT immune to peeking on twitter"
        href="https://twitter.com/intent/tweet/?text=Bayesian%20A%2fB%20Test%20is%20NOT%20immune%20to%20peeking&amp;url=https%3a%2f%2fnpodlozhniy.github.io%2fposts%2fbayesian-test%2f&amp;hashtags=ab-testing%2cbayesian-inference%2cmonte-carlo%2cpymc%2csequential-testing">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian A/B Test is NOT immune to peeking on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnpodlozhniy.github.io%2fposts%2fbayesian-test%2f&amp;title=Bayesian%20A%2fB%20Test%20is%20NOT%20immune%20to%20peeking&amp;summary=Bayesian%20A%2fB%20Test%20is%20NOT%20immune%20to%20peeking&amp;source=https%3a%2f%2fnpodlozhniy.github.io%2fposts%2fbayesian-test%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian A/B Test is NOT immune to peeking on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnpodlozhniy.github.io%2fposts%2fbayesian-test%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bayesian A/B Test is NOT immune to peeking on telegram"
        href="https://telegram.me/share/url?text=Bayesian%20A%2fB%20Test%20is%20NOT%20immune%20to%20peeking&amp;url=https%3a%2f%2fnpodlozhniy.github.io%2fposts%2fbayesian-test%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer><div id="disqus_thread"></div>
<script>
	
    var disqus_config = function () {
    this.page.url = 'https:\/\/npodlozhniy.github.io\/posts\/bayesian-test\/';  
    this.page.identifier = ''; 
	this.language = document.documentElement.lang; 
	};
 
    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://https-npodlozhniy-github-io-2.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
	
    document.addEventListener('theme-change', function(e) { 
		if (document.readyState == 'complete') {
			DISQUS.reset({ reload: true, config: disqus_config });
		}
	});

</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://npodlozhniy.github.io/">Nikita Podlozhniy</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }

		
		const event = new Event('theme-change');
		document.dispatchEvent(event)
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
